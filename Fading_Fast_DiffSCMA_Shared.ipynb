{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fading-Fast-DiffSCMA-Shared.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRQT+eqqn3ozfB4WeGV271",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekagra-ranjan/Auto-SCMA/blob/main/Fading_Fast_DiffSCMA_Shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDMhaO8y3j24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6c90c8cf-122e-4be8-b98e-338cc1ed337d"
      },
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt2\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import itertools\n",
        "import math\n",
        "from time import time\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cpu\"\n",
        "print(\"\\nDevice:\", device)\n",
        "\n",
        "seed = 6789 # also 0, 6789\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if device is not \"cpu\":\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.set_rng_state(torch.cuda.get_rng_state())\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "eps = 1e-8\n",
        "\n",
        "def pdb():\n",
        "    import pdb; pdb.set_trace()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67tG4PIq34FZ"
      },
      "source": [
        "# Number of symbols\n",
        "M = 4\n",
        "# Number of users\n",
        "J = 6\n",
        "# Number of orthogonal resources\n",
        "K = 4\n",
        "# sparse mapping matrix V: K x J\n",
        "V = torch.tensor([[0, 1, 1, 0, 1, 0],\n",
        "                  [1, 0, 1, 0, 0, 1],\n",
        "                  [0, 1, 0, 1, 0, 1],\n",
        "                  [1, 0, 0, 1, 1, 0]\n",
        "                ], dtype = torch.int32, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovwCuxyp3_6l"
      },
      "source": [
        "def get_codebook_from_condensed_codebook():\n",
        "    codebook = torch.zeros(J, M, K, 2).float().to(device)\n",
        "    for i in range(J):\n",
        "        resource_idx = (V[:, i]==1)\n",
        "        codebook[i, :, resource_idx, :] = condensed_codebook[i] # [J, M, dv, 2]\n",
        "    return codebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK7sP8VW33_A"
      },
      "source": [
        "def encode(symbols, codebook, h=None):\n",
        "    '''\n",
        "    Inputs: symbols: [BS, J], h: [BS, J, 2*K], codebook: [J, M, K, 2],\n",
        "    Returns [BS, 2*K]\n",
        "    '''\n",
        "\n",
        "    batch_size = symbols.shape[0]\n",
        "    encoded = torch.zeros((batch_size, 2*K)).to(device)\n",
        "    encoded_without_h = torch.zeros((batch_size, 2*K)).to(device)\n",
        "    \n",
        "    for user in range(J):\n",
        "        # pdb()\n",
        "        codeword = codebook[user][symbols[:, user]-1] # [BS, K, 2]\n",
        "        codeword = codeword * V[:, user].reshape(1, -1, 1) # makes sure that codeword for resources not connected dont exist\n",
        "        codeword = codeword.view(-1, 2*K) # [BS, 2*K]\n",
        "        encoded_without_h = encoded_without_h + codeword\n",
        "        codeword_faded = torch.zeros_like(codeword)\n",
        "        \n",
        "        if h is not None:\n",
        "            '''CORRECT THE MULT WITH H'''\n",
        "            real_idx = 2 * torch.arange(K).long()\n",
        "            img_idx = 2 * torch.arange(K).long() + 1\n",
        "            codeword_faded[:, real_idx] = codeword_faded[:, real_idx] + \\\n",
        "                                          (h[:, user, real_idx] * codeword[:, real_idx]) - \\\n",
        "                                          (h[:, user, img_idx] * codeword[:, img_idx])\n",
        "            codeword_faded[:, img_idx]  = codeword_faded[:, img_idx] + \\\n",
        "                                          (h[:, user, real_idx] * codeword[:, img_idx]) + \\\n",
        "                                          (h[:, user, img_idx] * codeword[:, real_idx])\n",
        "        else:\n",
        "            codeword_faded = codeword_faded + codeword\n",
        "\n",
        "        encoded = encoded + codeword_faded\n",
        "    # pdb()\n",
        "    return encoded, encoded_without_h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngnDFBGaDbBe"
      },
      "source": [
        "df = (V[0]==1).sum() # num of users connected to kth resource\n",
        "dv = (V.t()[0]==1).sum() # num of resources connected to jth user\n",
        "\n",
        "\n",
        "class MPA():\n",
        "\n",
        "    def __init__(self):\n",
        "        alphabets = np.arange(1, M+1)\n",
        "        self.symbols_resource = [torch.tensor(p) for p in itertools.product(alphabets, repeat=df)] \n",
        "        self.symbols_resource = torch.stack(self.symbols_resource).to(device)   # [M**df, df]\n",
        "        self.symbols_resource.unsqueeze_(0)                                     # [1, M**df, df]\n",
        "\n",
        "\n",
        "    def calc_distance(self, a, b):\n",
        "        '''\n",
        "        Args: a: [BS, 2]    b: [BS, M**df, 2]\n",
        "        Returns: distance: [BS, M**df]\n",
        "        '''\n",
        "        # pdb()\n",
        "        a = a.view(-1, 1, 2)\n",
        "        return ((a-b)**2).sum(-1).squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def get_codebook_centre_resource(centres_all, x_centres_all, codebook, h, resource_id):\n",
        "    def get_codebook_centre_resource(self, codebook, h, resource_id):\n",
        "        '''\n",
        "        Inputs:\n",
        "            codebook: [J, M, K, 2]\n",
        "            h dim: [batch_size, J, 2*K]\n",
        "            V: K x J\n",
        "\n",
        "        Outputs:\n",
        "            centres [BS, M**df, 2]: contains the real and img part of\n",
        "                                        codewords for all permutation\n",
        "                                        (i.e., M**df) of user symbols\n",
        "                                        connected to kth resource,\n",
        "                                        i.e., \\sum_{j} h^{T}x_{j}\n",
        "            x_centres [BS, M**df, 2*df]: contains the real and img part of\n",
        "                                            each df users in codewords for all\n",
        "                                            permutation (i.e., M**df) of user\n",
        "                                            symbols connected to kth resource, x_{j}\n",
        "        '''\n",
        "        \n",
        "        #---Create centres_all\n",
        "        t1 = time()\n",
        "        resource_user_idx = V[resource_id].nonzero().reshape(-1)\n",
        "        symbols_resource = self.symbols_resource.repeat(h.shape[0], 1, 1)            # [BS, M**df, df]\n",
        "        \n",
        "        symbols_resource_all = torch.zeros((M**df, J),\\\n",
        "                                            dtype = torch.int64, device=device) # [M**df, J]\n",
        "        symbols_resource_all[:, resource_user_idx] = symbols_resource[0]        # [M**df, J]\n",
        "        \n",
        "        h_repeat_perm = h.unsqueeze(1).repeat(1, M**df, 1, 1)                   # [BS, M**df, J, 2*K]\n",
        "        h_repeat_perm = h_repeat_perm.view(-1, J, 2*K)                          # [BS * M**df, J, 2*K]\n",
        "        \n",
        "        \n",
        "        symbols_resource = symbols_resource.view(-1, df)                        # [BS * M**df, df]\n",
        "        codeword_faded_k = torch.zeros((symbols_resource.shape[0], 2), device=device)\n",
        "        for user_idx, user in enumerate(resource_user_idx):\n",
        "            # pdb()\n",
        "            codeword = codebook[user][symbols_resource[:, user_idx]-1]          # [BS, K, 2]\n",
        "            codeword_k = codeword[:, resource_id, :]                            # [BS, 2]\n",
        "            codeword_k = codeword_k * V[resource_id, user] # makes sure that codeword for resources not connected dont exist\n",
        "            \n",
        "            '''CORRECT THE MULT WITH H'''\n",
        "            real_idx = 2 * resource_id\n",
        "            img_idx = 2 * resource_id + 1\n",
        "            codeword_faded_k[:, 0] = codeword_faded_k[:, 0] + \\\n",
        "                                            (h_repeat_perm[:, user, real_idx] * codeword_k[:, 0]) - \\\n",
        "                                            (h_repeat_perm[:, user, img_idx] * codeword_k[:, 1])\n",
        "            codeword_faded_k[:, 1]  = codeword_faded_k[:, 1] + \\\n",
        "                                            (h_repeat_perm[:, user, real_idx] * codeword_k[:, 1]) + \\\n",
        "                                            (h_repeat_perm[:, user, img_idx] * codeword_k[:, 0])\n",
        "        \n",
        "        # ---Create x_centres_all\n",
        "        t1 = time()\n",
        "        symbols_resource_all = symbols_resource_all.unsqueeze(1).repeat(1, df, 1) # [M**df, df, J]\n",
        "        symbols_resource_all = symbols_resource_all.view(-1, J)                 # [M**df * df, J]\n",
        "        h_x = torch.zeros((df, J, 2*K), device=device)                          # [df, J, 2*K]\n",
        "        real_idx = 2 * torch.arange(K).long()\n",
        "        temp = torch.zeros((2*K), device=device)\n",
        "        temp[real_idx] = 1.0\n",
        "        \n",
        "        for idx, val in enumerate(V[resource_id].nonzero().reshape(-1)):\n",
        "            h_x[idx, val, :] = temp\n",
        "        \n",
        "        h_x = h_x.view(1, df, J, 2*K)\n",
        "        h_x = h_x.repeat(M**df, 1, 1, 1)                                        # [M**df, df, J, 2*K]\n",
        "        h_x = h_x.view(-1, J, 2*K)\n",
        "        x_centres, _ = encode(symbols_resource_all, codebook, h=h_x)            # [M**df * df, 2*K]\n",
        "        x_centres = x_centres.view(M**df, df,2*K)\\\n",
        "                    [:, :, 2 * resource_id: 2 * resource_id + 2]                # [M**df, df, 2]\n",
        "        x_centres = x_centres.reshape(M**df, 2 * df)\n",
        "        \n",
        "        return codeword_faded_k.view(h.shape[0], M**df, 2), x_centres\n",
        "\n",
        "\n",
        "    def resource_to_user(self, x, u2r, codebook, h, sigma_square, beta):\n",
        "        '''\n",
        "        Inputs:  x: [BS, 2*K] , i.e., received signal, u2r: [BS, J, K, M], h: [BS, J, 2*K]\n",
        "        Output: [BS, K, J, M]\n",
        "        '''\n",
        "\n",
        "        message_all = torch.zeros((h.shape[0], K, J, M), device=device)\n",
        "        \n",
        "        for i in range(0, K):\n",
        "            # pdb()\n",
        "            '''\n",
        "            centres_all: [batch_size, M**df, 2]\n",
        "            x_centres_all: [BS, M**df, 2*df]\n",
        "            '''\n",
        "            t1 = time()\n",
        "            centres = self.centres_all[i]\n",
        "            x_centres = self.x_centres_all[i]\n",
        "            x_centres = x_centres.view(1, M**df, df, 2) # [1, M**df, df, 2]\n",
        "\n",
        "            distance = self.calc_distance(x[:, 2*i:2*i+2], centres) # distance([BS, 2], [BS, M**df, 2]) -> [BS, M**df]\n",
        "            distance = - (1.0/(2.0 * sigma_square)) * distance\n",
        "            assert distance.dtype == torch.float32, \"distance is not float32\"\n",
        "            M_k = torch.zeros((h.shape[0], J, M), device=device) # conditional prob of x_{j} given r_{k}\n",
        "\n",
        "            u2r_ = u2r.permute(0, 2, 1, 3) # [BS, K, J, M]\n",
        "\n",
        "\n",
        "            '''Check from here'''\n",
        "            u2r_resource = u2r_[:, i, V[i]==1] # [BS, df, M]\n",
        "            prior_big = torch.zeros((h.shape[0], M**df, df), device=device) # [BS, M**df, df]\n",
        "\n",
        "            t1 = time()\n",
        "            for df_idx in range(df):\n",
        "                \n",
        "                u2r_resource_df = u2r_resource[:, df_idx, :] # [BS, M]\n",
        "                u2r_resource_df = u2r_resource_df.reshape(1, -1) # [1, BS * M]\n",
        "                symbols_resource_df = (self.symbols_resource[:, :, df_idx] - 1).repeat(h.shape[0], 1) # [BS, M**df]\n",
        "                incrementor = torch.arange(0, M * h.shape[0], M).view(-1, 1).to(device) # [BS, 1]\n",
        "                symbols_resource_df_mask = symbols_resource_df + incrementor # [BS, M**df]\n",
        "                symbols_resource_df_mask = symbols_resource_df_mask.view(1, -1) # [1, BS * M**df]\n",
        "                prior_big_df = u2r_resource_df[0, symbols_resource_df_mask] # [1, BS * M**df]\n",
        "                prior_big_df = prior_big_df.view(-1, M**df) # [BS, M**df]\n",
        "                prior_big[:, :, df_idx] = prior_big_df # [BS, M**df, df]\n",
        "\n",
        "            prior_big_sum = prior_big.sum(-1) # [BS, M**df]\n",
        "            \n",
        "            user_idx = (V[i]).nonzero().reshape(-1)\n",
        "            # print('Time df', time()-t1)\n",
        "            t1 = time()\n",
        "            for idx, j in enumerate(user_idx):\n",
        "                # pdb()\n",
        "                \n",
        "                '''Careful: codebook is [J, M, K, 2] which is numpy'''\n",
        "                df_unique = codebook[j, :, i, :] # [M, 2]\n",
        "                x_centres_temp = x_centres[:, :, idx] # [1, M**df, 2]\n",
        "                assert df_unique.dtype == x_centres_temp.dtype, \"comparison vars dont have same dtype for mask_temp\"\n",
        "\n",
        "\n",
        "                # pdb()\n",
        "                mask_temp = (df_unique.view(1, M, 1, 2) == x_centres_temp.view(1, 1, M**df, 2)) # [1, M, M**df, 2]\n",
        "                mask = (mask_temp.sum(-1)==2) # [1, M, M**df]: for matching, both real and img parts will match hence 2.\n",
        "                mask = mask.repeat(h.shape[0], 1, 1) # [BS, M, M**df]\n",
        "\n",
        "                prior_big_m = prior_big_sum - prior_big[:, :, idx] # [BS, M**df]\n",
        "                message = distance + prior_big_m # [BS, M**df]\n",
        "                lowest_message = message.min()\n",
        "\n",
        "                message = message.unsqueeze(1).repeat(1, M, 1) # [BS, M, M**df]\n",
        "\n",
        "                # pdb()\n",
        "                message = torch.where(mask, message, lowest_message) # [BS, M, M**df]\n",
        "                \n",
        "                #---log-sum-MPA\n",
        "                message_max = message.max(-1)[0] # [BS, M]: 0 is used to use take the max and not the indices\n",
        "                message = message - message_max.view(-1, M, 1)\n",
        "                message = torch.exp(message) # [BS, M, M**df]\n",
        "                message = torch.where(mask, message, torch.tensor([0.0], device=device))\n",
        "                message = message.sum(-1) # [BS, M]\n",
        "                \n",
        "                message = torch.log(message) # [BS, M]\n",
        "                message = message + message_max # [BS, M]\n",
        "                # pdb()\n",
        "                \n",
        "                #---Max log MPA\n",
        "                # message = message.max(-1)[0] # [BS]: 0 is used to use take the max and not the indices\n",
        "                \n",
        "                message_all[:, i, j] = message + beta\n",
        "            # print('Time user_idx', time()-t1)\n",
        "                \n",
        "        return message_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def user_to_resource(self, r2u, gamma):\n",
        "        '''\n",
        "        Input: r2u: [BS, K, J, M]\n",
        "        Returns [BS, J, K, M]\n",
        "        '''\n",
        "        assert (r2u == float(\"-inf\")).sum() == 0, \"r2u has -inf values!\" # if -inf, then need to implement the counting of -inf freq\n",
        "\n",
        "        message_all = torch.zeros((r2u.shape[0], J, K, M), device=device)\n",
        "        r2u_ = r2u.permute(0, 2, 1, 3) # [BS, J, K, M]\n",
        "        # to filter out messages to those resources which are not onnected to the user\n",
        "        mask = torch.where(r2u_!=0, torch.tensor([1.0], device=device), torch.tensor([0.0], device=device))\n",
        "\n",
        "        # pdb()\n",
        "        ''' Need to change'''\n",
        "        message_all = -r2u_ # [BS, J, K, M]\n",
        "        for i in range(J):\n",
        "            message_all[:, i, :, :] = message_all[:, i, :, :] + r2u_[:, i].sum(-2).view(-1, 1, M) + gamma # [BS, 1, M]\n",
        "        # pdb()\n",
        "        message_all = mask * message_all\n",
        "        \n",
        "        #---normalisation\n",
        "\n",
        "        Z = torch.exp(message_all)\n",
        "        Z = torch.where(message_all!=0, Z, torch.tensor([torch.exp(-gamma) / M], device=device)) # [BS, J, K, M]\n",
        "        Z = torch.log(Z.sum(-1))\n",
        "\n",
        "        message_all = message_all - Z.view(-1, J, K, 1) # [BS, J, K, M]\n",
        "\n",
        "        return message_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def calc_output(self, r2u):\n",
        "        '''\n",
        "        Input: r2u: [BS, K, J, M]\n",
        "        Output: [BS, J, M]\n",
        "        '''\n",
        "        r2u_ = r2u.permute(0, 2, 3, 1) # [BS, J, M, K]\n",
        "        return r2u_.sum(-1) + gamma\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, codebook, h, sigma_square, beta, gamma):\n",
        "        ''' \n",
        "        Inputs:\n",
        "        x: [BS, 2*K] , i.e., received signal\n",
        "        h: [BS, J, 2*K]\n",
        "\n",
        "        Variables used:\n",
        "        u2r [BS, J, K, M]: user to resource message\n",
        "        r2u [BS, K, J, M]: resource to user message\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "            centres_all: [batch_size, M**df, 2]\n",
        "            x_centres_all: [BS, M**df, 2*df]\n",
        "        '''\n",
        "        self.centres_all = torch.zeros((K, h.shape[0], M**df, 2), device=device)        # [K, BS, M**df, 2]\n",
        "        self.x_centres_all = torch.zeros((K, M**df, 2*df), device=device)               # [K, M**sd, 2*df]\n",
        "\n",
        "        t1 = time()\n",
        "        for i in range(K):\n",
        "            self.centres_all[i], self.x_centres_all[i] = self.get_codebook_centre_resource(codebook, h, i)\n",
        "        # print('centres:', time()-t1)\n",
        "        \n",
        "        u2r = torch.zeros((x.shape[0], J, K, M), device=device) # Initialise u2r with 0\n",
        "        u2r = u2r + gamma\n",
        "        \n",
        "        # pdb(True)\n",
        "        t1 = time()\n",
        "        r2u = self.resource_to_user(x, u2r, codebook, h, sigma_square, beta) # [BS, K, J, M]\n",
        "        # print('0: r2u: ', time()-t1)\n",
        "        u2r = self.user_to_resource(r2u, gamma) # [BS, J, K, M]\n",
        "        t1=time()\n",
        "        # print('0: u2r: ', time()-t1)\n",
        "\n",
        "        for i in range(n_iter - 1):\n",
        "            # pdb()\n",
        "            t1=time()\n",
        "            r2u = self.resource_to_user(x, u2r, codebook, h, sigma_square, beta) # [BS, K, J, M]\n",
        "            # print(i, ': r2u: ', time()-t1)\n",
        "            t1=time()\n",
        "            u2r = self.user_to_resource(r2u, gamma) # [BS, J, K, M]\n",
        "            # print(i, ': u2r: ', time()-t1)\n",
        "        # pdb()\n",
        "        t1=time()\n",
        "        y = self.calc_output(r2u)\n",
        "        # print('output: ', time()-t1)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J14v_J1_330_"
      },
      "source": [
        "def add_noise(codewords, J, snr):\n",
        "    '''\n",
        "    codewords: [batch_size, 2*K]\n",
        "    snr: scalar\n",
        "    '''\n",
        "    # pdb()\n",
        "    batch_size = codewords.shape[0]\n",
        "    Es = torch.mean(codewords**2) # per real value or img value\n",
        "    Es_db = 10*torch.log10(Es)\n",
        "    noise_db = Es_db - torch.as_tensor(snr).to(device)\n",
        "    noise_power = 10**(noise_db/10)\n",
        "    noise = torch.randn(batch_size, 2*K).to(device) * torch.sqrt(noise_power).to(device)\n",
        "    output = codewords + noise\n",
        "\n",
        "    # pdb()\n",
        "\n",
        "    return noise, noise_power"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkcaPNWb33xj"
      },
      "source": [
        "def compute_ber(symbols, decoded_symbols, M):\n",
        "    '''\n",
        "    Inputs: symbols, decoded_symbols: [BS*J]\n",
        "    '''\n",
        "    symbols_0 = symbols - 1\n",
        "    decoded_symbols_0 = decoded_symbols - 1\n",
        "    true = np.unpackbits(symbols_0.reshape(-1, 1).astype('uint8'), axis=1)[:, -int(np.log2(M)):] # [BS*J, log2(M)]\n",
        "    predicted = np.unpackbits(decoded_symbols_0.reshape(-1, 1).astype('uint8'), axis=1)[:, -int(np.log2(M)):] # [BS*J, log2(M)]\n",
        "\n",
        "    Nerr_check = (true!=predicted) # [BS*J, log2(M)]\n",
        "    ber_batch = (true!=predicted).mean() # [1]\n",
        "    \n",
        "    Nerr_per_user = np.sum(Nerr_check, axis=-1) # [BS*J]\n",
        "    Nerr_per_user = np.sum(Nerr_per_user.reshape(-1, J), axis=0) # [J]\n",
        "    \n",
        "\n",
        "    return ber_batch, Nerr_per_user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_5LZkaD33t8"
      },
      "source": [
        "batch_size = 1024\n",
        "\n",
        "df = (V[0]==1).sum() # num of users connected to a resource (needs to be same for all resources)\n",
        "dv = (V[:, 0]==1).sum() # num of resources connected to a user (needs to be same for all users)\n",
        "\n",
        "#---Initialize the condensed_codebook\n",
        "condensed_codebook = torch.randn((J, M, dv, 2), dtype = torch.float32, device = device, requires_grad = True)\n",
        "\n",
        "#---Create MPA\n",
        "mpa = MPA()\n",
        "\n",
        "#---Create Optimizer and Loss Function\n",
        "params = [condensed_codebook]\n",
        "\n",
        "# optimizer = torch.optim.RMSprop(params, lr=1e-4, alpha=0.99, eps=1e-08, weight_decay=1e-5) # wt decay was 1e-5\n",
        "optimizer = torch.optim.Adam(params, lr=1e-2, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor = 0.5, patience = 150, mode = 'min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBlwYUQh33rp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "f4ca3433-5a0c-49bb-9bcd-bc51cc2b41f7"
      },
      "source": [
        "symbols = np.random.randint(1, M+1, (batch_size, J))\n",
        "enc = OneHotEncoder(categories=np.stack(J*[range(1,M+1)]))\n",
        "_ = enc.fit(symbols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:76: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if self.categories != 'auto':\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if self.categories == 'auto':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTV1EgM82sJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b3a8388-0182-43a4-e03d-414adafc3d87"
      },
      "source": [
        "is_fading = True\n",
        "is_noise = True\n",
        "print(\"Fading: \", is_fading, \"  Noise: \", is_noise)\n",
        "\n",
        "iterations = 500\n",
        "n_iter = 3\n",
        "maxNumErrs = 100 # min number of bits error to be seen before ending the run\n",
        "maxNumBits = 1e7 # max number of bits per user to be processed\n",
        "\n",
        "# beta = 0 #+ np.log(1/(2* math.pi * sigma_square)**0.5)\n",
        "\n",
        "Nerr_per_user = np.zeros([J])\n",
        "Nbits = 0 # number of bits processed till now\n",
        "\n",
        "for iteration in range(iterations):\n",
        "\n",
        "    EbN = 6\n",
        "    snr = EbN + 10*np.log10(np.log2(M)*J/K)\n",
        "\n",
        "    # pdb()\n",
        "    #---Get codebook\n",
        "    codebook = get_codebook_from_condensed_codebook()\n",
        "\n",
        "    #---Generate symbols and channel parameters\n",
        "    if is_fading is True:\n",
        "        h = (torch.randn(batch_size, J, 2*K)/torch.sqrt(torch.tensor(2.0))).to(device)\n",
        "    else:\n",
        "        h = torch.zeros(batch_size, J, 2*K).to(device)\n",
        "        real_idx = 2 * torch.arange(K).long()\n",
        "        h[:, :, real_idx] = 1.0\n",
        "\n",
        "    symbols = np.random.randint(1, M+1, (batch_size, J))\n",
        "    \n",
        "    codewords_faded, codewords_faded_without_h = encode(symbols, codebook, h = h)    \n",
        "    received_signal = codewords_faded\n",
        "    \n",
        "    '''noise is being calc using codewords_faded_without_h '''\n",
        "    noise, sigma_square = add_noise(codewords_faded_without_h, J, snr)\n",
        "    \n",
        "    if is_noise is True:\n",
        "        received_signal = received_signal + noise\n",
        "    \n",
        "\n",
        "    #---Decode\n",
        "    beta = torch.log((1/(2* math.pi * sigma_square)**0.5))\n",
        "    gamma = torch.log(torch.tensor((1.0/M), device=device, dtype=torch.float32)) # log(p_x{j})\n",
        "    decoded_symbols_one_hot = mpa.forward(received_signal, codebook, h, sigma_square, beta, gamma)\n",
        "    \n",
        "    # pdb()\n",
        "\n",
        "    # Backward pass\n",
        "    target = torch.LongTensor(symbols.reshape(-1) - 1).to(device)\n",
        "    loss = loss_func(decoded_symbols_one_hot.view(-1, M), target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # scheduler.step(loss)\n",
        "\n",
        "\n",
        "    # Compute metrics\n",
        "    decoded_symbols = torch.argmax(decoded_symbols_one_hot, dim=-1) + 1\n",
        "    accuracy_batch = np.mean(symbols == decoded_symbols.cpu().data.numpy())\n",
        "    ber_batch, Nerr_per_user_batch = compute_ber(symbols.reshape(batch_size*J), decoded_symbols.cpu().data.numpy().reshape(batch_size*J), M)\n",
        "    \n",
        "    Nerr_per_user += Nerr_per_user_batch\n",
        "    Nbits += batch_size * np.log2(M)\n",
        "\n",
        "    # if (iteration + 1) % 50 == 0:\n",
        "    print('iter: ', iteration+1, '/', iterations, ' Acc_batch = {:.4f}'.format(accuracy_batch),\n",
        "          ' SER_batch = {:.4f}'.format(1-accuracy_batch), ' BER_batch = {:.4f}'.format(ber_batch),\n",
        "          ' loss: {:.3f}'.format(loss), ' lr: {:.4f}'.format(optimizer.param_groups[0]['lr']))\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fading:  True   Noise:  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter:  1 / 500  Acc_batch = 0.8213  SER_batch = 0.1787  BER_batch = 0.1131  loss: 0.422  lr: 0.0100\n",
            "iter:  2 / 500  Acc_batch = 0.8229  SER_batch = 0.1771  BER_batch = 0.1124  loss: 0.404  lr: 0.0100\n",
            "iter:  3 / 500  Acc_batch = 0.8219  SER_batch = 0.1781  BER_batch = 0.1112  loss: 0.410  lr: 0.0100\n",
            "iter:  4 / 500  Acc_batch = 0.8512  SER_batch = 0.1488  BER_batch = 0.0949  loss: 0.362  lr: 0.0100\n",
            "iter:  5 / 500  Acc_batch = 0.8270  SER_batch = 0.1730  BER_batch = 0.1093  loss: 0.406  lr: 0.0100\n",
            "iter:  6 / 500  Acc_batch = 0.8273  SER_batch = 0.1727  BER_batch = 0.1100  loss: 0.398  lr: 0.0100\n",
            "iter:  7 / 500  Acc_batch = 0.8348  SER_batch = 0.1652  BER_batch = 0.1042  loss: 0.383  lr: 0.0100\n",
            "iter:  8 / 500  Acc_batch = 0.8459  SER_batch = 0.1541  BER_batch = 0.0964  loss: 0.366  lr: 0.0100\n",
            "iter:  9 / 500  Acc_batch = 0.8384  SER_batch = 0.1616  BER_batch = 0.1013  loss: 0.387  lr: 0.0100\n",
            "iter:  10 / 500  Acc_batch = 0.8311  SER_batch = 0.1689  BER_batch = 0.1059  loss: 0.395  lr: 0.0100\n",
            "iter:  11 / 500  Acc_batch = 0.8485  SER_batch = 0.1515  BER_batch = 0.0968  loss: 0.372  lr: 0.0100\n",
            "iter:  12 / 500  Acc_batch = 0.8460  SER_batch = 0.1540  BER_batch = 0.0959  loss: 0.358  lr: 0.0100\n",
            "iter:  13 / 500  Acc_batch = 0.8385  SER_batch = 0.1615  BER_batch = 0.1034  loss: 0.369  lr: 0.0100\n",
            "iter:  14 / 500  Acc_batch = 0.8480  SER_batch = 0.1520  BER_batch = 0.0954  loss: 0.368  lr: 0.0100\n",
            "iter:  15 / 500  Acc_batch = 0.8524  SER_batch = 0.1476  BER_batch = 0.0941  loss: 0.358  lr: 0.0100\n",
            "iter:  16 / 500  Acc_batch = 0.8394  SER_batch = 0.1606  BER_batch = 0.1029  loss: 0.383  lr: 0.0100\n",
            "iter:  17 / 500  Acc_batch = 0.8522  SER_batch = 0.1478  BER_batch = 0.0934  loss: 0.355  lr: 0.0100\n",
            "iter:  18 / 500  Acc_batch = 0.8542  SER_batch = 0.1458  BER_batch = 0.0922  loss: 0.356  lr: 0.0100\n",
            "iter:  19 / 500  Acc_batch = 0.8517  SER_batch = 0.1483  BER_batch = 0.0952  loss: 0.358  lr: 0.0100\n",
            "iter:  20 / 500  Acc_batch = 0.8595  SER_batch = 0.1405  BER_batch = 0.0890  loss: 0.344  lr: 0.0100\n",
            "iter:  21 / 500  Acc_batch = 0.8597  SER_batch = 0.1403  BER_batch = 0.0879  loss: 0.344  lr: 0.0100\n",
            "iter:  22 / 500  Acc_batch = 0.8569  SER_batch = 0.1431  BER_batch = 0.0912  loss: 0.344  lr: 0.0100\n",
            "iter:  23 / 500  Acc_batch = 0.8504  SER_batch = 0.1496  BER_batch = 0.0926  loss: 0.365  lr: 0.0100\n",
            "iter:  24 / 500  Acc_batch = 0.8582  SER_batch = 0.1418  BER_batch = 0.0895  loss: 0.344  lr: 0.0100\n",
            "iter:  25 / 500  Acc_batch = 0.8587  SER_batch = 0.1413  BER_batch = 0.0881  loss: 0.329  lr: 0.0100\n",
            "iter:  26 / 500  Acc_batch = 0.8524  SER_batch = 0.1476  BER_batch = 0.0936  loss: 0.350  lr: 0.0100\n",
            "iter:  27 / 500  Acc_batch = 0.8634  SER_batch = 0.1366  BER_batch = 0.0861  loss: 0.327  lr: 0.0100\n",
            "iter:  28 / 500  Acc_batch = 0.8581  SER_batch = 0.1419  BER_batch = 0.0914  loss: 0.346  lr: 0.0100\n",
            "iter:  29 / 500  Acc_batch = 0.8613  SER_batch = 0.1387  BER_batch = 0.0894  loss: 0.326  lr: 0.0100\n",
            "iter:  30 / 500  Acc_batch = 0.8620  SER_batch = 0.1380  BER_batch = 0.0881  loss: 0.332  lr: 0.0100\n",
            "iter:  31 / 500  Acc_batch = 0.8688  SER_batch = 0.1312  BER_batch = 0.0820  loss: 0.324  lr: 0.0100\n",
            "iter:  32 / 500  Acc_batch = 0.8677  SER_batch = 0.1323  BER_batch = 0.0846  loss: 0.320  lr: 0.0100\n",
            "iter:  33 / 500  Acc_batch = 0.8696  SER_batch = 0.1304  BER_batch = 0.0827  loss: 0.321  lr: 0.0100\n",
            "iter:  34 / 500  Acc_batch = 0.8594  SER_batch = 0.1406  BER_batch = 0.0893  loss: 0.342  lr: 0.0100\n",
            "iter:  35 / 500  Acc_batch = 0.8727  SER_batch = 0.1273  BER_batch = 0.0798  loss: 0.319  lr: 0.0100\n",
            "iter:  36 / 500  Acc_batch = 0.8734  SER_batch = 0.1266  BER_batch = 0.0800  loss: 0.305  lr: 0.0100\n",
            "iter:  37 / 500  Acc_batch = 0.8776  SER_batch = 0.1224  BER_batch = 0.0770  loss: 0.306  lr: 0.0100\n",
            "iter:  38 / 500  Acc_batch = 0.8607  SER_batch = 0.1393  BER_batch = 0.0894  loss: 0.342  lr: 0.0100\n",
            "iter:  39 / 500  Acc_batch = 0.8582  SER_batch = 0.1418  BER_batch = 0.0892  loss: 0.338  lr: 0.0100\n",
            "iter:  40 / 500  Acc_batch = 0.8706  SER_batch = 0.1294  BER_batch = 0.0827  loss: 0.316  lr: 0.0100\n",
            "iter:  41 / 500  Acc_batch = 0.8730  SER_batch = 0.1270  BER_batch = 0.0810  loss: 0.321  lr: 0.0100\n",
            "iter:  42 / 500  Acc_batch = 0.8726  SER_batch = 0.1274  BER_batch = 0.0795  loss: 0.314  lr: 0.0100\n",
            "iter:  43 / 500  Acc_batch = 0.8770  SER_batch = 0.1230  BER_batch = 0.0776  loss: 0.307  lr: 0.0100\n",
            "iter:  44 / 500  Acc_batch = 0.8820  SER_batch = 0.1180  BER_batch = 0.0741  loss: 0.292  lr: 0.0100\n",
            "iter:  45 / 500  Acc_batch = 0.8675  SER_batch = 0.1325  BER_batch = 0.0831  loss: 0.326  lr: 0.0100\n",
            "iter:  46 / 500  Acc_batch = 0.8674  SER_batch = 0.1326  BER_batch = 0.0812  loss: 0.326  lr: 0.0100\n",
            "iter:  47 / 500  Acc_batch = 0.8638  SER_batch = 0.1362  BER_batch = 0.0854  loss: 0.323  lr: 0.0100\n",
            "iter:  48 / 500  Acc_batch = 0.8792  SER_batch = 0.1208  BER_batch = 0.0749  loss: 0.308  lr: 0.0100\n",
            "iter:  49 / 500  Acc_batch = 0.8714  SER_batch = 0.1286  BER_batch = 0.0827  loss: 0.303  lr: 0.0100\n",
            "iter:  50 / 500  Acc_batch = 0.8735  SER_batch = 0.1265  BER_batch = 0.0801  loss: 0.314  lr: 0.0100\n",
            "iter:  51 / 500  Acc_batch = 0.8737  SER_batch = 0.1263  BER_batch = 0.0817  loss: 0.313  lr: 0.0100\n",
            "iter:  52 / 500  Acc_batch = 0.8820  SER_batch = 0.1180  BER_batch = 0.0740  loss: 0.298  lr: 0.0100\n",
            "iter:  53 / 500  Acc_batch = 0.8766  SER_batch = 0.1234  BER_batch = 0.0781  loss: 0.302  lr: 0.0100\n",
            "iter:  54 / 500  Acc_batch = 0.8625  SER_batch = 0.1375  BER_batch = 0.0868  loss: 0.330  lr: 0.0100\n",
            "iter:  55 / 500  Acc_batch = 0.8818  SER_batch = 0.1182  BER_batch = 0.0750  loss: 0.297  lr: 0.0100\n",
            "iter:  56 / 500  Acc_batch = 0.8737  SER_batch = 0.1263  BER_batch = 0.0806  loss: 0.300  lr: 0.0100\n",
            "iter:  57 / 500  Acc_batch = 0.8781  SER_batch = 0.1219  BER_batch = 0.0780  loss: 0.295  lr: 0.0100\n",
            "iter:  58 / 500  Acc_batch = 0.8760  SER_batch = 0.1240  BER_batch = 0.0785  loss: 0.312  lr: 0.0100\n",
            "iter:  59 / 500  Acc_batch = 0.8709  SER_batch = 0.1291  BER_batch = 0.0814  loss: 0.315  lr: 0.0100\n",
            "iter:  60 / 500  Acc_batch = 0.8743  SER_batch = 0.1257  BER_batch = 0.0793  loss: 0.317  lr: 0.0100\n",
            "iter:  61 / 500  Acc_batch = 0.8807  SER_batch = 0.1193  BER_batch = 0.0750  loss: 0.291  lr: 0.0100\n",
            "iter:  62 / 500  Acc_batch = 0.8828  SER_batch = 0.1172  BER_batch = 0.0722  loss: 0.294  lr: 0.0100\n",
            "iter:  63 / 500  Acc_batch = 0.8826  SER_batch = 0.1174  BER_batch = 0.0720  loss: 0.293  lr: 0.0100\n",
            "iter:  64 / 500  Acc_batch = 0.8817  SER_batch = 0.1183  BER_batch = 0.0746  loss: 0.294  lr: 0.0100\n",
            "iter:  65 / 500  Acc_batch = 0.8742  SER_batch = 0.1258  BER_batch = 0.0777  loss: 0.311  lr: 0.0100\n",
            "iter:  66 / 500  Acc_batch = 0.8719  SER_batch = 0.1281  BER_batch = 0.0796  loss: 0.310  lr: 0.0100\n",
            "iter:  67 / 500  Acc_batch = 0.8765  SER_batch = 0.1235  BER_batch = 0.0796  loss: 0.312  lr: 0.0100\n",
            "iter:  68 / 500  Acc_batch = 0.8730  SER_batch = 0.1270  BER_batch = 0.0807  loss: 0.295  lr: 0.0100\n",
            "iter:  69 / 500  Acc_batch = 0.8826  SER_batch = 0.1174  BER_batch = 0.0736  loss: 0.289  lr: 0.0100\n",
            "iter:  70 / 500  Acc_batch = 0.8687  SER_batch = 0.1313  BER_batch = 0.0820  loss: 0.325  lr: 0.0100\n",
            "iter:  71 / 500  Acc_batch = 0.8770  SER_batch = 0.1230  BER_batch = 0.0782  loss: 0.305  lr: 0.0100\n",
            "iter:  72 / 500  Acc_batch = 0.8846  SER_batch = 0.1154  BER_batch = 0.0719  loss: 0.283  lr: 0.0100\n",
            "iter:  73 / 500  Acc_batch = 0.8659  SER_batch = 0.1341  BER_batch = 0.0846  loss: 0.329  lr: 0.0100\n",
            "iter:  74 / 500  Acc_batch = 0.8722  SER_batch = 0.1278  BER_batch = 0.0813  loss: 0.318  lr: 0.0100\n",
            "iter:  75 / 500  Acc_batch = 0.8784  SER_batch = 0.1216  BER_batch = 0.0767  loss: 0.294  lr: 0.0100\n",
            "iter:  76 / 500  Acc_batch = 0.8844  SER_batch = 0.1156  BER_batch = 0.0715  loss: 0.282  lr: 0.0100\n",
            "iter:  77 / 500  Acc_batch = 0.8708  SER_batch = 0.1292  BER_batch = 0.0840  loss: 0.305  lr: 0.0100\n",
            "iter:  78 / 500  Acc_batch = 0.8770  SER_batch = 0.1230  BER_batch = 0.0793  loss: 0.296  lr: 0.0100\n",
            "iter:  79 / 500  Acc_batch = 0.8822  SER_batch = 0.1178  BER_batch = 0.0751  loss: 0.302  lr: 0.0100\n",
            "iter:  80 / 500  Acc_batch = 0.8854  SER_batch = 0.1146  BER_batch = 0.0736  loss: 0.282  lr: 0.0100\n",
            "iter:  81 / 500  Acc_batch = 0.8893  SER_batch = 0.1107  BER_batch = 0.0714  loss: 0.274  lr: 0.0100\n",
            "iter:  82 / 500  Acc_batch = 0.8857  SER_batch = 0.1143  BER_batch = 0.0723  loss: 0.304  lr: 0.0100\n",
            "iter:  83 / 500  Acc_batch = 0.8748  SER_batch = 0.1252  BER_batch = 0.0804  loss: 0.306  lr: 0.0100\n",
            "iter:  84 / 500  Acc_batch = 0.8807  SER_batch = 0.1193  BER_batch = 0.0767  loss: 0.292  lr: 0.0100\n",
            "iter:  85 / 500  Acc_batch = 0.8820  SER_batch = 0.1180  BER_batch = 0.0745  loss: 0.297  lr: 0.0100\n",
            "iter:  86 / 500  Acc_batch = 0.8757  SER_batch = 0.1243  BER_batch = 0.0775  loss: 0.308  lr: 0.0100\n",
            "iter:  87 / 500  Acc_batch = 0.8927  SER_batch = 0.1073  BER_batch = 0.0688  loss: 0.277  lr: 0.0100\n",
            "iter:  88 / 500  Acc_batch = 0.8778  SER_batch = 0.1222  BER_batch = 0.0772  loss: 0.312  lr: 0.0100\n",
            "iter:  89 / 500  Acc_batch = 0.8944  SER_batch = 0.1056  BER_batch = 0.0671  loss: 0.268  lr: 0.0100\n",
            "iter:  90 / 500  Acc_batch = 0.8950  SER_batch = 0.1050  BER_batch = 0.0657  loss: 0.258  lr: 0.0100\n",
            "iter:  91 / 500  Acc_batch = 0.8727  SER_batch = 0.1273  BER_batch = 0.0806  loss: 0.306  lr: 0.0100\n",
            "iter:  92 / 500  Acc_batch = 0.8854  SER_batch = 0.1146  BER_batch = 0.0719  loss: 0.280  lr: 0.0100\n",
            "iter:  93 / 500  Acc_batch = 0.8799  SER_batch = 0.1201  BER_batch = 0.0741  loss: 0.300  lr: 0.0100\n",
            "iter:  94 / 500  Acc_batch = 0.8849  SER_batch = 0.1151  BER_batch = 0.0742  loss: 0.292  lr: 0.0100\n",
            "iter:  95 / 500  Acc_batch = 0.8784  SER_batch = 0.1216  BER_batch = 0.0776  loss: 0.291  lr: 0.0100\n",
            "iter:  96 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0728  loss: 0.293  lr: 0.0100\n",
            "iter:  97 / 500  Acc_batch = 0.8761  SER_batch = 0.1239  BER_batch = 0.0789  loss: 0.300  lr: 0.0100\n",
            "iter:  98 / 500  Acc_batch = 0.8870  SER_batch = 0.1130  BER_batch = 0.0706  loss: 0.276  lr: 0.0100\n",
            "iter:  99 / 500  Acc_batch = 0.8747  SER_batch = 0.1253  BER_batch = 0.0815  loss: 0.310  lr: 0.0100\n",
            "iter:  100 / 500  Acc_batch = 0.8841  SER_batch = 0.1159  BER_batch = 0.0744  loss: 0.277  lr: 0.0100\n",
            "iter:  101 / 500  Acc_batch = 0.8817  SER_batch = 0.1183  BER_batch = 0.0732  loss: 0.297  lr: 0.0100\n",
            "iter:  102 / 500  Acc_batch = 0.8864  SER_batch = 0.1136  BER_batch = 0.0719  loss: 0.285  lr: 0.0100\n",
            "iter:  103 / 500  Acc_batch = 0.8792  SER_batch = 0.1208  BER_batch = 0.0788  loss: 0.293  lr: 0.0100\n",
            "iter:  104 / 500  Acc_batch = 0.8849  SER_batch = 0.1151  BER_batch = 0.0736  loss: 0.282  lr: 0.0100\n",
            "iter:  105 / 500  Acc_batch = 0.8810  SER_batch = 0.1190  BER_batch = 0.0750  loss: 0.283  lr: 0.0100\n",
            "iter:  106 / 500  Acc_batch = 0.8796  SER_batch = 0.1204  BER_batch = 0.0758  loss: 0.295  lr: 0.0100\n",
            "iter:  107 / 500  Acc_batch = 0.8760  SER_batch = 0.1240  BER_batch = 0.0788  loss: 0.299  lr: 0.0100\n",
            "iter:  108 / 500  Acc_batch = 0.8883  SER_batch = 0.1117  BER_batch = 0.0699  loss: 0.292  lr: 0.0100\n",
            "iter:  109 / 500  Acc_batch = 0.8815  SER_batch = 0.1185  BER_batch = 0.0741  loss: 0.286  lr: 0.0100\n",
            "iter:  110 / 500  Acc_batch = 0.8717  SER_batch = 0.1283  BER_batch = 0.0809  loss: 0.324  lr: 0.0100\n",
            "iter:  111 / 500  Acc_batch = 0.8800  SER_batch = 0.1200  BER_batch = 0.0771  loss: 0.303  lr: 0.0100\n",
            "iter:  112 / 500  Acc_batch = 0.8849  SER_batch = 0.1151  BER_batch = 0.0728  loss: 0.293  lr: 0.0100\n",
            "iter:  113 / 500  Acc_batch = 0.8957  SER_batch = 0.1043  BER_batch = 0.0648  loss: 0.272  lr: 0.0100\n",
            "iter:  114 / 500  Acc_batch = 0.8870  SER_batch = 0.1130  BER_batch = 0.0719  loss: 0.274  lr: 0.0100\n",
            "iter:  115 / 500  Acc_batch = 0.8810  SER_batch = 0.1190  BER_batch = 0.0758  loss: 0.290  lr: 0.0100\n",
            "iter:  116 / 500  Acc_batch = 0.8820  SER_batch = 0.1180  BER_batch = 0.0752  loss: 0.291  lr: 0.0100\n",
            "iter:  117 / 500  Acc_batch = 0.8766  SER_batch = 0.1234  BER_batch = 0.0793  loss: 0.318  lr: 0.0100\n",
            "iter:  118 / 500  Acc_batch = 0.8838  SER_batch = 0.1162  BER_batch = 0.0732  loss: 0.287  lr: 0.0100\n",
            "iter:  119 / 500  Acc_batch = 0.8932  SER_batch = 0.1068  BER_batch = 0.0689  loss: 0.266  lr: 0.0100\n",
            "iter:  120 / 500  Acc_batch = 0.8874  SER_batch = 0.1126  BER_batch = 0.0740  loss: 0.279  lr: 0.0100\n",
            "iter:  121 / 500  Acc_batch = 0.8708  SER_batch = 0.1292  BER_batch = 0.0796  loss: 0.327  lr: 0.0100\n",
            "iter:  122 / 500  Acc_batch = 0.8931  SER_batch = 0.1069  BER_batch = 0.0664  loss: 0.271  lr: 0.0100\n",
            "iter:  123 / 500  Acc_batch = 0.8874  SER_batch = 0.1126  BER_batch = 0.0703  loss: 0.268  lr: 0.0100\n",
            "iter:  124 / 500  Acc_batch = 0.8804  SER_batch = 0.1196  BER_batch = 0.0758  loss: 0.290  lr: 0.0100\n",
            "iter:  125 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0690  loss: 0.272  lr: 0.0100\n",
            "iter:  126 / 500  Acc_batch = 0.8800  SER_batch = 0.1200  BER_batch = 0.0764  loss: 0.298  lr: 0.0100\n",
            "iter:  127 / 500  Acc_batch = 0.8791  SER_batch = 0.1209  BER_batch = 0.0758  loss: 0.288  lr: 0.0100\n",
            "iter:  128 / 500  Acc_batch = 0.8804  SER_batch = 0.1196  BER_batch = 0.0748  loss: 0.288  lr: 0.0100\n",
            "iter:  129 / 500  Acc_batch = 0.8826  SER_batch = 0.1174  BER_batch = 0.0738  loss: 0.296  lr: 0.0100\n",
            "iter:  130 / 500  Acc_batch = 0.8818  SER_batch = 0.1182  BER_batch = 0.0732  loss: 0.304  lr: 0.0100\n",
            "iter:  131 / 500  Acc_batch = 0.8971  SER_batch = 0.1029  BER_batch = 0.0645  loss: 0.259  lr: 0.0100\n",
            "iter:  132 / 500  Acc_batch = 0.8856  SER_batch = 0.1144  BER_batch = 0.0750  loss: 0.286  lr: 0.0100\n",
            "iter:  133 / 500  Acc_batch = 0.8719  SER_batch = 0.1281  BER_batch = 0.0806  loss: 0.302  lr: 0.0100\n",
            "iter:  134 / 500  Acc_batch = 0.8826  SER_batch = 0.1174  BER_batch = 0.0730  loss: 0.301  lr: 0.0100\n",
            "iter:  135 / 500  Acc_batch = 0.8828  SER_batch = 0.1172  BER_batch = 0.0739  loss: 0.293  lr: 0.0100\n",
            "iter:  136 / 500  Acc_batch = 0.8781  SER_batch = 0.1219  BER_batch = 0.0770  loss: 0.293  lr: 0.0100\n",
            "iter:  137 / 500  Acc_batch = 0.8949  SER_batch = 0.1051  BER_batch = 0.0665  loss: 0.263  lr: 0.0100\n",
            "iter:  138 / 500  Acc_batch = 0.8892  SER_batch = 0.1108  BER_batch = 0.0710  loss: 0.268  lr: 0.0100\n",
            "iter:  139 / 500  Acc_batch = 0.8846  SER_batch = 0.1154  BER_batch = 0.0738  loss: 0.281  lr: 0.0100\n",
            "iter:  140 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0723  loss: 0.287  lr: 0.0100\n",
            "iter:  141 / 500  Acc_batch = 0.8957  SER_batch = 0.1043  BER_batch = 0.0668  loss: 0.270  lr: 0.0100\n",
            "iter:  142 / 500  Acc_batch = 0.8864  SER_batch = 0.1136  BER_batch = 0.0724  loss: 0.290  lr: 0.0100\n",
            "iter:  143 / 500  Acc_batch = 0.8853  SER_batch = 0.1147  BER_batch = 0.0749  loss: 0.282  lr: 0.0100\n",
            "iter:  144 / 500  Acc_batch = 0.8885  SER_batch = 0.1115  BER_batch = 0.0723  loss: 0.278  lr: 0.0100\n",
            "iter:  145 / 500  Acc_batch = 0.8869  SER_batch = 0.1131  BER_batch = 0.0721  loss: 0.276  lr: 0.0100\n",
            "iter:  146 / 500  Acc_batch = 0.8903  SER_batch = 0.1097  BER_batch = 0.0698  loss: 0.276  lr: 0.0100\n",
            "iter:  147 / 500  Acc_batch = 0.8836  SER_batch = 0.1164  BER_batch = 0.0736  loss: 0.280  lr: 0.0100\n",
            "iter:  148 / 500  Acc_batch = 0.8958  SER_batch = 0.1042  BER_batch = 0.0670  loss: 0.256  lr: 0.0100\n",
            "iter:  149 / 500  Acc_batch = 0.8822  SER_batch = 0.1178  BER_batch = 0.0749  loss: 0.291  lr: 0.0100\n",
            "iter:  150 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0680  loss: 0.276  lr: 0.0100\n",
            "iter:  151 / 500  Acc_batch = 0.8846  SER_batch = 0.1154  BER_batch = 0.0736  loss: 0.283  lr: 0.0100\n",
            "iter:  152 / 500  Acc_batch = 0.8923  SER_batch = 0.1077  BER_batch = 0.0688  loss: 0.271  lr: 0.0100\n",
            "iter:  153 / 500  Acc_batch = 0.8813  SER_batch = 0.1187  BER_batch = 0.0757  loss: 0.285  lr: 0.0100\n",
            "iter:  154 / 500  Acc_batch = 0.8872  SER_batch = 0.1128  BER_batch = 0.0725  loss: 0.282  lr: 0.0100\n",
            "iter:  155 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0752  loss: 0.290  lr: 0.0100\n",
            "iter:  156 / 500  Acc_batch = 0.8828  SER_batch = 0.1172  BER_batch = 0.0754  loss: 0.299  lr: 0.0100\n",
            "iter:  157 / 500  Acc_batch = 0.8914  SER_batch = 0.1086  BER_batch = 0.0691  loss: 0.269  lr: 0.0100\n",
            "iter:  158 / 500  Acc_batch = 0.8849  SER_batch = 0.1151  BER_batch = 0.0731  loss: 0.290  lr: 0.0100\n",
            "iter:  159 / 500  Acc_batch = 0.8841  SER_batch = 0.1159  BER_batch = 0.0751  loss: 0.285  lr: 0.0100\n",
            "iter:  160 / 500  Acc_batch = 0.8892  SER_batch = 0.1108  BER_batch = 0.0707  loss: 0.279  lr: 0.0100\n",
            "iter:  161 / 500  Acc_batch = 0.8779  SER_batch = 0.1221  BER_batch = 0.0786  loss: 0.302  lr: 0.0100\n",
            "iter:  162 / 500  Acc_batch = 0.8869  SER_batch = 0.1131  BER_batch = 0.0725  loss: 0.277  lr: 0.0100\n",
            "iter:  163 / 500  Acc_batch = 0.8864  SER_batch = 0.1136  BER_batch = 0.0728  loss: 0.282  lr: 0.0100\n",
            "iter:  164 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0731  loss: 0.303  lr: 0.0100\n",
            "iter:  165 / 500  Acc_batch = 0.8830  SER_batch = 0.1170  BER_batch = 0.0744  loss: 0.283  lr: 0.0100\n",
            "iter:  166 / 500  Acc_batch = 0.8755  SER_batch = 0.1245  BER_batch = 0.0809  loss: 0.294  lr: 0.0100\n",
            "iter:  167 / 500  Acc_batch = 0.8914  SER_batch = 0.1086  BER_batch = 0.0686  loss: 0.279  lr: 0.0100\n",
            "iter:  168 / 500  Acc_batch = 0.8792  SER_batch = 0.1208  BER_batch = 0.0775  loss: 0.294  lr: 0.0100\n",
            "iter:  169 / 500  Acc_batch = 0.8970  SER_batch = 0.1030  BER_batch = 0.0655  loss: 0.263  lr: 0.0100\n",
            "iter:  170 / 500  Acc_batch = 0.8815  SER_batch = 0.1185  BER_batch = 0.0743  loss: 0.295  lr: 0.0100\n",
            "iter:  171 / 500  Acc_batch = 0.8799  SER_batch = 0.1201  BER_batch = 0.0766  loss: 0.303  lr: 0.0100\n",
            "iter:  172 / 500  Acc_batch = 0.8893  SER_batch = 0.1107  BER_batch = 0.0704  loss: 0.285  lr: 0.0100\n",
            "iter:  173 / 500  Acc_batch = 0.8945  SER_batch = 0.1055  BER_batch = 0.0691  loss: 0.269  lr: 0.0100\n",
            "iter:  174 / 500  Acc_batch = 0.8885  SER_batch = 0.1115  BER_batch = 0.0722  loss: 0.286  lr: 0.0100\n",
            "iter:  175 / 500  Acc_batch = 0.8826  SER_batch = 0.1174  BER_batch = 0.0759  loss: 0.289  lr: 0.0100\n",
            "iter:  176 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0700  loss: 0.272  lr: 0.0100\n",
            "iter:  177 / 500  Acc_batch = 0.8830  SER_batch = 0.1170  BER_batch = 0.0755  loss: 0.290  lr: 0.0100\n",
            "iter:  178 / 500  Acc_batch = 0.8859  SER_batch = 0.1141  BER_batch = 0.0725  loss: 0.280  lr: 0.0100\n",
            "iter:  179 / 500  Acc_batch = 0.8905  SER_batch = 0.1095  BER_batch = 0.0723  loss: 0.273  lr: 0.0100\n",
            "iter:  180 / 500  Acc_batch = 0.8840  SER_batch = 0.1160  BER_batch = 0.0755  loss: 0.285  lr: 0.0100\n",
            "iter:  181 / 500  Acc_batch = 0.8926  SER_batch = 0.1074  BER_batch = 0.0691  loss: 0.275  lr: 0.0100\n",
            "iter:  182 / 500  Acc_batch = 0.8924  SER_batch = 0.1076  BER_batch = 0.0666  loss: 0.273  lr: 0.0100\n",
            "iter:  183 / 500  Acc_batch = 0.8962  SER_batch = 0.1038  BER_batch = 0.0667  loss: 0.269  lr: 0.0100\n",
            "iter:  184 / 500  Acc_batch = 0.8905  SER_batch = 0.1095  BER_batch = 0.0697  loss: 0.269  lr: 0.0100\n",
            "iter:  185 / 500  Acc_batch = 0.8840  SER_batch = 0.1160  BER_batch = 0.0734  loss: 0.287  lr: 0.0100\n",
            "iter:  186 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0695  loss: 0.274  lr: 0.0100\n",
            "iter:  187 / 500  Acc_batch = 0.8901  SER_batch = 0.1099  BER_batch = 0.0692  loss: 0.272  lr: 0.0100\n",
            "iter:  188 / 500  Acc_batch = 0.8874  SER_batch = 0.1126  BER_batch = 0.0712  loss: 0.282  lr: 0.0100\n",
            "iter:  189 / 500  Acc_batch = 0.8853  SER_batch = 0.1147  BER_batch = 0.0728  loss: 0.282  lr: 0.0100\n",
            "iter:  190 / 500  Acc_batch = 0.8968  SER_batch = 0.1032  BER_batch = 0.0653  loss: 0.268  lr: 0.0100\n",
            "iter:  191 / 500  Acc_batch = 0.8869  SER_batch = 0.1131  BER_batch = 0.0716  loss: 0.274  lr: 0.0100\n",
            "iter:  192 / 500  Acc_batch = 0.8805  SER_batch = 0.1195  BER_batch = 0.0767  loss: 0.285  lr: 0.0100\n",
            "iter:  193 / 500  Acc_batch = 0.8826  SER_batch = 0.1174  BER_batch = 0.0749  loss: 0.286  lr: 0.0100\n",
            "iter:  194 / 500  Acc_batch = 0.8828  SER_batch = 0.1172  BER_batch = 0.0745  loss: 0.304  lr: 0.0100\n",
            "iter:  195 / 500  Acc_batch = 0.8787  SER_batch = 0.1213  BER_batch = 0.0764  loss: 0.297  lr: 0.0100\n",
            "iter:  196 / 500  Acc_batch = 0.8812  SER_batch = 0.1188  BER_batch = 0.0758  loss: 0.294  lr: 0.0100\n",
            "iter:  197 / 500  Acc_batch = 0.8921  SER_batch = 0.1079  BER_batch = 0.0696  loss: 0.276  lr: 0.0100\n",
            "iter:  198 / 500  Acc_batch = 0.8797  SER_batch = 0.1203  BER_batch = 0.0774  loss: 0.299  lr: 0.0100\n",
            "iter:  199 / 500  Acc_batch = 0.8887  SER_batch = 0.1113  BER_batch = 0.0723  loss: 0.276  lr: 0.0100\n",
            "iter:  200 / 500  Acc_batch = 0.8864  SER_batch = 0.1136  BER_batch = 0.0731  loss: 0.289  lr: 0.0100\n",
            "iter:  201 / 500  Acc_batch = 0.8809  SER_batch = 0.1191  BER_batch = 0.0767  loss: 0.303  lr: 0.0100\n",
            "iter:  202 / 500  Acc_batch = 0.8787  SER_batch = 0.1213  BER_batch = 0.0776  loss: 0.290  lr: 0.0100\n",
            "iter:  203 / 500  Acc_batch = 0.8875  SER_batch = 0.1125  BER_batch = 0.0713  loss: 0.282  lr: 0.0100\n",
            "iter:  204 / 500  Acc_batch = 0.8924  SER_batch = 0.1076  BER_batch = 0.0689  loss: 0.269  lr: 0.0100\n",
            "iter:  205 / 500  Acc_batch = 0.8818  SER_batch = 0.1182  BER_batch = 0.0748  loss: 0.284  lr: 0.0100\n",
            "iter:  206 / 500  Acc_batch = 0.8901  SER_batch = 0.1099  BER_batch = 0.0697  loss: 0.282  lr: 0.0100\n",
            "iter:  207 / 500  Acc_batch = 0.8892  SER_batch = 0.1108  BER_batch = 0.0710  loss: 0.266  lr: 0.0100\n",
            "iter:  208 / 500  Acc_batch = 0.8958  SER_batch = 0.1042  BER_batch = 0.0674  loss: 0.257  lr: 0.0100\n",
            "iter:  209 / 500  Acc_batch = 0.8963  SER_batch = 0.1037  BER_batch = 0.0659  loss: 0.261  lr: 0.0100\n",
            "iter:  210 / 500  Acc_batch = 0.8932  SER_batch = 0.1068  BER_batch = 0.0666  loss: 0.269  lr: 0.0100\n",
            "iter:  211 / 500  Acc_batch = 0.8911  SER_batch = 0.1089  BER_batch = 0.0697  loss: 0.273  lr: 0.0100\n",
            "iter:  212 / 500  Acc_batch = 0.8835  SER_batch = 0.1165  BER_batch = 0.0740  loss: 0.285  lr: 0.0100\n",
            "iter:  213 / 500  Acc_batch = 0.8826  SER_batch = 0.1174  BER_batch = 0.0753  loss: 0.287  lr: 0.0100\n",
            "iter:  214 / 500  Acc_batch = 0.8962  SER_batch = 0.1038  BER_batch = 0.0672  loss: 0.261  lr: 0.0100\n",
            "iter:  215 / 500  Acc_batch = 0.8921  SER_batch = 0.1079  BER_batch = 0.0707  loss: 0.273  lr: 0.0100\n",
            "iter:  216 / 500  Acc_batch = 0.8866  SER_batch = 0.1134  BER_batch = 0.0736  loss: 0.288  lr: 0.0100\n",
            "iter:  217 / 500  Acc_batch = 0.8944  SER_batch = 0.1056  BER_batch = 0.0690  loss: 0.263  lr: 0.0100\n",
            "iter:  218 / 500  Acc_batch = 0.8936  SER_batch = 0.1064  BER_batch = 0.0696  loss: 0.270  lr: 0.0100\n",
            "iter:  219 / 500  Acc_batch = 0.8854  SER_batch = 0.1146  BER_batch = 0.0719  loss: 0.289  lr: 0.0100\n",
            "iter:  220 / 500  Acc_batch = 0.8898  SER_batch = 0.1102  BER_batch = 0.0713  loss: 0.274  lr: 0.0100\n",
            "iter:  221 / 500  Acc_batch = 0.8966  SER_batch = 0.1034  BER_batch = 0.0675  loss: 0.256  lr: 0.0100\n",
            "iter:  222 / 500  Acc_batch = 0.8861  SER_batch = 0.1139  BER_batch = 0.0723  loss: 0.281  lr: 0.0100\n",
            "iter:  223 / 500  Acc_batch = 0.8895  SER_batch = 0.1105  BER_batch = 0.0708  loss: 0.285  lr: 0.0100\n",
            "iter:  224 / 500  Acc_batch = 0.8840  SER_batch = 0.1160  BER_batch = 0.0750  loss: 0.295  lr: 0.0100\n",
            "iter:  225 / 500  Acc_batch = 0.8805  SER_batch = 0.1195  BER_batch = 0.0765  loss: 0.301  lr: 0.0100\n",
            "iter:  226 / 500  Acc_batch = 0.8892  SER_batch = 0.1108  BER_batch = 0.0708  loss: 0.283  lr: 0.0100\n",
            "iter:  227 / 500  Acc_batch = 0.8849  SER_batch = 0.1151  BER_batch = 0.0745  loss: 0.291  lr: 0.0100\n",
            "iter:  228 / 500  Acc_batch = 0.8898  SER_batch = 0.1102  BER_batch = 0.0706  loss: 0.282  lr: 0.0100\n",
            "iter:  229 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0706  loss: 0.271  lr: 0.0100\n",
            "iter:  230 / 500  Acc_batch = 0.8892  SER_batch = 0.1108  BER_batch = 0.0722  loss: 0.285  lr: 0.0100\n",
            "iter:  231 / 500  Acc_batch = 0.9062  SER_batch = 0.0938  BER_batch = 0.0592  loss: 0.244  lr: 0.0100\n",
            "iter:  232 / 500  Acc_batch = 0.8906  SER_batch = 0.1094  BER_batch = 0.0709  loss: 0.283  lr: 0.0100\n",
            "iter:  233 / 500  Acc_batch = 0.8866  SER_batch = 0.1134  BER_batch = 0.0728  loss: 0.284  lr: 0.0100\n",
            "iter:  234 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0737  loss: 0.272  lr: 0.0100\n",
            "iter:  235 / 500  Acc_batch = 0.8872  SER_batch = 0.1128  BER_batch = 0.0728  loss: 0.279  lr: 0.0100\n",
            "iter:  236 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0704  loss: 0.276  lr: 0.0100\n",
            "iter:  237 / 500  Acc_batch = 0.8826  SER_batch = 0.1174  BER_batch = 0.0769  loss: 0.301  lr: 0.0100\n",
            "iter:  238 / 500  Acc_batch = 0.8813  SER_batch = 0.1187  BER_batch = 0.0758  loss: 0.300  lr: 0.0100\n",
            "iter:  239 / 500  Acc_batch = 0.8844  SER_batch = 0.1156  BER_batch = 0.0750  loss: 0.283  lr: 0.0100\n",
            "iter:  240 / 500  Acc_batch = 0.8880  SER_batch = 0.1120  BER_batch = 0.0740  loss: 0.283  lr: 0.0100\n",
            "iter:  241 / 500  Acc_batch = 0.8722  SER_batch = 0.1278  BER_batch = 0.0807  loss: 0.301  lr: 0.0100\n",
            "iter:  242 / 500  Acc_batch = 0.8864  SER_batch = 0.1136  BER_batch = 0.0713  loss: 0.286  lr: 0.0100\n",
            "iter:  243 / 500  Acc_batch = 0.8836  SER_batch = 0.1164  BER_batch = 0.0747  loss: 0.285  lr: 0.0100\n",
            "iter:  244 / 500  Acc_batch = 0.8976  SER_batch = 0.1024  BER_batch = 0.0662  loss: 0.261  lr: 0.0100\n",
            "iter:  245 / 500  Acc_batch = 0.8825  SER_batch = 0.1175  BER_batch = 0.0757  loss: 0.285  lr: 0.0100\n",
            "iter:  246 / 500  Acc_batch = 0.8805  SER_batch = 0.1195  BER_batch = 0.0782  loss: 0.291  lr: 0.0100\n",
            "iter:  247 / 500  Acc_batch = 0.8900  SER_batch = 0.1100  BER_batch = 0.0711  loss: 0.279  lr: 0.0100\n",
            "iter:  248 / 500  Acc_batch = 0.8846  SER_batch = 0.1154  BER_batch = 0.0741  loss: 0.291  lr: 0.0100\n",
            "iter:  249 / 500  Acc_batch = 0.8901  SER_batch = 0.1099  BER_batch = 0.0696  loss: 0.279  lr: 0.0100\n",
            "iter:  250 / 500  Acc_batch = 0.8849  SER_batch = 0.1151  BER_batch = 0.0736  loss: 0.297  lr: 0.0100\n",
            "iter:  251 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0714  loss: 0.276  lr: 0.0100\n",
            "iter:  252 / 500  Acc_batch = 0.8937  SER_batch = 0.1063  BER_batch = 0.0685  loss: 0.276  lr: 0.0100\n",
            "iter:  253 / 500  Acc_batch = 0.8792  SER_batch = 0.1208  BER_batch = 0.0780  loss: 0.303  lr: 0.0100\n",
            "iter:  254 / 500  Acc_batch = 0.8937  SER_batch = 0.1063  BER_batch = 0.0688  loss: 0.270  lr: 0.0100\n",
            "iter:  255 / 500  Acc_batch = 0.8896  SER_batch = 0.1104  BER_batch = 0.0720  loss: 0.272  lr: 0.0100\n",
            "iter:  256 / 500  Acc_batch = 0.8853  SER_batch = 0.1147  BER_batch = 0.0738  loss: 0.293  lr: 0.0100\n",
            "iter:  257 / 500  Acc_batch = 0.8911  SER_batch = 0.1089  BER_batch = 0.0716  loss: 0.269  lr: 0.0100\n",
            "iter:  258 / 500  Acc_batch = 0.8942  SER_batch = 0.1058  BER_batch = 0.0697  loss: 0.264  lr: 0.0100\n",
            "iter:  259 / 500  Acc_batch = 0.8766  SER_batch = 0.1234  BER_batch = 0.0790  loss: 0.300  lr: 0.0100\n",
            "iter:  260 / 500  Acc_batch = 0.8931  SER_batch = 0.1069  BER_batch = 0.0703  loss: 0.274  lr: 0.0100\n",
            "iter:  261 / 500  Acc_batch = 0.8968  SER_batch = 0.1032  BER_batch = 0.0670  loss: 0.266  lr: 0.0100\n",
            "iter:  262 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0737  loss: 0.296  lr: 0.0100\n",
            "iter:  263 / 500  Acc_batch = 0.8831  SER_batch = 0.1169  BER_batch = 0.0776  loss: 0.292  lr: 0.0100\n",
            "iter:  264 / 500  Acc_batch = 0.8870  SER_batch = 0.1130  BER_batch = 0.0719  loss: 0.274  lr: 0.0100\n",
            "iter:  265 / 500  Acc_batch = 0.8880  SER_batch = 0.1120  BER_batch = 0.0707  loss: 0.275  lr: 0.0100\n",
            "iter:  266 / 500  Acc_batch = 0.8890  SER_batch = 0.1110  BER_batch = 0.0722  loss: 0.287  lr: 0.0100\n",
            "iter:  267 / 500  Acc_batch = 0.8870  SER_batch = 0.1130  BER_batch = 0.0719  loss: 0.286  lr: 0.0100\n",
            "iter:  268 / 500  Acc_batch = 0.8911  SER_batch = 0.1089  BER_batch = 0.0702  loss: 0.269  lr: 0.0100\n",
            "iter:  269 / 500  Acc_batch = 0.8979  SER_batch = 0.1021  BER_batch = 0.0666  loss: 0.260  lr: 0.0100\n",
            "iter:  270 / 500  Acc_batch = 0.8792  SER_batch = 0.1208  BER_batch = 0.0784  loss: 0.291  lr: 0.0100\n",
            "iter:  271 / 500  Acc_batch = 0.8844  SER_batch = 0.1156  BER_batch = 0.0747  loss: 0.287  lr: 0.0100\n",
            "iter:  272 / 500  Acc_batch = 0.8895  SER_batch = 0.1105  BER_batch = 0.0725  loss: 0.276  lr: 0.0100\n",
            "iter:  273 / 500  Acc_batch = 0.8903  SER_batch = 0.1097  BER_batch = 0.0698  loss: 0.281  lr: 0.0100\n",
            "iter:  274 / 500  Acc_batch = 0.8921  SER_batch = 0.1079  BER_batch = 0.0693  loss: 0.266  lr: 0.0100\n",
            "iter:  275 / 500  Acc_batch = 0.8758  SER_batch = 0.1242  BER_batch = 0.0804  loss: 0.316  lr: 0.0100\n",
            "iter:  276 / 500  Acc_batch = 0.8883  SER_batch = 0.1117  BER_batch = 0.0731  loss: 0.277  lr: 0.0100\n",
            "iter:  277 / 500  Acc_batch = 0.8916  SER_batch = 0.1084  BER_batch = 0.0682  loss: 0.269  lr: 0.0100\n",
            "iter:  278 / 500  Acc_batch = 0.8911  SER_batch = 0.1089  BER_batch = 0.0701  loss: 0.268  lr: 0.0100\n",
            "iter:  279 / 500  Acc_batch = 0.8846  SER_batch = 0.1154  BER_batch = 0.0762  loss: 0.282  lr: 0.0100\n",
            "iter:  280 / 500  Acc_batch = 0.8861  SER_batch = 0.1139  BER_batch = 0.0731  loss: 0.281  lr: 0.0100\n",
            "iter:  281 / 500  Acc_batch = 0.8830  SER_batch = 0.1170  BER_batch = 0.0750  loss: 0.285  lr: 0.0100\n",
            "iter:  282 / 500  Acc_batch = 0.8830  SER_batch = 0.1170  BER_batch = 0.0749  loss: 0.289  lr: 0.0100\n",
            "iter:  283 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0754  loss: 0.292  lr: 0.0100\n",
            "iter:  284 / 500  Acc_batch = 0.8949  SER_batch = 0.1051  BER_batch = 0.0669  loss: 0.267  lr: 0.0100\n",
            "iter:  285 / 500  Acc_batch = 0.8936  SER_batch = 0.1064  BER_batch = 0.0691  loss: 0.265  lr: 0.0100\n",
            "iter:  286 / 500  Acc_batch = 0.8936  SER_batch = 0.1064  BER_batch = 0.0684  loss: 0.273  lr: 0.0100\n",
            "iter:  287 / 500  Acc_batch = 0.8945  SER_batch = 0.1055  BER_batch = 0.0677  loss: 0.268  lr: 0.0100\n",
            "iter:  288 / 500  Acc_batch = 0.8927  SER_batch = 0.1073  BER_batch = 0.0705  loss: 0.271  lr: 0.0100\n",
            "iter:  289 / 500  Acc_batch = 0.8864  SER_batch = 0.1136  BER_batch = 0.0746  loss: 0.279  lr: 0.0100\n",
            "iter:  290 / 500  Acc_batch = 0.8810  SER_batch = 0.1190  BER_batch = 0.0767  loss: 0.296  lr: 0.0100\n",
            "iter:  291 / 500  Acc_batch = 0.8957  SER_batch = 0.1043  BER_batch = 0.0684  loss: 0.258  lr: 0.0100\n",
            "iter:  292 / 500  Acc_batch = 0.8944  SER_batch = 0.1056  BER_batch = 0.0676  loss: 0.265  lr: 0.0100\n",
            "iter:  293 / 500  Acc_batch = 0.8900  SER_batch = 0.1100  BER_batch = 0.0715  loss: 0.277  lr: 0.0100\n",
            "iter:  294 / 500  Acc_batch = 0.8882  SER_batch = 0.1118  BER_batch = 0.0711  loss: 0.282  lr: 0.0100\n",
            "iter:  295 / 500  Acc_batch = 0.8882  SER_batch = 0.1118  BER_batch = 0.0723  loss: 0.285  lr: 0.0100\n",
            "iter:  296 / 500  Acc_batch = 0.8936  SER_batch = 0.1064  BER_batch = 0.0680  loss: 0.270  lr: 0.0100\n",
            "iter:  297 / 500  Acc_batch = 0.8859  SER_batch = 0.1141  BER_batch = 0.0721  loss: 0.285  lr: 0.0100\n",
            "iter:  298 / 500  Acc_batch = 0.8825  SER_batch = 0.1175  BER_batch = 0.0765  loss: 0.299  lr: 0.0100\n",
            "iter:  299 / 500  Acc_batch = 0.8882  SER_batch = 0.1118  BER_batch = 0.0718  loss: 0.282  lr: 0.0100\n",
            "iter:  300 / 500  Acc_batch = 0.8880  SER_batch = 0.1120  BER_batch = 0.0735  loss: 0.272  lr: 0.0100\n",
            "iter:  301 / 500  Acc_batch = 0.8846  SER_batch = 0.1154  BER_batch = 0.0758  loss: 0.284  lr: 0.0100\n",
            "iter:  302 / 500  Acc_batch = 0.8934  SER_batch = 0.1066  BER_batch = 0.0685  loss: 0.267  lr: 0.0100\n",
            "iter:  303 / 500  Acc_batch = 0.8896  SER_batch = 0.1104  BER_batch = 0.0714  loss: 0.277  lr: 0.0100\n",
            "iter:  304 / 500  Acc_batch = 0.8875  SER_batch = 0.1125  BER_batch = 0.0734  loss: 0.290  lr: 0.0100\n",
            "iter:  305 / 500  Acc_batch = 0.8846  SER_batch = 0.1154  BER_batch = 0.0741  loss: 0.295  lr: 0.0100\n",
            "iter:  306 / 500  Acc_batch = 0.8888  SER_batch = 0.1112  BER_batch = 0.0716  loss: 0.270  lr: 0.0100\n",
            "iter:  307 / 500  Acc_batch = 0.8843  SER_batch = 0.1157  BER_batch = 0.0728  loss: 0.287  lr: 0.0100\n",
            "iter:  308 / 500  Acc_batch = 0.8963  SER_batch = 0.1037  BER_batch = 0.0676  loss: 0.260  lr: 0.0100\n",
            "iter:  309 / 500  Acc_batch = 0.8916  SER_batch = 0.1084  BER_batch = 0.0695  loss: 0.269  lr: 0.0100\n",
            "iter:  310 / 500  Acc_batch = 0.8903  SER_batch = 0.1097  BER_batch = 0.0722  loss: 0.271  lr: 0.0100\n",
            "iter:  311 / 500  Acc_batch = 0.8921  SER_batch = 0.1079  BER_batch = 0.0691  loss: 0.268  lr: 0.0100\n",
            "iter:  312 / 500  Acc_batch = 0.8901  SER_batch = 0.1099  BER_batch = 0.0691  loss: 0.265  lr: 0.0100\n",
            "iter:  313 / 500  Acc_batch = 0.8905  SER_batch = 0.1095  BER_batch = 0.0715  loss: 0.272  lr: 0.0100\n",
            "iter:  314 / 500  Acc_batch = 0.8880  SER_batch = 0.1120  BER_batch = 0.0734  loss: 0.275  lr: 0.0100\n",
            "iter:  315 / 500  Acc_batch = 0.8911  SER_batch = 0.1089  BER_batch = 0.0686  loss: 0.280  lr: 0.0100\n",
            "iter:  316 / 500  Acc_batch = 0.8853  SER_batch = 0.1147  BER_batch = 0.0732  loss: 0.284  lr: 0.0100\n",
            "iter:  317 / 500  Acc_batch = 0.8960  SER_batch = 0.1040  BER_batch = 0.0673  loss: 0.259  lr: 0.0100\n",
            "iter:  318 / 500  Acc_batch = 0.8921  SER_batch = 0.1079  BER_batch = 0.0687  loss: 0.266  lr: 0.0100\n",
            "iter:  319 / 500  Acc_batch = 0.8872  SER_batch = 0.1128  BER_batch = 0.0735  loss: 0.290  lr: 0.0100\n",
            "iter:  320 / 500  Acc_batch = 0.8874  SER_batch = 0.1126  BER_batch = 0.0733  loss: 0.285  lr: 0.0100\n",
            "iter:  321 / 500  Acc_batch = 0.8833  SER_batch = 0.1167  BER_batch = 0.0736  loss: 0.299  lr: 0.0100\n",
            "iter:  322 / 500  Acc_batch = 0.8918  SER_batch = 0.1082  BER_batch = 0.0683  loss: 0.264  lr: 0.0100\n",
            "iter:  323 / 500  Acc_batch = 0.8859  SER_batch = 0.1141  BER_batch = 0.0739  loss: 0.294  lr: 0.0100\n",
            "iter:  324 / 500  Acc_batch = 0.8914  SER_batch = 0.1086  BER_batch = 0.0715  loss: 0.279  lr: 0.0100\n",
            "iter:  325 / 500  Acc_batch = 0.8787  SER_batch = 0.1213  BER_batch = 0.0793  loss: 0.299  lr: 0.0100\n",
            "iter:  326 / 500  Acc_batch = 0.8830  SER_batch = 0.1170  BER_batch = 0.0768  loss: 0.283  lr: 0.0100\n",
            "iter:  327 / 500  Acc_batch = 0.8864  SER_batch = 0.1136  BER_batch = 0.0732  loss: 0.275  lr: 0.0100\n",
            "iter:  328 / 500  Acc_batch = 0.8817  SER_batch = 0.1183  BER_batch = 0.0758  loss: 0.291  lr: 0.0100\n",
            "iter:  329 / 500  Acc_batch = 0.8953  SER_batch = 0.1047  BER_batch = 0.0676  loss: 0.268  lr: 0.0100\n",
            "iter:  330 / 500  Acc_batch = 0.8818  SER_batch = 0.1182  BER_batch = 0.0758  loss: 0.292  lr: 0.0100\n",
            "iter:  331 / 500  Acc_batch = 0.8923  SER_batch = 0.1077  BER_batch = 0.0706  loss: 0.272  lr: 0.0100\n",
            "iter:  332 / 500  Acc_batch = 0.8913  SER_batch = 0.1087  BER_batch = 0.0697  loss: 0.270  lr: 0.0100\n",
            "iter:  333 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0754  loss: 0.283  lr: 0.0100\n",
            "iter:  334 / 500  Acc_batch = 0.8874  SER_batch = 0.1126  BER_batch = 0.0735  loss: 0.281  lr: 0.0100\n",
            "iter:  335 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0752  loss: 0.292  lr: 0.0100\n",
            "iter:  336 / 500  Acc_batch = 0.8804  SER_batch = 0.1196  BER_batch = 0.0791  loss: 0.295  lr: 0.0100\n",
            "iter:  337 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0738  loss: 0.273  lr: 0.0100\n",
            "iter:  338 / 500  Acc_batch = 0.8831  SER_batch = 0.1169  BER_batch = 0.0752  loss: 0.287  lr: 0.0100\n",
            "iter:  339 / 500  Acc_batch = 0.8918  SER_batch = 0.1082  BER_batch = 0.0699  loss: 0.266  lr: 0.0100\n",
            "iter:  340 / 500  Acc_batch = 0.8831  SER_batch = 0.1169  BER_batch = 0.0751  loss: 0.299  lr: 0.0100\n",
            "iter:  341 / 500  Acc_batch = 0.8932  SER_batch = 0.1068  BER_batch = 0.0697  loss: 0.266  lr: 0.0100\n",
            "iter:  342 / 500  Acc_batch = 0.8825  SER_batch = 0.1175  BER_batch = 0.0761  loss: 0.296  lr: 0.0100\n",
            "iter:  343 / 500  Acc_batch = 0.8913  SER_batch = 0.1087  BER_batch = 0.0697  loss: 0.265  lr: 0.0100\n",
            "iter:  344 / 500  Acc_batch = 0.8872  SER_batch = 0.1128  BER_batch = 0.0726  loss: 0.278  lr: 0.0100\n",
            "iter:  345 / 500  Acc_batch = 0.8955  SER_batch = 0.1045  BER_batch = 0.0683  loss: 0.264  lr: 0.0100\n",
            "iter:  346 / 500  Acc_batch = 0.8957  SER_batch = 0.1043  BER_batch = 0.0681  loss: 0.261  lr: 0.0100\n",
            "iter:  347 / 500  Acc_batch = 0.8804  SER_batch = 0.1196  BER_batch = 0.0770  loss: 0.285  lr: 0.0100\n",
            "iter:  348 / 500  Acc_batch = 0.8896  SER_batch = 0.1104  BER_batch = 0.0728  loss: 0.271  lr: 0.0100\n",
            "iter:  349 / 500  Acc_batch = 0.8856  SER_batch = 0.1144  BER_batch = 0.0735  loss: 0.278  lr: 0.0100\n",
            "iter:  350 / 500  Acc_batch = 0.8740  SER_batch = 0.1260  BER_batch = 0.0819  loss: 0.307  lr: 0.0100\n",
            "iter:  351 / 500  Acc_batch = 0.8932  SER_batch = 0.1068  BER_batch = 0.0699  loss: 0.276  lr: 0.0100\n",
            "iter:  352 / 500  Acc_batch = 0.8796  SER_batch = 0.1204  BER_batch = 0.0789  loss: 0.300  lr: 0.0100\n",
            "iter:  353 / 500  Acc_batch = 0.8903  SER_batch = 0.1097  BER_batch = 0.0711  loss: 0.268  lr: 0.0100\n",
            "iter:  354 / 500  Acc_batch = 0.8864  SER_batch = 0.1136  BER_batch = 0.0736  loss: 0.277  lr: 0.0100\n",
            "iter:  355 / 500  Acc_batch = 0.8916  SER_batch = 0.1084  BER_batch = 0.0704  loss: 0.267  lr: 0.0100\n",
            "iter:  356 / 500  Acc_batch = 0.8983  SER_batch = 0.1017  BER_batch = 0.0662  loss: 0.247  lr: 0.0100\n",
            "iter:  357 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0715  loss: 0.277  lr: 0.0100\n",
            "iter:  358 / 500  Acc_batch = 0.8923  SER_batch = 0.1077  BER_batch = 0.0690  loss: 0.266  lr: 0.0100\n",
            "iter:  359 / 500  Acc_batch = 0.8978  SER_batch = 0.1022  BER_batch = 0.0668  loss: 0.266  lr: 0.0100\n",
            "iter:  360 / 500  Acc_batch = 0.8966  SER_batch = 0.1034  BER_batch = 0.0684  loss: 0.267  lr: 0.0100\n",
            "iter:  361 / 500  Acc_batch = 0.8825  SER_batch = 0.1175  BER_batch = 0.0752  loss: 0.293  lr: 0.0100\n",
            "iter:  362 / 500  Acc_batch = 0.8913  SER_batch = 0.1087  BER_batch = 0.0723  loss: 0.280  lr: 0.0100\n",
            "iter:  363 / 500  Acc_batch = 0.8934  SER_batch = 0.1066  BER_batch = 0.0680  loss: 0.269  lr: 0.0100\n",
            "iter:  364 / 500  Acc_batch = 0.8804  SER_batch = 0.1196  BER_batch = 0.0764  loss: 0.286  lr: 0.0100\n",
            "iter:  365 / 500  Acc_batch = 0.8916  SER_batch = 0.1084  BER_batch = 0.0701  loss: 0.273  lr: 0.0100\n",
            "iter:  366 / 500  Acc_batch = 0.8900  SER_batch = 0.1100  BER_batch = 0.0723  loss: 0.273  lr: 0.0100\n",
            "iter:  367 / 500  Acc_batch = 0.8890  SER_batch = 0.1110  BER_batch = 0.0733  loss: 0.282  lr: 0.0100\n",
            "iter:  368 / 500  Acc_batch = 0.8905  SER_batch = 0.1095  BER_batch = 0.0689  loss: 0.274  lr: 0.0100\n",
            "iter:  369 / 500  Acc_batch = 0.8940  SER_batch = 0.1060  BER_batch = 0.0686  loss: 0.270  lr: 0.0100\n",
            "iter:  370 / 500  Acc_batch = 0.8963  SER_batch = 0.1037  BER_batch = 0.0667  loss: 0.273  lr: 0.0100\n",
            "iter:  371 / 500  Acc_batch = 0.8968  SER_batch = 0.1032  BER_batch = 0.0667  loss: 0.257  lr: 0.0100\n",
            "iter:  372 / 500  Acc_batch = 0.8903  SER_batch = 0.1097  BER_batch = 0.0717  loss: 0.276  lr: 0.0100\n",
            "iter:  373 / 500  Acc_batch = 0.8870  SER_batch = 0.1130  BER_batch = 0.0737  loss: 0.288  lr: 0.0100\n",
            "iter:  374 / 500  Acc_batch = 0.8911  SER_batch = 0.1089  BER_batch = 0.0709  loss: 0.282  lr: 0.0100\n",
            "iter:  375 / 500  Acc_batch = 0.8991  SER_batch = 0.1009  BER_batch = 0.0636  loss: 0.253  lr: 0.0100\n",
            "iter:  376 / 500  Acc_batch = 0.8831  SER_batch = 0.1169  BER_batch = 0.0759  loss: 0.291  lr: 0.0100\n",
            "iter:  377 / 500  Acc_batch = 0.8927  SER_batch = 0.1073  BER_batch = 0.0681  loss: 0.268  lr: 0.0100\n",
            "iter:  378 / 500  Acc_batch = 0.8958  SER_batch = 0.1042  BER_batch = 0.0658  loss: 0.259  lr: 0.0100\n",
            "iter:  379 / 500  Acc_batch = 0.8901  SER_batch = 0.1099  BER_batch = 0.0719  loss: 0.273  lr: 0.0100\n",
            "iter:  380 / 500  Acc_batch = 0.8866  SER_batch = 0.1134  BER_batch = 0.0732  loss: 0.280  lr: 0.0100\n",
            "iter:  381 / 500  Acc_batch = 0.8914  SER_batch = 0.1086  BER_batch = 0.0704  loss: 0.287  lr: 0.0100\n",
            "iter:  382 / 500  Acc_batch = 0.8887  SER_batch = 0.1113  BER_batch = 0.0734  loss: 0.279  lr: 0.0100\n",
            "iter:  383 / 500  Acc_batch = 0.8906  SER_batch = 0.1094  BER_batch = 0.0707  loss: 0.279  lr: 0.0100\n",
            "iter:  384 / 500  Acc_batch = 0.8885  SER_batch = 0.1115  BER_batch = 0.0723  loss: 0.286  lr: 0.0100\n",
            "iter:  385 / 500  Acc_batch = 0.8924  SER_batch = 0.1076  BER_batch = 0.0689  loss: 0.277  lr: 0.0100\n",
            "iter:  386 / 500  Acc_batch = 0.8926  SER_batch = 0.1074  BER_batch = 0.0705  loss: 0.271  lr: 0.0100\n",
            "iter:  387 / 500  Acc_batch = 0.8916  SER_batch = 0.1084  BER_batch = 0.0701  loss: 0.272  lr: 0.0100\n",
            "iter:  388 / 500  Acc_batch = 0.8849  SER_batch = 0.1151  BER_batch = 0.0750  loss: 0.280  lr: 0.0100\n",
            "iter:  389 / 500  Acc_batch = 0.8844  SER_batch = 0.1156  BER_batch = 0.0763  loss: 0.289  lr: 0.0100\n",
            "iter:  390 / 500  Acc_batch = 0.8908  SER_batch = 0.1092  BER_batch = 0.0689  loss: 0.284  lr: 0.0100\n",
            "iter:  391 / 500  Acc_batch = 0.8910  SER_batch = 0.1090  BER_batch = 0.0699  loss: 0.277  lr: 0.0100\n",
            "iter:  392 / 500  Acc_batch = 0.9051  SER_batch = 0.0949  BER_batch = 0.0622  loss: 0.255  lr: 0.0100\n",
            "iter:  393 / 500  Acc_batch = 0.8818  SER_batch = 0.1182  BER_batch = 0.0745  loss: 0.295  lr: 0.0100\n",
            "iter:  394 / 500  Acc_batch = 0.9006  SER_batch = 0.0994  BER_batch = 0.0655  loss: 0.263  lr: 0.0100\n",
            "iter:  395 / 500  Acc_batch = 0.8838  SER_batch = 0.1162  BER_batch = 0.0763  loss: 0.281  lr: 0.0100\n",
            "iter:  396 / 500  Acc_batch = 0.8905  SER_batch = 0.1095  BER_batch = 0.0704  loss: 0.274  lr: 0.0100\n",
            "iter:  397 / 500  Acc_batch = 0.8796  SER_batch = 0.1204  BER_batch = 0.0788  loss: 0.293  lr: 0.0100\n",
            "iter:  398 / 500  Acc_batch = 0.8890  SER_batch = 0.1110  BER_batch = 0.0720  loss: 0.265  lr: 0.0100\n",
            "iter:  399 / 500  Acc_batch = 0.8883  SER_batch = 0.1117  BER_batch = 0.0730  loss: 0.270  lr: 0.0100\n",
            "iter:  400 / 500  Acc_batch = 0.8836  SER_batch = 0.1164  BER_batch = 0.0759  loss: 0.292  lr: 0.0100\n",
            "iter:  401 / 500  Acc_batch = 0.8914  SER_batch = 0.1086  BER_batch = 0.0684  loss: 0.269  lr: 0.0100\n",
            "iter:  402 / 500  Acc_batch = 0.8833  SER_batch = 0.1167  BER_batch = 0.0763  loss: 0.283  lr: 0.0100\n",
            "iter:  403 / 500  Acc_batch = 0.8877  SER_batch = 0.1123  BER_batch = 0.0728  loss: 0.273  lr: 0.0100\n",
            "iter:  404 / 500  Acc_batch = 0.8945  SER_batch = 0.1055  BER_batch = 0.0682  loss: 0.263  lr: 0.0100\n",
            "iter:  405 / 500  Acc_batch = 0.8875  SER_batch = 0.1125  BER_batch = 0.0729  loss: 0.275  lr: 0.0100\n",
            "iter:  406 / 500  Acc_batch = 0.8945  SER_batch = 0.1055  BER_batch = 0.0681  loss: 0.273  lr: 0.0100\n",
            "iter:  407 / 500  Acc_batch = 0.8926  SER_batch = 0.1074  BER_batch = 0.0697  loss: 0.276  lr: 0.0100\n",
            "iter:  408 / 500  Acc_batch = 0.8950  SER_batch = 0.1050  BER_batch = 0.0681  loss: 0.261  lr: 0.0100\n",
            "iter:  409 / 500  Acc_batch = 0.8757  SER_batch = 0.1243  BER_batch = 0.0807  loss: 0.288  lr: 0.0100\n",
            "iter:  410 / 500  Acc_batch = 0.8872  SER_batch = 0.1128  BER_batch = 0.0743  loss: 0.273  lr: 0.0100\n",
            "iter:  411 / 500  Acc_batch = 0.8898  SER_batch = 0.1102  BER_batch = 0.0724  loss: 0.278  lr: 0.0100\n",
            "iter:  412 / 500  Acc_batch = 0.8817  SER_batch = 0.1183  BER_batch = 0.0765  loss: 0.303  lr: 0.0100\n",
            "iter:  413 / 500  Acc_batch = 0.8997  SER_batch = 0.1003  BER_batch = 0.0632  loss: 0.258  lr: 0.0100\n",
            "iter:  414 / 500  Acc_batch = 0.8872  SER_batch = 0.1128  BER_batch = 0.0731  loss: 0.282  lr: 0.0100\n",
            "iter:  415 / 500  Acc_batch = 0.8949  SER_batch = 0.1051  BER_batch = 0.0679  loss: 0.266  lr: 0.0100\n",
            "iter:  416 / 500  Acc_batch = 0.8939  SER_batch = 0.1061  BER_batch = 0.0687  loss: 0.272  lr: 0.0100\n",
            "iter:  417 / 500  Acc_batch = 0.8905  SER_batch = 0.1095  BER_batch = 0.0720  loss: 0.275  lr: 0.0100\n",
            "iter:  418 / 500  Acc_batch = 0.8968  SER_batch = 0.1032  BER_batch = 0.0671  loss: 0.258  lr: 0.0100\n",
            "iter:  419 / 500  Acc_batch = 0.8804  SER_batch = 0.1196  BER_batch = 0.0789  loss: 0.296  lr: 0.0100\n",
            "iter:  420 / 500  Acc_batch = 0.8825  SER_batch = 0.1175  BER_batch = 0.0771  loss: 0.290  lr: 0.0100\n",
            "iter:  421 / 500  Acc_batch = 0.8874  SER_batch = 0.1126  BER_batch = 0.0735  loss: 0.280  lr: 0.0100\n",
            "iter:  422 / 500  Acc_batch = 0.8900  SER_batch = 0.1100  BER_batch = 0.0718  loss: 0.270  lr: 0.0100\n",
            "iter:  423 / 500  Acc_batch = 0.8856  SER_batch = 0.1144  BER_batch = 0.0736  loss: 0.276  lr: 0.0100\n",
            "iter:  424 / 500  Acc_batch = 0.8867  SER_batch = 0.1133  BER_batch = 0.0736  loss: 0.282  lr: 0.0100\n",
            "iter:  425 / 500  Acc_batch = 0.8872  SER_batch = 0.1128  BER_batch = 0.0715  loss: 0.277  lr: 0.0100\n",
            "iter:  426 / 500  Acc_batch = 0.8898  SER_batch = 0.1102  BER_batch = 0.0719  loss: 0.273  lr: 0.0100\n",
            "iter:  427 / 500  Acc_batch = 0.8856  SER_batch = 0.1144  BER_batch = 0.0743  loss: 0.284  lr: 0.0100\n",
            "iter:  428 / 500  Acc_batch = 0.8879  SER_batch = 0.1121  BER_batch = 0.0751  loss: 0.279  lr: 0.0100\n",
            "iter:  429 / 500  Acc_batch = 0.8887  SER_batch = 0.1113  BER_batch = 0.0701  loss: 0.282  lr: 0.0100\n",
            "iter:  430 / 500  Acc_batch = 0.8942  SER_batch = 0.1058  BER_batch = 0.0678  loss: 0.276  lr: 0.0100\n",
            "iter:  431 / 500  Acc_batch = 0.8869  SER_batch = 0.1131  BER_batch = 0.0737  loss: 0.276  lr: 0.0100\n",
            "iter:  432 / 500  Acc_batch = 0.8851  SER_batch = 0.1149  BER_batch = 0.0743  loss: 0.286  lr: 0.0100\n",
            "iter:  433 / 500  Acc_batch = 0.8911  SER_batch = 0.1089  BER_batch = 0.0695  loss: 0.274  lr: 0.0100\n",
            "iter:  434 / 500  Acc_batch = 0.9004  SER_batch = 0.0996  BER_batch = 0.0642  loss: 0.249  lr: 0.0100\n",
            "iter:  435 / 500  Acc_batch = 0.9004  SER_batch = 0.0996  BER_batch = 0.0653  loss: 0.259  lr: 0.0100\n",
            "iter:  436 / 500  Acc_batch = 0.8888  SER_batch = 0.1112  BER_batch = 0.0726  loss: 0.282  lr: 0.0100\n",
            "iter:  437 / 500  Acc_batch = 0.8916  SER_batch = 0.1084  BER_batch = 0.0703  loss: 0.256  lr: 0.0100\n",
            "iter:  438 / 500  Acc_batch = 0.8975  SER_batch = 0.1025  BER_batch = 0.0662  loss: 0.265  lr: 0.0100\n",
            "iter:  439 / 500  Acc_batch = 0.8936  SER_batch = 0.1064  BER_batch = 0.0678  loss: 0.274  lr: 0.0100\n",
            "iter:  440 / 500  Acc_batch = 0.8789  SER_batch = 0.1211  BER_batch = 0.0776  loss: 0.300  lr: 0.0100\n",
            "iter:  441 / 500  Acc_batch = 0.8890  SER_batch = 0.1110  BER_batch = 0.0715  loss: 0.276  lr: 0.0100\n",
            "iter:  442 / 500  Acc_batch = 0.8802  SER_batch = 0.1198  BER_batch = 0.0775  loss: 0.292  lr: 0.0100\n",
            "iter:  443 / 500  Acc_batch = 0.8978  SER_batch = 0.1022  BER_batch = 0.0653  loss: 0.259  lr: 0.0100\n",
            "iter:  444 / 500  Acc_batch = 0.8835  SER_batch = 0.1165  BER_batch = 0.0769  loss: 0.288  lr: 0.0100\n",
            "iter:  445 / 500  Acc_batch = 0.8991  SER_batch = 0.1009  BER_batch = 0.0662  loss: 0.252  lr: 0.0100\n",
            "iter:  446 / 500  Acc_batch = 0.8875  SER_batch = 0.1125  BER_batch = 0.0741  loss: 0.276  lr: 0.0100\n",
            "iter:  447 / 500  Acc_batch = 0.8926  SER_batch = 0.1074  BER_batch = 0.0708  loss: 0.264  lr: 0.0100\n",
            "iter:  448 / 500  Acc_batch = 0.9010  SER_batch = 0.0990  BER_batch = 0.0648  loss: 0.244  lr: 0.0100\n",
            "iter:  449 / 500  Acc_batch = 0.8910  SER_batch = 0.1090  BER_batch = 0.0701  loss: 0.277  lr: 0.0100\n",
            "iter:  450 / 500  Acc_batch = 0.8879  SER_batch = 0.1121  BER_batch = 0.0732  loss: 0.285  lr: 0.0100\n",
            "iter:  451 / 500  Acc_batch = 0.8905  SER_batch = 0.1095  BER_batch = 0.0723  loss: 0.278  lr: 0.0100\n",
            "iter:  452 / 500  Acc_batch = 0.8901  SER_batch = 0.1099  BER_batch = 0.0714  loss: 0.275  lr: 0.0100\n",
            "iter:  453 / 500  Acc_batch = 0.8866  SER_batch = 0.1134  BER_batch = 0.0730  loss: 0.275  lr: 0.0100\n",
            "iter:  454 / 500  Acc_batch = 0.8815  SER_batch = 0.1185  BER_batch = 0.0787  loss: 0.294  lr: 0.0100\n",
            "iter:  455 / 500  Acc_batch = 0.8916  SER_batch = 0.1084  BER_batch = 0.0684  loss: 0.275  lr: 0.0100\n",
            "iter:  456 / 500  Acc_batch = 0.8877  SER_batch = 0.1123  BER_batch = 0.0732  loss: 0.275  lr: 0.0100\n",
            "iter:  457 / 500  Acc_batch = 0.8971  SER_batch = 0.1029  BER_batch = 0.0669  loss: 0.257  lr: 0.0100\n",
            "iter:  458 / 500  Acc_batch = 0.8923  SER_batch = 0.1077  BER_batch = 0.0700  loss: 0.279  lr: 0.0100\n",
            "iter:  459 / 500  Acc_batch = 0.8892  SER_batch = 0.1108  BER_batch = 0.0725  loss: 0.271  lr: 0.0100\n",
            "iter:  460 / 500  Acc_batch = 0.8903  SER_batch = 0.1097  BER_batch = 0.0731  loss: 0.272  lr: 0.0100\n",
            "iter:  461 / 500  Acc_batch = 0.9001  SER_batch = 0.0999  BER_batch = 0.0641  loss: 0.252  lr: 0.0100\n",
            "iter:  462 / 500  Acc_batch = 0.8885  SER_batch = 0.1115  BER_batch = 0.0721  loss: 0.270  lr: 0.0100\n",
            "iter:  463 / 500  Acc_batch = 0.8934  SER_batch = 0.1066  BER_batch = 0.0683  loss: 0.272  lr: 0.0100\n",
            "iter:  464 / 500  Acc_batch = 0.8831  SER_batch = 0.1169  BER_batch = 0.0754  loss: 0.287  lr: 0.0100\n",
            "iter:  465 / 500  Acc_batch = 0.8836  SER_batch = 0.1164  BER_batch = 0.0762  loss: 0.284  lr: 0.0100\n",
            "iter:  466 / 500  Acc_batch = 0.8900  SER_batch = 0.1100  BER_batch = 0.0717  loss: 0.268  lr: 0.0100\n",
            "iter:  467 / 500  Acc_batch = 0.8882  SER_batch = 0.1118  BER_batch = 0.0734  loss: 0.281  lr: 0.0100\n",
            "iter:  468 / 500  Acc_batch = 0.8861  SER_batch = 0.1139  BER_batch = 0.0743  loss: 0.278  lr: 0.0100\n",
            "iter:  469 / 500  Acc_batch = 0.8973  SER_batch = 0.1027  BER_batch = 0.0661  loss: 0.264  lr: 0.0100\n",
            "iter:  470 / 500  Acc_batch = 0.8809  SER_batch = 0.1191  BER_batch = 0.0776  loss: 0.303  lr: 0.0100\n",
            "iter:  471 / 500  Acc_batch = 0.8955  SER_batch = 0.1045  BER_batch = 0.0662  loss: 0.266  lr: 0.0100\n",
            "iter:  472 / 500  Acc_batch = 0.8836  SER_batch = 0.1164  BER_batch = 0.0749  loss: 0.284  lr: 0.0100\n",
            "iter:  473 / 500  Acc_batch = 0.8787  SER_batch = 0.1213  BER_batch = 0.0789  loss: 0.301  lr: 0.0100\n",
            "iter:  474 / 500  Acc_batch = 0.8856  SER_batch = 0.1144  BER_batch = 0.0744  loss: 0.293  lr: 0.0100\n",
            "iter:  475 / 500  Acc_batch = 0.8848  SER_batch = 0.1152  BER_batch = 0.0774  loss: 0.291  lr: 0.0100\n",
            "iter:  476 / 500  Acc_batch = 0.8853  SER_batch = 0.1147  BER_batch = 0.0733  loss: 0.290  lr: 0.0100\n",
            "iter:  477 / 500  Acc_batch = 0.8836  SER_batch = 0.1164  BER_batch = 0.0735  loss: 0.287  lr: 0.0100\n",
            "iter:  478 / 500  Acc_batch = 0.8975  SER_batch = 0.1025  BER_batch = 0.0659  loss: 0.264  lr: 0.0100\n",
            "iter:  479 / 500  Acc_batch = 0.8973  SER_batch = 0.1027  BER_batch = 0.0663  loss: 0.257  lr: 0.0100\n",
            "iter:  480 / 500  Acc_batch = 0.8856  SER_batch = 0.1144  BER_batch = 0.0764  loss: 0.277  lr: 0.0100\n",
            "iter:  481 / 500  Acc_batch = 0.8940  SER_batch = 0.1060  BER_batch = 0.0693  loss: 0.265  lr: 0.0100\n",
            "iter:  482 / 500  Acc_batch = 0.9006  SER_batch = 0.0994  BER_batch = 0.0656  loss: 0.252  lr: 0.0100\n",
            "iter:  483 / 500  Acc_batch = 0.8937  SER_batch = 0.1063  BER_batch = 0.0693  loss: 0.261  lr: 0.0100\n",
            "iter:  484 / 500  Acc_batch = 0.8950  SER_batch = 0.1050  BER_batch = 0.0687  loss: 0.261  lr: 0.0100\n",
            "iter:  485 / 500  Acc_batch = 0.8931  SER_batch = 0.1069  BER_batch = 0.0697  loss: 0.270  lr: 0.0100\n",
            "iter:  486 / 500  Acc_batch = 0.8882  SER_batch = 0.1118  BER_batch = 0.0723  loss: 0.289  lr: 0.0100\n",
            "iter:  487 / 500  Acc_batch = 0.8914  SER_batch = 0.1086  BER_batch = 0.0719  loss: 0.280  lr: 0.0100\n",
            "iter:  488 / 500  Acc_batch = 0.8953  SER_batch = 0.1047  BER_batch = 0.0667  loss: 0.272  lr: 0.0100\n",
            "iter:  489 / 500  Acc_batch = 0.8927  SER_batch = 0.1073  BER_batch = 0.0696  loss: 0.270  lr: 0.0100\n",
            "iter:  490 / 500  Acc_batch = 0.8898  SER_batch = 0.1102  BER_batch = 0.0704  loss: 0.268  lr: 0.0100\n",
            "iter:  491 / 500  Acc_batch = 0.8901  SER_batch = 0.1099  BER_batch = 0.0716  loss: 0.273  lr: 0.0100\n",
            "iter:  492 / 500  Acc_batch = 0.9015  SER_batch = 0.0985  BER_batch = 0.0658  loss: 0.253  lr: 0.0100\n",
            "iter:  493 / 500  Acc_batch = 0.8838  SER_batch = 0.1162  BER_batch = 0.0757  loss: 0.292  lr: 0.0100\n",
            "iter:  494 / 500  Acc_batch = 0.8843  SER_batch = 0.1157  BER_batch = 0.0752  loss: 0.292  lr: 0.0100\n",
            "iter:  495 / 500  Acc_batch = 0.8892  SER_batch = 0.1108  BER_batch = 0.0728  loss: 0.268  lr: 0.0100\n",
            "iter:  496 / 500  Acc_batch = 0.8929  SER_batch = 0.1071  BER_batch = 0.0695  loss: 0.267  lr: 0.0100\n",
            "iter:  497 / 500  Acc_batch = 0.8957  SER_batch = 0.1043  BER_batch = 0.0694  loss: 0.267  lr: 0.0100\n",
            "iter:  498 / 500  Acc_batch = 0.8900  SER_batch = 0.1100  BER_batch = 0.0724  loss: 0.270  lr: 0.0100\n",
            "iter:  499 / 500  Acc_batch = 0.8936  SER_batch = 0.1064  BER_batch = 0.0677  loss: 0.266  lr: 0.0100\n",
            "iter:  500 / 500  Acc_batch = 0.8888  SER_batch = 0.1112  BER_batch = 0.0714  loss: 0.279  lr: 0.0100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wpxsX1p7Ecr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b5695752-3a3f-4de3-fc36-b6188cb3fd77"
      },
      "source": [
        "#---Testing\n",
        "# is_fading = True\n",
        "# is_noise  = True\n",
        "EbN = np.arange(0, 15+1)\n",
        "\n",
        "accuracy_all = np.zeros_like(EbN ,dtype=np.float64)\n",
        "ber_all = np.zeros_like(EbN ,dtype=np.float64)\n",
        "test_samples = 10000\n",
        "steps = 10 # total_samples = step * test_samples\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, ebn in enumerate(EbN):\n",
        "\n",
        "        Nerr_per_user_ebn = np.zeros([J])\n",
        "        Nbits = 0 # number of bits processed till now\n",
        "        accuracy_ebn = []\n",
        "        wrong_symbols_ebn = []\n",
        "        ber_ebn = []\n",
        "        wrong_bits_ebn = []\n",
        "\n",
        "        # steps = test_samples // 10000\n",
        "        step = 1\n",
        "\n",
        "        # while(step <= steps):\n",
        "        while( (Nerr_per_user_ebn.min() < maxNumErrs) and (Nbits < maxNumBits) ):\n",
        "        \n",
        "            step = step + 1\n",
        "\n",
        "            snr = ebn + 10*np.log10(np.log2(M)*J/K)\n",
        "\n",
        "            codebook = get_codebook_from_condensed_codebook()\n",
        "\n",
        "            #---Generate symbols and channel parameters\n",
        "            symbols = np.random.randint(1, M+1, (test_samples, J))\n",
        "            symbols_one_hot = torch.Tensor(enc.transform(symbols).toarray().reshape((test_samples, J, M))).to(device) # [test_samples, J, M]\n",
        "\n",
        "            #---Fading\n",
        "            if is_fading is True:\n",
        "                h = (torch.randn(test_samples, J, 2*K)/torch.sqrt(torch.tensor(2.0))).to(device)\n",
        "            else:\n",
        "                h = torch.zeros(test_samples, J, 2*K).to(device)\n",
        "                real_idx = 2 * torch.arange(K).long()\n",
        "                h[:, :, real_idx] = 1.0\n",
        "                \n",
        "            #---Encode\n",
        "            codewords_faded, codewords_faded_without_h = encode(symbols, codebook, h = h)    \n",
        "            received_signal = codewords_faded\n",
        "            \n",
        "            #---Noise\n",
        "            '''noise is being calc using codewords_faded_without_h '''\n",
        "            noise, sigma_square = add_noise(codewords_faded_without_h, J, snr)\n",
        "            \n",
        "            if is_noise is True:\n",
        "                received_signal = received_signal + noise\n",
        "            \n",
        "\n",
        "            #---Decode\n",
        "            beta = torch.log((1/(2* math.pi * sigma_square)**0.5))\n",
        "            gamma = torch.log(torch.tensor((1.0/M), device=device, dtype=torch.float32)) # log(p_x{j})\n",
        "            decoded_symbols_one_hot = mpa.forward(received_signal, codebook, h, sigma_square, beta, gamma)\n",
        "            \n",
        "            \n",
        "            #---Compute metrics\n",
        "            decoded_symbols = torch.argmax(decoded_symbols_one_hot, dim=-1) + 1\n",
        "            accuracy_batch = np.mean(symbols == decoded_symbols.cpu().data.numpy())\n",
        "            wrong_symbols_batch = (symbols != decoded_symbols.cpu().data.numpy()).sum()\n",
        "            ber_batch, Nerr_per_user = compute_ber(symbols.reshape(test_samples*J), decoded_symbols.cpu().data.numpy().reshape(test_samples*J), M)\n",
        "            wrong_bits_batch = Nerr_per_user.sum()\n",
        "\n",
        "            accuracy_ebn.append(accuracy_batch)\n",
        "            wrong_symbols_ebn.append(wrong_symbols_batch)\n",
        "            ber_ebn.append(ber_batch)\n",
        "            wrong_bits_ebn.append(wrong_bits_batch)\n",
        "\n",
        "            Nerr_per_user_ebn += Nerr_per_user\n",
        "            Nbits += test_samples * np.log2(M)\n",
        "\n",
        "        accuracy = np.array(accuracy_ebn).mean()\n",
        "        wrong_symbols = np.array(wrong_symbols_ebn).sum()\n",
        "        ber = np.array(ber_ebn).mean()\n",
        "        wrong_bits = np.array(wrong_bits_ebn).sum()\n",
        "        accuracy_all[i] = accuracy\n",
        "        ber_all[i] = ber\n",
        "\n",
        "        print(\"EbN:\", ebn, ' Acc: {:.8f}'.format(accuracy),\n",
        "            ' SER: {:.8f}'.format(1-accuracy),'BER: {:.8f}'.format(ber),\n",
        "            ' Wrong Symbols: ', wrong_symbols, '/', test_samples*(step-1)*J,\n",
        "            ' Wrong Bits: ', wrong_bits, '/', test_samples*(step-1)*J*np.log2(M))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EbN: 0  Acc: 0.67876667  SER: 0.32123333 BER: 0.20923333  Wrong Symbols:  19274 / 60000  Wrong Bits:  25108 / 120000.0\n",
            "EbN: 1  Acc: 0.71118333  SER: 0.28881667 BER: 0.18860000  Wrong Symbols:  17329 / 60000  Wrong Bits:  22632 / 120000.0\n",
            "EbN: 2  Acc: 0.74675000  SER: 0.25325000 BER: 0.16540833  Wrong Symbols:  15195 / 60000  Wrong Bits:  19849 / 120000.0\n",
            "EbN: 3  Acc: 0.77961667  SER: 0.22038333 BER: 0.14300000  Wrong Symbols:  13223 / 60000  Wrong Bits:  17160 / 120000.0\n",
            "EbN: 4  Acc: 0.82216667  SER: 0.17783333 BER: 0.11560000  Wrong Symbols:  10670 / 60000  Wrong Bits:  13872 / 120000.0\n",
            "EbN: 5  Acc: 0.85798333  SER: 0.14201667 BER: 0.09203333  Wrong Symbols:  8521 / 60000  Wrong Bits:  11044 / 120000.0\n",
            "EbN: 6  Acc: 0.89105000  SER: 0.10895000 BER: 0.07072500  Wrong Symbols:  6537 / 60000  Wrong Bits:  8487 / 120000.0\n",
            "EbN: 7  Acc: 0.91960000  SER: 0.08040000 BER: 0.05238333  Wrong Symbols:  4824 / 60000  Wrong Bits:  6286 / 120000.0\n",
            "EbN: 8  Acc: 0.94236667  SER: 0.05763333 BER: 0.03748333  Wrong Symbols:  3458 / 60000  Wrong Bits:  4498 / 120000.0\n",
            "EbN: 9  Acc: 0.95883333  SER: 0.04116667 BER: 0.02675833  Wrong Symbols:  2470 / 60000  Wrong Bits:  3211 / 120000.0\n",
            "EbN: 10  Acc: 0.97468333  SER: 0.02531667 BER: 0.01642500  Wrong Symbols:  1519 / 60000  Wrong Bits:  1971 / 120000.0\n",
            "EbN: 11  Acc: 0.98426667  SER: 0.01573333 BER: 0.01020833  Wrong Symbols:  944 / 60000  Wrong Bits:  1225 / 120000.0\n",
            "EbN: 12  Acc: 0.99050000  SER: 0.00950000 BER: 0.00624167  Wrong Symbols:  570 / 60000  Wrong Bits:  749 / 120000.0\n",
            "EbN: 13  Acc: 0.99395833  SER: 0.00604167 BER: 0.00392917  Wrong Symbols:  725 / 120000  Wrong Bits:  943 / 240000.0\n",
            "EbN: 14  Acc: 0.99683889  SER: 0.00316111 BER: 0.00208333  Wrong Symbols:  569 / 180000  Wrong Bits:  750 / 360000.0\n",
            "EbN: 15  Acc: 0.99794667  SER: 0.00205333 BER: 0.00133333  Wrong Symbols:  616 / 300000  Wrong Bits:  800 / 600000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXu7YuaIpyVF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "7ff5046b-464e-467e-8237-ddc7e556e064"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 11 04:56:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   78C    P0    46W /  70W |   1221MiB / 15079MiB |     48%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcK9bak8wTqB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "a6f05e05-f673-4127-feca-0c621aee9e6b"
      },
      "source": [
        "#---Plot Metrics across different channels\n",
        "ymin = 1e-3\n",
        "ymax = 1e-0\n",
        "xmin = EbN[0]\n",
        "xmax = EbN[-1]\n",
        "\n",
        "\n",
        "f1 = plt.figure()\n",
        "f2 = plt.figure()\n",
        "\n",
        "ax1 = f1.add_subplot(111)\n",
        "ax1.set_title('SER vs EbN')\n",
        "ax1.set_yscale('log')\n",
        "ax1.set_ylim([ymin,ymax])\n",
        "# ax1.set_xlim([xmin,xmax])\n",
        "ax1.set_xticks(EbN)\n",
        "ax1.grid(which=\"both\")\n",
        "ax1.plot(EbN, 1-accuracy_all, '-or')\n",
        "\n",
        "ax2 = f2.add_subplot(111)\n",
        "ax2.set_title('BER vs EbN')\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_ylim([ymin,ymax])\n",
        "# ax1.set_xlim([xmin,xmax])\n",
        "ax2.set_xticks(EbN)\n",
        "ax2.grid(which=\"both\")\n",
        "ax2.plot(EbN, ber_all, '-or')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgU5dX38e9hWAcVUVlkGwQNiqgIbvExorgBiegT8yYRlKCgr1Hjnicq7glPjPJGTdQksrmgkmiiwS0qcYtGVCBqUAQ3QJBF4oKKog7n/eNuZJiNma6urpqu3+e66mK6eubXZ2A4XXPXXXeZuyMiIqWvWdIFiIhIcajhi4hkhBq+iEhGqOGLiGSEGr6ISEao4YuIZIQavkjCzGyRmR2adB1S+tTwpckxswPM7J9m9pGZvW9mz5jZ3rnnRptZpZl9Um3rknt+kZl9ltu3wsxuNrMtYq633po287UHmZmb2Y3V9j9tZqNjK1pKkhq+NClmthVwP/BbYBugK3A5sK7Kpz3r7ltU296t8vyR7r4F0B/YE7igCKVvrqb6fAocb2Y94ytPskANX5qabwC4+53uXunun7n7I+7+cmOD3H0F8DCh8ddgZj8ws9nV9p1tZjNyHw8zs1fN7GMzW2Zm5zX+2/na3rmsD8xsqpm1rvLch8DNwKUR8kXU8KXJWQhUmtktZjbUzNrnG2Rm3YChwBt1fMp9QB8z26nKvhHAHbmPJwP/1923BPoBj+VbCzASOALoTXhTu6ja8+OBY8ysT4TXkIxTw5cmxd3XAAcADkwE3jOzGWbWqcqn7WdmH1bZ3qwWc6+ZfQy8A6yijiNnd18L/BU4FiDX+HcGZuQ+5Uugr5lt5e4fuPvcekrfXE3Xu/s77v4+obkfW62WFcDvgSvqeQ2ReqnhS5Pj7vPdfbS7dyMcWXcBrq3yKbPcfesqW+9qEUfnjsoPIjTw7ep5uTvY2HxHAPfm3ggAjgGGAYvN7Ekz+2Y9OZur6Z0qHy/OfU/V/Qo4wsz2qOd1ROqkhi9Nmru/Rhjf7pfH1z6Z+9oJ9Xzao0AHM+tPaPwbhnNw9xfc/SigI3Av8KfG1lBF9yof9wBqnNB19/8Q3th+HuF1JMPU8KVJMbOdzezc3Pg7Ztad0Ihn5Rl5LXBYXUfN7v4lcBdwNWFW0KO5121pZiPNrF3uc9YA6/OsAeA0M+tmZtsA44A/1vF5vwb2B3aJ8FqSUWr40tR8DOwLPGdmnxIa/Tzg3Cqf881a5rzvXVuYu78H3ApcUs9r3gEcCtzl7l9V2X88sMjM1gCnEE681mVzNd0BPAK8BbwJ/KKOetcAVxHefEQaxXQDFBGRbNARvohIRjQv1guZWVvgRuAL4Al3v71Yry0iIhGP8M1sipmtMrN51fYPMbMFZvaGmZ2f2/1d4G53PwkYHuV1RUSk8aIO6dwMDKm6w8zKgBsIVzD2BY41s75ANzbONa6M+LoiItJIkYZ03P2pWhZ02gd4w93fAjCz6cBRwFJC03+Ret5ozOxk4GSANm3aDOzevXtdn1qv9evX06xZ4U5RFDovjsys5cWRmfa8ODLTnhdHZqnnLVy4cLW7d6jxhLtH2oCewLwqj78HTKry+HjgeqAtMBX4HTCyIdkDBw70fD3++ON5f20x8uLIzFpeHJlpz4sjM+15cWSWeh4w22vpqUU7aevunwInFOv1RERkU3FMy1zGppeJd8vtExGRBEW+8Co3hn+/u/fLPW5OWML2EEKjfwEY4e6vNCLzSODIrl27njRt2rS86vrkk0/YYovC3cio0HlxZGYtL47MtOfFkZn2vDgySz3v4IMPnuPue9V4orZxnoZuwJ3AcsIysUuBMbn9wwhN/01gXL75GsNXXrEz054XR2ba8+LILPU84hjDd/dj69j/IPBglGwRESmsVK6loyEd5SWVmfa8ODLTnhdHZqnnxTKkE/emIR3lFTsz7XlxZKY9L47MUs+jjiEdLZ4mIpIRavgiIhmhhi8ikhE6aZtQXhyZWcuLIzPteXFkpj0vjsxSz9NJ24h0Iip9eXFkpj0vjsy058WRWep56KStiEi2qeGLiGSExvATyosjM2t5cWSmPS+OzLTnxZFZ6nkaw49I45Lpy4sjM+15cWSmPS+OzFLPQ2P4IiLZpoYvIpIRavgiIhmhhi8ikhGapZNQXhyZWcuLIzPteXFkpj0vjsxSz9MsnYg08yB9eXFkpj0vjsy058WRWep5aJaOiEi2qeGLiGSEGr6ISEao4YuIZIQavohIRmhaZkJ5cWRmLS+OzLTnxZGZ9rw4Mks9T9MyI9JUs/TlxZGZ9rw4MtOeF0dmqeehaZkiItmmhi8ikhGl1/Bvvx169mTQ4MHQs2d4LCIiNE+6gIK6/XY4+WRYuxYDWLw4PAYYOTLJykREEldaR/jjxsHatZvuW7sWzj8fUjgbSUSkmEqr4S9ZUvv+pUuhQwc48ED48Y/ht7+Fv/8dli/f/BuBhohEpESU1pBOjx5hGKe69u3hu9+FV1+F6dPhww83fa5v343brruGP7t0gTvu0BCRiJSMkrrwquPMmfSZMIGydeu+3lfZqhULzjuPVYceGna40/L99ylftIi2ixdTvngxbRctou2iRbRYs+brr/uqbVuarVtHs6++qvE6n3fqxKzp0/P+/jZI28UaTS0vjsy058WRmfa8ODJLPS87F15Nm+ZeUeHrzdwrKsLjhli/3n3lSvfHH3e//nr3U091DwM+tW9XXun+97+7f/BB42vMSdvFGk0tL47MtOfFkZn2vDgySz2POi68Kq0hHQhDLSNH8uQTT3DQQQc1/OvMoGPHsG34ugceqH2IqHnzcCJ4g512gr322rgNGAAFPsIREYmqtE7aFtr48VBevum+8nK4+WZYvRoefjh8Tr9+8PTTcO65MGgQbLVVOBfwox+FE8TPPguffbYxQyeCRSQBpXeEX0gbTsyOG4cvWYL16BEa/Ib9hx8etg1WrIA5c2D27LA9/DDcemt4rqwsvDFsvXV4A/jiC50IFpGiUsPfnMYMEXXuDN/+dtggjPYvW7bxDWD2bHj0UVi/ftOvW7sWzjoL9t8/HPGbxfGdiEjGqeHHyQy6dQvb0UeHfc3qGEVbvRp69QrnEPbdd+O2997Qrl3xahaRkqUx/GLr0aP2/Z07w403wtChsHAhXHQRHHZYuE5g113hxBPhD3+Al16C6lNFdU5ARBpAR/jFNn781xdzfa28HCZMCMNHP/5x2PfBB/DCC/Dcc2GbMQOmTt34+XvtFX4D+OILuOkm+OwznRMQkXqp4Rfb5k4Eb9C+/aYnhd3hrbc2vgE89xxcd11o+NWtXRvWFVLDF5Eq1PCTkM+1AmbQu3fYRowI+9atgzZtal8PaPFiuO++8IbRqlXBSheRpktj+E1Zq1Z1nxMwg+HDw0ngUaNC86+y5ISIZI8aflNX18VhU6fCgw/CMceEZr+h+R9/vJq/SEaV1OJpVaVtMaM4MzvOnEmvSZNotWoV6zp25K2xYzcuFgfYl1/Sfu5cOjzxBNs98wwtPv6Yr9q2ZfX++/PeoEF8sPferG/ZMrb64sqLIzPteXFkpj0vjsxSz8vO4mk5aVvMqBiZDcpbt879oYfcTzjBvX37sBDcllu6jxzp/te/un/2Wf4L0BWivoQz054XR2ba8+LILPU8MrN4mtSvZUsYMiRsf/hDuBHMXXfBPfeE+futWoV5/pWVmuYpUmI0hp9lLVqExj95MqxcCX/7W1gJtLJy089buxYuvDCZGkWkYNTwJWjRAo44ouY9gTdYsgROPx2eeabmWkAi0iSo4cum6prmWV4efhM44ICw5s/554dlHlJ40l9EaqeGL5uqa5rnTTeFYZ9bbw33/J0wAfr3D0s+/+IX8OabydQrIg2mhi+bGjkyNPeKCtwMKirC45Ejw41djj8+zO9fvjws9rbttnDxxbDjjmFtn2uvDc+JSOqo4UtNI0fCokU8+dhjsGhR7bNzOnQIC7099VSYyXPVVfDll3D22dC1KxxyCEyaFBaB02qeIqmghi/R9egBP/0pzJ0L8+eHI/533oGTToLttgu3ely8GHPfOM1TTV+k6NTwpbB23hkuvxwWLAh3+GrbtvZpnuPGJVOfSIap4Us8zGDgQPjkk9qfX7wYbrsNPv+8uHWJZJgavsSrrmmezZuHVTy7dQvDQZrlIxI7NXyJV13TPG++OSzrcNBBcM01YZbPkCHw17/WvIWjiBSEGr7Eq75pnoMHw913h6t4L78c5s0LN3vfYYcwt3/FiqSrFykpavgSv81N8+zSBS65JDx3zz2wyy5hpk/37vD978Pjj+uKXpECUMOX9GjePBzhP/IILFwIZ54JM2eG3wR23RV+8xv48MPwuZrbL9JoaviSTjvtFJZvWLYsjPdvtVV4A+jaFQ4+GMaO1dx+kUZSw5d0a9MmXLg1axbMmRNu4P7kkzWnc2puv8hmFa3hm1kvM5tsZncX6zWlxAwYABMn1v38kiXFq0WkCWpQwzezKWa2yszmVds/xMwWmNkbZnZ+fRnu/pa7j4lSrAhQ99x+M/jVr2DNmuLWI9JENPQI/2ZgSNUdZlYG3AAMBfoCx5pZXzPbzczur7Z1LGjVkm21ze1v1Sos23z++eENYdw4WLUqmfpEUsq8gdPdzKwncL+798s9/iZwmbsfkXt8AYC7/3IzOXe7+/fqef5k4GSATp06DZw+fXqD6qsubXeRL0ZmlvI6zpxJr0mTaLVqFes6duStsWNZdeihbLFgAT3uvJMOTz3F+hYtWDFsGO/84Ad83rlz0WuMIy+OzLTnxZFZ6nkHH3zwHHffq8YTtd3ZvLYN6AnMq/L4e8CkKo+PB66v5+u3BX4PvAlc0JDXHDhwYN53bU/bXeSLkZm1vHozX3vNfcwY9xYt3MvK3I87zn3evPzzCl1fijLTnhdHZqnnAbO9lp5atJO27v4fdz/F3Xv7Zn4LEImsT5+wHv9bb8EZZ4QLuvr1g6OOCjN+RDKo6EM6DXytI4Eju3btetK0adPyykjbr1jFyMxaXmMym3/0EV3vvZduf/kLLdas4cM99mDxiBF8sPfe4WRvTDXq5yadmaWeF8eQTnPgLWAHoCXwErBrQ/MasmlIR3kFz/z4Y/drrnHv1s0d3Pfc0/2Pf3S/9Vb3igpfb+ZeUeE+bVoy9SWQmfa8ODJLPY8oQzpmdifwLNDHzJaa2Rh3/wo4HXgYmA/8yd1fyfstSaQYttgCzjorLMc8ZUq4YOsHP9BduSQTGtTw3f1Yd9/e3Vu4ezd3n5zb/6C7f8PDuPz4eEsVKaCWLeGEE+CVV8JtGKsPberKXSlBDR7DLyaN4SuvmJmDBg8OR/bVOPDkzJlQVpZ3dlq/56aUF0dmqedFHsNPYtMYvvKKkllREcbza9t23jmM8VdWJldfzJlpz4sjs9TzSHpapkhq1XVXrjPPhGbNwhj/gAFw331al1+aNDV8kbruynXttfDyyzBtWrgZ+/DhsN9+8OijavzSJGkMP6G8ODKzlhdHZl159tVXdHr4YXredhutV67kw9135+0xY/ho992LWl8cmWnPiyOz1PM0hh+RxiXTlxdH5mbzPv/c/frr3Tt3DmP8Rxzh/vzzRasvjsy058WRWep5aAxfpABatYLTTgvz+K++GmbPhn32CbdmfPnlpKsTqZcavkg+ysvhvPPg7bfh5z+HJ56A/v3h2GNhwQLdc1dSSQ1fJIott4SLLgqLtF1wQZjJs/POunJXUkknbRPKiyMza3lxZEbNa/HBB+x73HE0X7u2xnOfd+rErDzv71BV2r7nuPPiyCz1PJ20jUgnotKXF0dmQfLMar+Iyyx6tqf0e44xL47MUs9DJ21FiqSue+4CXHopfPRR8WoRqUINX6TQartyt3Vr2GsvuOIK6NUrzPCpZdhHJE5q+CKFVtuVu5MmwfPPb5zG+T//AzvuCDfeCF98kXTFkhFq+CJxGDkSFi3iycceg0WLwmOAgQPhoYfgqaegd+8wp79PH7jlFqisTLRkKX2apZNQXhyZWcuLI7Ooee5s88IL7DBpElu+/jqfVlTw9gknsPrAAze57WKiNaYgL47MUs/TLJ2INPMgfXlxZCaSt369+913u++yS5jNM2CA+0MPhf1pqTHBvDgySz0PzdIRSSkzOOYY+Pe/w9DO++/D0KEwaBD84x9JVyclRA1fJC3KymDUqLA0ww03wBtvwIEHhuY/Z46Wa5DImiddgIhU07IlnHoqjB4dGv+VV4YpnWVlUFmJwcblGmDjCWGRzdARvkhalZfDT38a1ulp167mLB7daF0aSQ1fJO3atYM1a2p/bsmS4tYiTZqmZSaUF0dm1vLiyExr3n4//CGtV66ssf+Lrbfmn/fcEyk7rd9znJmlnqdpmRFpqln68uLITG3etGnu5eU1F2MD94svdv/qq+RrjCkvjsxSz0PTMkWasLqWaxg9OtyA5bDDYMWKpKuUlFPDF2kqqi/XcOKJMHVq2GbNCnfceuyxpKuUFFPDF2nqRo8OC7O1bx+O9H/+c63LI7VSwxcpBf36wQsvwIgRcMkl4WKtVauSrkpSRg1fpFRssQXceitMnBhW4+zfP/wpkqOGL1JKzGDsWHjuufAGcPDB8Mtfwvr1SVcmKaCGL1KK9tgjrL/z/e/DhRfCd74Dq1cnXZUkTBdeJZQXR2bW8uLITHteozPd6TJjBjvecANfbL01r158MWt22y3WGhP/npWnC6+i0sUk6cuLIzPteXlnzpnj3quXe1mZ+1VXuVdWRsurR2q+5wznoQuvRDJswACYOxeOPjrcT/eoo8KFXFpuOVO0PLJIVrRrB3fdBddfD2edBQ88AO5abjlDdIQvkiVm8JOfQMeOYUWeqrTccslTwxfJolpW3gS03HKJU8MXyaIePWrf37FjceuQolLDF8mi8ePDHbWqMgtH/ldcobV4SpQavkgW1bbc8sSJYf+ll4ZF2N59N+kqpcDU8EWyqvpyy2PGwG23wZQpYWmG/v3hb39LukopIDV8EdnIDE44AWbPhs6dw6qbP/sZfPll0pVJAajhi0hNu+wSjvJPOQWuugq+9a3wW4A0aWr4IlK7Nm3gd7+DP/0J5s8PQzx//nPSVUkEWjwtobw4MrOWF0dm2vPiyGxIXuvly+l7xRVs9dprLBs+nDdPO431LVsWpb44Mks9T4unRaQFodKXF0dm2vPiyGxw3rp17ued5w7uu+/uPn9+tLxGSPu/S9ry0OJpIhJJy5Zw9dXw4INhyubAgXDLLUlXJY2ghi8ijTN0KLz4IuyzT7iB+qhR8PHHSVclDaCGLyKN17UrzJwJl10WllUeOBD+93+13HLKqeGLSH7KysJVuY89Bu+9F1baXLwYc9+43LKafqqo4YtINIMGhRumV6flllNHDV9Eolu2rPb9Wm45VdTwRSS6upZb3nbb4tYh9VLDF5HoaltuuVkzWL0azj5ba/GkhBq+iERX23LLU6fCGWfAtdfC4MGwfHnSVWaeGr6IFEb15ZZHjYLrrgszdebOhQED4Omnk64y09TwRSReI0bArFlhJs/BB8NvflPzBupSFGr4IhK/3XYLa+wPGwZnngnHHQeffpp0VZmjhi8ixdGuHdxzTzjBe+edsN9+8PrrSVeVKWr4IlI8zZrBhReGWycuXw577QUzZiRdVWao4YtI8R1+OMyZAzvtBEcdBRddBJWVSVdV8tTwRSQZFRVh1s6YMWGYZ9gw+M9/kq6qpBWt4ZvZ0WY20cz+aGaHF+t1RSTFWreGSZNg4kR44omw6ubs2UlXVbIa1PDNbIqZrTKzedX2DzGzBWb2hpmdX1+Gu9/r7icBpwA/yL9kESk5Y8eGo313OOAAmDw56YpKUkOP8G8GhlTdYWZlwA3AUKAvcKyZ9TWz3czs/mpbxypfelHu60RENtp77zCu/61vhTeAk06Cm2/WGvsF1OCbmJtZT+B+d++Xe/xN4DJ3PyL3+AIAd/9lHV9vwJXAo+4+s57XORk4GaBTp04Dp0+f3tDvZRNpu6lwMTKzlhdHZtrz4shMXV5lJTtMmULFHXfgZmF9/Q1PtWrFgvPOY9WhhyZbY8rzIt/EHOgJzKvy+HvApCqPjweur+frzwDmAL8HTmnIa+om5sordmba8+LITG1ehw7hhunVt4qKyNGp/Z4LlEcdNzFvnvdbSCO5+2+A3xTr9USkiVu9uvb9WmM/b1Fm6SwDuld53C23T0QkurrW2O/Uqbh1lJAoY/jNgYXAIYRG/wIwwt1fiVyU2ZHAkV27dj1p2rRpeWWkbUytGJlZy4sjM+15cWSmNa/jzJn0mTCBsnXrvt63oVstGj2aJccdh5eVJVpjWvMijeEDdwLLgS+BpcCY3P5hhKb/JjCuIVmN2TSGr7xiZ6Y9L47MVOdNm+ZeUeHrzcLY/U03uY8YEcby99vP/Y03kq8xhXnUMYbfoCEddz/W3bd39xbu3s3dJ+f2P+ju33D33u4+Pu+3IxGR2lRfY/+kk8LUzDvugPnzoX9/mDJFyy03UIOHdIpJQzrKSyoz7XlxZKY9r67MVitXsvOVV9L+xRd574ADWHjeeXzZrl0iNaYtL/K0zCQ2Dekor9iZac+LIzPtefVmVla6T5jg3rKle+fO7g89FC0vT2nLI8qQjohIKjVrBueeC88/D9tuC0OHwk9+AmvXJl1ZKqnhi0jTt8ceYdG1s86C668Pi7DNnZt0Vamjhi8ipaF1a7jmGnj0UVizBvbdF668UuvsV6GTtgnlxZGZtbw4MtOeF0dm2vPyyWy+Zg3f+PWv6fjkk3y4++68dsEFfN65c2w1pi1PJ20j0sm39OXFkZn2vDgy056Xd+b69e633OK+5ZZhu/XWmvP6p01Lrr4Y80h6LR0RkaIyg1Gj4MAD4fjjw8dlZVBZiQEsXgwnnxw+d+TIJCstGo3hi0hp69kz3E1r661rjuevXQvjxiVRVSI0hp9QXhyZWcuLIzPteXFkpj2vUJmDBg/eZG39DdwsXMkbQdr+DjWGH1FqxiWVF2tm2vPiyEx7XsEyKyo8K+vrowuvRCTTxo+H8vKa+0eNKn4tCVHDF5FsGDkSbroJKipwM+jaFTp3DnP3n3km6eqKQg1fRLKj6uqbS5eGm6Z36QJDhsA//5l0dbFTwxeR7OrSBR5/HLbfPjT9Z59NuqJYaZZOQnlxZGYtL47MtOfFkZn2vDgyq+e1fO89+p9zDi3ff5+Xr76aNX37pqq+xtIsnYhSO/Mgw3lxZKY9L47MtOfFkVlr3tKl7jvu6L7VVu6zZkXPi0CzdERE4tS1axje6dABDj8cnnsu6YoKTg1fRGSDbt1C099uu9D0n38+6YoKSg1fRKSq7t3DUgwbmv4LLyRdUcGo4YuIVNe9ezjS32YbOOywcHOVEqCGLyJSmx49QtNv3z40/Tlzkq4oMk3LTCgvjsys5cWRmfa8ODLTnhdHZmPyWq1YQf+zz6b5J5/w0oQJfNKnT6rqq42mZUbUZKealXBeHJlpz4sjM+15cWQ2Ou/tt8Mia+3bu8+ZEz1vMzQtU0QkKRvW1N9ySzj0UPjXv5KuKC9q+CIiDVG96b/4YtIVNZoavohIQ+2wQziR27YtHHJIk2v6avgiIo3Rq1do+uXlcMAB0KULgwYPDr8B3H570tXVSw1fRKSxeveGc84J98RdvjzcOnHDTdFT3PTV8EVE8nHddeEmiVWl/KboavgiIvlYsqRx+1NAF14llBdHZtby4shMe14cmWnPiyOzEHn7/fCHtF65ssZ+N2PhOeew/NvfBrNE6tOFVxGV5MUkTTwvjsy058WRmfa8ODILkjdtmnt5uXsY2Albmzbuu+wSPj7iCPclSxKpD114JSJSQNVvil5RARMnwrx5cMMN8PTT0K8fTJ5cc6w/IWr4IiL5qnpT9EWLwuNmzeDUU+Hll2HPPWHsWBg2LNw0PWFq+CIicejVCx57DH77W3jqqXC0P3Vqokf7avgiInFp1gxOPz0c7e++O5x4InznO7BsWTLlJPKqIiJZ0rt3WIfnuuvCVbq77gq33FL0o301fBGRYmjWDM44A156CXbbDUaPhuHD4d13i1dC0V5JRERgp53C0f4118DMmeFo/7bbinK0r4YvIlJsZWVw1lnhaL9vXxg1Co4+Gm68EXr2jG0xtuYFTRMRkYb7xjfCDJ7rroOf/QxmzADAYONibBCmexaAjvBFRJJUVhZW3uzYseZzBV6MTQ1fRCQNli+vfX8BF2PT4mkJ5cWRmbW8ODLTnhdHZtrz4shMY15di7F93qkTs6ZPb1SWFk+LKDMLQjWhvDgy054XR2ba8+LITGVebYuxlZeH/Y2EFk8TEUmx2hZju+mmgp2wBY3hi4ikR22LsRWQGr6ISEao4YuIZIQavohIRqjhi4hkhBq+iEhGqOGLiGSEGr6ISEao4YuIZIQavohIRqjhi4hkhBq+iEhGqOGLiGSEGr6ISEao4YuIZIQavohIRhSt4ZvZLmb2ezO728x+XKzXFRGRoEEN38ymmNkqM5tXbf8QM1tgZm+Y2fn1Zbj7fHc/Bfg+8F/5lywiIvlo6BH+zcCQqjvMrAy4ARgK9AWONbO+Zrabmd1fbeuY+5rhwAPAgwX7DkREpEEs3O+2AZ9o1hO439375R5/E7jM3Y/IPb4AwN1/2YCsB9z923U8dzJwcu5hH2BBgwqsaTtgdZ5fW4y8ODKzlhdHZtrz4shMe14cmaWeV+HuHarvbB4hsCvwTpXHS4F96/pkMzsI+C7QinqO8N39JuCmCHVteL3Z7r5X1Jy48uLIzFpeHJlpz4sjM+15cWRmLW+DKA2/Udz9CeCJYr2eiIhsKsosnWVA9yqPu+X2iYhICkVp+C8AO5nZDmbWEvghMKMwZRVE5GGhmPPiyMxaXhyZac+LIzPteXFkZi0PaOBJWzO7EziIcCJhJXCpu082s2HAtUAZMMXdx8dRpIiIRNfgWToiItK0aWkFEZGMKMmG35grgBuQVetVxhHyupvZ42b2qpm9YmZnFiCztZk9b2Yv5TIvL1CtZWb2LzO7vwBZi8zs32b2opnNLkDe1rllOl4zs/m560Ki5NQf030AAAUpSURBVPXJ1bZhW2NmZ0XMPDv37zHPzO40s9YR887MZb2Sb221/Tyb2TZm9qiZvZ77s33EvP+Tq3G9mTVqamEdeVfn/p1fNrN7zGzrAmT+PJf3opk9YmZdouRVee5cM3Mz2y5ifZeZ2bIqP4/DGppXL3cvqY1wPuFNoBfQEngJ6Bsh70BgADCvQPVtDwzIfbwlsDBKfbkcA7bIfdwCeA7YrwC1ngPcQbjgLmrWImC7Av473wKMzX3cEti6wD9DKwgXr+Sb0RV4G2iTe/wnYHSEvH7APKCcMJ16JrBjHjk1fp6Bq4Dzcx+fD/wqYt4uhIsmnwD2KkB9hwPNcx//qjH11ZO5VZWPzwB+HyUvt7878DCwuDE/63XUdxlwXiF+nqtupXiEvw/whru/5e5fANOBo/INc/engPcLVZy7L3f3ubmPPwbmE5pDlEx3909yD1vktkgnZ8ysG/BtYFKUnDiYWTvCf5LJAO7+hbt/WMCXOAR4090XR8xpDrQxs+aERv1uhKxdgOfcfa27fwU8SbiQsVHq+Hk+ivAGSu7Po6PkeVg3K68r5OvIeyT3PQPMIkwBj5q5psrDtjTi/0s9PeEa4H8ak7WZvIIrxYZf2xXAkRpqXHLLVexJOCKPmlVmZi8Cq4BH3T1q5rWEH971UWvLceARM5uTWz4jih2A94CpuSGnSWbWNnqJX/shcGeUAHdfBkwAlgDLgY/c/ZEIkfOAb5nZtmZWDgxj0+tgoujk7stzH68AOhUoNw4nAg8VIsjMxpvZO8BI4JKIWUcBy9z9pULUlnN6bthpSmOG2epTig2/STCzLYA/A2dVO9rIi7tXunt/wtHPPmbWL0Jt3wFWufucqHVVcYC7DyAstneamR0YIas54Vfg37n7nsCnhKGIyHLXlAwH7oqY055w5LwD0AVoa2bH5Zvn7vMJwxmPAH8DXgQqo9RYx+s4EX87jIuZjQO+Am4vRJ67j3P37rm80yPUVQ5cSMQ3jWp+B/QG+hMOGP5fIUJLseGn/gpgM2tBaPa3u/tfCpmdG9p4nGqrmzbSfwHDzWwRYUhssJlNi1jXstyfq4B7CENv+VoKLK3yW8zdhDeAQhgKzHX3lRFzDgXedvf33P1L4C/A/lEC3X2yuw909wOBDwjnfwphpZltD5D7c1WBcgvGzEYD3wFG5t6UCul24JgIX9+b8Mb+Uu7/TDdgrpl1zjfQ3VfmDuLWAxOJ9v/la6XY8FN9BbCZGWHseb67/7pAmR02zFwwszbAYcBr+ea5+wXu3s3dexL+/h5z97yPTs2srZltueFjwkm4vGc9ufsK4B0z65PbdQjwar551RxLxOGcnCXAfmZWnvs3P4RwviZvtnGZ8R6E8fs7IlcZzAB+lPv4R8BfC5RbEGY2hDC8ONzd1xYoc6cqD48i2v+Xf7t7R3fvmfs/s5QwMWNFhPq2r/Lwv4nw/2UThT4LnIaNML65kDBbZ1zErDsJv1J9SfiHHBMx7wDCr8wvE34tfxEYFjFzd+Bfucx5wCUF/Ls8iIizdAgzpl7Kba9E/TfJZfYHZue+53uB9gXIbAv8B2hXoL+7ywmNZB5wG9AqYt4/CG9sLwGH5JlR4+cZ2Bb4O/A6YfbPNhHz/jv38TrClfkPR8x7g3BebsP/lwbPqKkn88+5f5eXgfuArlHyqj2/iMbN0qmtvtuAf+fqmwFsX4ifSV1pKyKSEaU4pCMiIrVQwxcRyQg1fBGRjFDDFxHJCDV8EZGMUMMXEckINXwRkYz4//hFpMCaZs2+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7_MDz6szq8m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "147e0713-402e-4b63-eb60-7710df36bde0"
      },
      "source": [
        "#---See the learned Codebook\n",
        "batch_size_plot = M\n",
        "symbols = np.arange(M)+1\n",
        "symbols = np.stack(J*[symbols], axis=0).transpose()\n",
        "\n",
        "#---Get codebook\n",
        "codebook = get_codebook_from_condensed_codebook()\n",
        "\n",
        "h_all = []\n",
        "real_idx = 2 * torch.arange(K).long()\n",
        "\n",
        "for i in range(J):\n",
        "  h = np.zeros([J, 2*K])\n",
        "#   h[i, :] = np.ones([1, 2*K])\n",
        "  h[i, real_idx] = 1.0\n",
        "  h = np.stack(batch_size_plot*[h], axis = 0)\n",
        "  h = torch.Tensor(h).to(device)\n",
        "  h_all.append(h)\n",
        "\n",
        "\n",
        "#---Forward Pass\n",
        "codewords_faded_all = []\n",
        "for i in range(J):\n",
        "    codewords_faded, codewords_faded_without_h = encode(symbols, codebook, h = h_all[i])\n",
        "    codewords_faded_all.append(codewords_faded)\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#---code for color assignment for arbitrary num of users\n",
        "# user_colors = {}\n",
        "# users = np.arange(J)\n",
        "# unique_users = list(set(users)) \n",
        "# step_size = (256**3) // len(unique_users)\n",
        "# for i, user in enumerate(unique_users):\n",
        "#     temp = step_size*i\n",
        "#     color = np.zeros([3], dtype=np.float32) # RGB\n",
        "#     for pigment in range(3):\n",
        "#       r = temp % 256\n",
        "#       color[pigment] = r/256\n",
        "#       temp = temp//256\n",
        "#     user_colors[user] = color.tolist()\n",
        "# colors = [user_colors[user] for user in users]\n",
        "\n",
        "colors = ['cyan', 'green', 'blue', 'magenta', 'orange', 'red']\n",
        "\n",
        "for k in range(K):\n",
        "  fig, ax = plt.subplots()\n",
        "  for user in range(J):\n",
        "    data = codewords_faded_all[user].cpu().detach().numpy()\n",
        "    if np.abs(data[:, 2*k]).sum()!=0:\n",
        "      ax.scatter(data[:, 2*k], data[:, 2*k+1], c=colors[user], label='User '+str(user+1))\n",
        "\n",
        "  ax.legend()\n",
        "  ax.axhline(y=0, color='k')\n",
        "  ax.axvline(x=0, color='k')\n",
        "  ax.set_title('Resource: '+str(k+1))\n",
        "  ax.imshow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfX4OEofMFCT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}