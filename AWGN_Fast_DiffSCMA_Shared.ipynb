{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AWGN-Fast-DiffSCMA-Shared.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekagra-ranjan/Auto-SCMA/blob/main/AWGN_Fast_DiffSCMA_Shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDMhaO8y3j24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "451cf9d2-8ecb-4226-bda0-f6505652e672"
      },
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt2\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import itertools\n",
        "import math\n",
        "from time import time\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cpu\"\n",
        "print(\"\\nDevice:\", device)\n",
        "\n",
        "seed = 6789 # also 0, 6789\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if device is not \"cpu\":\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.set_rng_state(torch.cuda.get_rng_state())\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "eps = 1e-8\n",
        "\n",
        "def pdb():\n",
        "    import pdb; pdb.set_trace()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67tG4PIq34FZ"
      },
      "source": [
        "# Number of symbols\n",
        "M = 4\n",
        "# Number of users\n",
        "J = 6\n",
        "# Number of orthogonal resources\n",
        "K = 4\n",
        "# sparse mapping matrix V: K x J\n",
        "V = torch.tensor([[0, 1, 1, 0, 1, 0],\n",
        "                  [1, 0, 1, 0, 0, 1],\n",
        "                  [0, 1, 0, 1, 0, 1],\n",
        "                  [1, 0, 0, 1, 1, 0]\n",
        "                ], dtype = torch.int32, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovwCuxyp3_6l"
      },
      "source": [
        "def get_codebook_from_condensed_codebook():\n",
        "    codebook = torch.zeros(J, M, K, 2).float().to(device)\n",
        "    for i in range(J):\n",
        "        resource_idx = (V[:, i]==1)\n",
        "        codebook[i, :, resource_idx, :] = condensed_codebook[i] # [J, M, dv, 2]\n",
        "    return codebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK7sP8VW33_A"
      },
      "source": [
        "def encode(symbols, codebook, h=None):\n",
        "    '''\n",
        "    Inputs: symbols: [BS, J], h: [BS, J, 2*K], codebook: [J, M, K, 2],\n",
        "    Returns [BS, 2*K]\n",
        "    '''\n",
        "\n",
        "    batch_size = symbols.shape[0]\n",
        "    encoded = torch.zeros((batch_size, 2*K)).to(device)\n",
        "    encoded_without_h = torch.zeros((batch_size, 2*K)).to(device)\n",
        "    \n",
        "    for user in range(J):\n",
        "        # pdb()\n",
        "        codeword = codebook[user][symbols[:, user]-1] # [BS, K, 2]\n",
        "        codeword = codeword * V[:, user].reshape(1, -1, 1) # makes sure that codeword for resources not connected dont exist\n",
        "        codeword = codeword.view(-1, 2*K) # [BS, 2*K]\n",
        "        encoded_without_h = encoded_without_h + codeword\n",
        "        codeword_faded = torch.zeros_like(codeword)\n",
        "        \n",
        "        if h is not None:\n",
        "            '''CORRECT THE MULT WITH H'''\n",
        "            real_idx = 2 * torch.arange(K).long()\n",
        "            img_idx = 2 * torch.arange(K).long() + 1\n",
        "            codeword_faded[:, real_idx] = codeword_faded[:, real_idx] + \\\n",
        "                                          (h[:, user, real_idx] * codeword[:, real_idx]) - \\\n",
        "                                          (h[:, user, img_idx] * codeword[:, img_idx])\n",
        "            codeword_faded[:, img_idx]  = codeword_faded[:, img_idx] + \\\n",
        "                                          (h[:, user, real_idx] * codeword[:, img_idx]) + \\\n",
        "                                          (h[:, user, img_idx] * codeword[:, real_idx])\n",
        "        else:\n",
        "            codeword_faded = codeword_faded + codeword\n",
        "\n",
        "        encoded = encoded + codeword_faded\n",
        "    # pdb()\n",
        "    return encoded, encoded_without_h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGe0ih9BLaMS"
      },
      "source": [
        "def calc_distance(a, b):\n",
        "    '''\n",
        "    Args: a: [BS, 2]    b: [BS, M**df, 2]\n",
        "    Returns: distance: [BS, M**df]\n",
        "    '''\n",
        "    # pdb()\n",
        "    a = a.view(-1, 1, 2)\n",
        "    return ((a-b)**2).sum(-1).squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def get_codebook_centre_resource(centres_all, x_centres_all, codebook, h, resource_id):\n",
        "def get_codebook_centre_resource(codebook, h, resource_id):\n",
        "    '''\n",
        "    Inputs:\n",
        "        codebook: [J, M, K, 2]\n",
        "        h dim: [batch_size, J, 2*K]\n",
        "        V: K x J\n",
        "\n",
        "    Outputs:\n",
        "        centres_all [BS, M**df, 2]: contains the real and img part of\n",
        "                                    codewords for all permutation\n",
        "                                    (i.e., M**df) of user symbols\n",
        "                                    connected to kth resource,\n",
        "                                    i.e., \\sum_{j} h^{T}x_{j}\n",
        "        x_centres_all [BS, M**df, 2*df]: contains the real and img part of\n",
        "                                         each df users in codewords for all\n",
        "                                         permutation (i.e., M**df) of user\n",
        "                                         symbols connected to kth resource, x_{j}\n",
        "    '''\n",
        "    centres_all = torch.zeros((h.shape[0], M**df, 2), device=device)        # [BS, M**df, 2]\n",
        "    x_centres_all = torch.zeros((M**df, 2*df), device=device)               # [M**sd, 2*df]\n",
        "            \n",
        "\n",
        "    #---Create centres_all\n",
        "    t1 = time()\n",
        "    resource_user_idx = V[resource_id].nonzero().reshape(-1)\n",
        "    alphabets = np.arange(1, M+1)\n",
        "    symbols_resource = [torch.tensor(p) for p in itertools.product(alphabets, repeat=df)] \n",
        "    symbols_resource = torch.stack(symbols_resource).to(device)             # [M**df, df]\n",
        "\n",
        "    symbols_resource.unsqueeze_(0)                                          # [1, M**df, df]\n",
        "    symbols_resource = symbols_resource.repeat(h.shape[0], 1, 1)            # [BS, M**df, df]\n",
        "    \n",
        "    symbols_resource_all = torch.zeros((M**df, J),\\\n",
        "                                        dtype = torch.int64, device=device) # [M**df, J]\n",
        "    symbols_resource_all[:, resource_user_idx] = symbols_resource[0]        # [M**df, J]\n",
        "    \n",
        "    h_repeat_perm = h.unsqueeze(1).repeat(1, M**df, 1, 1)                   # [BS, M**df, J, 2*K]\n",
        "    h_repeat_perm = h_repeat_perm.view(-1, J, 2*K)                          # [BS * M**df, J, 2*K]\n",
        "    \n",
        "     \n",
        "    symbols_resource = symbols_resource.view(-1, df)                        # [BS * M**df, df]\n",
        "    codeword_faded_k = torch.zeros((symbols_resource.shape[0], 2), device=device)\n",
        "    for user_idx, user in enumerate(resource_user_idx):\n",
        "        # pdb()\n",
        "        codeword = codebook[user][symbols_resource[:, user_idx]-1]          # [BS, K, 2]\n",
        "        codeword_k = codeword[:, resource_id, :]                            # [BS, 2]\n",
        "        codeword_k = codeword_k * V[resource_id, user] # makes sure that codeword for resources not connected dont exist\n",
        "        \n",
        "        '''CORRECT THE MULT WITH H'''\n",
        "        real_idx = 2 * resource_id\n",
        "        img_idx = 2 * resource_id + 1\n",
        "        codeword_faded_k[:, 0] = codeword_faded_k[:, 0] + \\\n",
        "                                        (h_repeat_perm[:, user, real_idx] * codeword_k[:, 0]) - \\\n",
        "                                        (h_repeat_perm[:, user, img_idx] * codeword_k[:, 1])\n",
        "        codeword_faded_k[:, 1]  = codeword_faded_k[:, 1] + \\\n",
        "                                        (h_repeat_perm[:, user, real_idx] * codeword_k[:, 1]) + \\\n",
        "                                        (h_repeat_perm[:, user, img_idx] * codeword_k[:, 0])\n",
        "    \n",
        "    centres_all[:, :, :] = codeword_faded_k.view(h.shape[0], M**df, 2) \n",
        "    # print('1 Time:', time()-t1)\n",
        "    \n",
        "\n",
        "    # ---Create x_centres_all\n",
        "    t1 = time()\n",
        "    symbols_resource_all = symbols_resource_all.unsqueeze(1).repeat(1, df, 1) # [M**df, df, J]\n",
        "    symbols_resource_all = symbols_resource_all.view(-1, J)                 # [M**df * df, J]\n",
        "    h_x = torch.zeros((df, J, 2*K), device=device)                          # [df, J, 2*K]\n",
        "    real_idx = 2 * torch.arange(K).long()\n",
        "    temp = torch.zeros((2*K), device=device)\n",
        "    temp[real_idx] = 1.0\n",
        "    \n",
        "    for idx, val in enumerate(V[resource_id].nonzero().reshape(-1)):\n",
        "        h_x[idx, val, :] = temp\n",
        "    \n",
        "    h_x = h_x.view(1, df, J, 2*K)\n",
        "    h_x = h_x.repeat(M**df, 1, 1, 1)                                        # [M**df, df, J, 2*K]\n",
        "    h_x = h_x.view(-1, J, 2*K)\n",
        "    x_centres, _ = encode(symbols_resource_all, codebook, h=h_x)            # [M**df * df, 2*K]\n",
        "    x_centres = x_centres.view(M**df, df,2*K)\\\n",
        "                [:, :, 2 * resource_id: 2 * resource_id + 2]                # [M**df, df, 2]\n",
        "    x_centres = x_centres.reshape(M**df, 2 * df)\n",
        "    x_centres_all[:, :] = x_centres\n",
        "    # print('2 Time:', time()-t1)\n",
        "    \n",
        "    return centres_all, x_centres_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngnDFBGaDbBe"
      },
      "source": [
        "df = (V[0]==1).sum() # num of users connected to kth resource\n",
        "dv = (V.t()[0]==1).sum() # num of resources connected to jth user\n",
        "\n",
        "\n",
        "class MPA():\n",
        "\n",
        "    def __init__(self):\n",
        "        alphabets = np.arange(1, M+1)\n",
        "        self.symbols_resource = [torch.tensor(p) for p in itertools.product(alphabets, repeat=df)] \n",
        "        self.symbols_resource = torch.stack(self.symbols_resource).to(device)   # [M**df, df]\n",
        "        self.symbols_resource.unsqueeze_(0)                                     # [1, M**df, df]\n",
        "\n",
        "\n",
        "    def calc_distance(self, a, b):\n",
        "        '''\n",
        "        Args: a: [BS, 2]    b: [BS, M**df, 2]\n",
        "        Returns: distance: [BS, M**df]\n",
        "        '''\n",
        "        # pdb()\n",
        "        a = a.view(-1, 1, 2)\n",
        "        return ((a-b)**2).sum(-1).squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def get_codebook_centre_resource(centres_all, x_centres_all, codebook, h, resource_id):\n",
        "    def get_codebook_centre_resource(self, codebook, h, resource_id):\n",
        "        '''\n",
        "        Inputs:\n",
        "            codebook: [J, M, K, 2]\n",
        "            h dim: [batch_size, J, 2*K]\n",
        "            V: K x J\n",
        "\n",
        "        Outputs:\n",
        "            centres [BS, M**df, 2]: contains the real and img part of\n",
        "                                        codewords for all permutation\n",
        "                                        (i.e., M**df) of user symbols\n",
        "                                        connected to kth resource,\n",
        "                                        i.e., \\sum_{j} h^{T}x_{j}\n",
        "            x_centres [BS, M**df, 2*df]: contains the real and img part of\n",
        "                                            each df users in codewords for all\n",
        "                                            permutation (i.e., M**df) of user\n",
        "                                            symbols connected to kth resource, x_{j}\n",
        "        '''\n",
        "        \n",
        "        #---Create centres_all\n",
        "        t1 = time()\n",
        "        resource_user_idx = V[resource_id].nonzero().reshape(-1)\n",
        "        symbols_resource = self.symbols_resource.repeat(h.shape[0], 1, 1)            # [BS, M**df, df]\n",
        "        \n",
        "        symbols_resource_all = torch.zeros((M**df, J),\\\n",
        "                                            dtype = torch.int64, device=device) # [M**df, J]\n",
        "        symbols_resource_all[:, resource_user_idx] = symbols_resource[0]        # [M**df, J]\n",
        "        \n",
        "        h_repeat_perm = h.unsqueeze(1).repeat(1, M**df, 1, 1)                   # [BS, M**df, J, 2*K]\n",
        "        h_repeat_perm = h_repeat_perm.view(-1, J, 2*K)                          # [BS * M**df, J, 2*K]\n",
        "        \n",
        "        \n",
        "        symbols_resource = symbols_resource.view(-1, df)                        # [BS * M**df, df]\n",
        "        codeword_faded_k = torch.zeros((symbols_resource.shape[0], 2), device=device)\n",
        "        for user_idx, user in enumerate(resource_user_idx):\n",
        "            # pdb()\n",
        "            codeword = codebook[user][symbols_resource[:, user_idx]-1]          # [BS, K, 2]\n",
        "            codeword_k = codeword[:, resource_id, :]                            # [BS, 2]\n",
        "            codeword_k = codeword_k * V[resource_id, user] # makes sure that codeword for resources not connected dont exist\n",
        "            \n",
        "            '''CORRECT THE MULT WITH H'''\n",
        "            real_idx = 2 * resource_id\n",
        "            img_idx = 2 * resource_id + 1\n",
        "            codeword_faded_k[:, 0] = codeword_faded_k[:, 0] + \\\n",
        "                                            (h_repeat_perm[:, user, real_idx] * codeword_k[:, 0]) - \\\n",
        "                                            (h_repeat_perm[:, user, img_idx] * codeword_k[:, 1])\n",
        "            codeword_faded_k[:, 1]  = codeword_faded_k[:, 1] + \\\n",
        "                                            (h_repeat_perm[:, user, real_idx] * codeword_k[:, 1]) + \\\n",
        "                                            (h_repeat_perm[:, user, img_idx] * codeword_k[:, 0])\n",
        "        \n",
        "        # ---Create x_centres_all\n",
        "        t1 = time()\n",
        "        symbols_resource_all = symbols_resource_all.unsqueeze(1).repeat(1, df, 1) # [M**df, df, J]\n",
        "        symbols_resource_all = symbols_resource_all.view(-1, J)                 # [M**df * df, J]\n",
        "        h_x = torch.zeros((df, J, 2*K), device=device)                          # [df, J, 2*K]\n",
        "        real_idx = 2 * torch.arange(K).long()\n",
        "        temp = torch.zeros((2*K), device=device)\n",
        "        temp[real_idx] = 1.0\n",
        "        \n",
        "        for idx, val in enumerate(V[resource_id].nonzero().reshape(-1)):\n",
        "            h_x[idx, val, :] = temp\n",
        "        \n",
        "        h_x = h_x.view(1, df, J, 2*K)\n",
        "        h_x = h_x.repeat(M**df, 1, 1, 1)                                        # [M**df, df, J, 2*K]\n",
        "        h_x = h_x.view(-1, J, 2*K)\n",
        "        x_centres, _ = encode(symbols_resource_all, codebook, h=h_x)            # [M**df * df, 2*K]\n",
        "        x_centres = x_centres.view(M**df, df,2*K)\\\n",
        "                    [:, :, 2 * resource_id: 2 * resource_id + 2]                # [M**df, df, 2]\n",
        "        x_centres = x_centres.reshape(M**df, 2 * df)\n",
        "        \n",
        "        return codeword_faded_k.view(h.shape[0], M**df, 2), x_centres\n",
        "\n",
        "\n",
        "    def resource_to_user(self, x, u2r, codebook, h, sigma_square, beta):\n",
        "        '''\n",
        "        Inputs:  x: [BS, 2*K] , i.e., received signal, u2r: [BS, J, K, M], h: [BS, J, 2*K]\n",
        "        Output: [BS, K, J, M]\n",
        "        '''\n",
        "\n",
        "        message_all = torch.zeros((h.shape[0], K, J, M), device=device)\n",
        "        \n",
        "        for i in range(0, K):\n",
        "            # pdb()\n",
        "            '''\n",
        "            centres_all: [batch_size, M**df, 2]\n",
        "            x_centres_all: [BS, M**df, 2*df]\n",
        "            '''\n",
        "            t1 = time()\n",
        "            centres = self.centres_all[i]\n",
        "            x_centres = self.x_centres_all[i]\n",
        "            x_centres = x_centres.view(1, M**df, df, 2) # [1, M**df, df, 2]\n",
        "\n",
        "            distance = self.calc_distance(x[:, 2*i:2*i+2], centres) # distance([BS, 2], [BS, M**df, 2]) -> [BS, M**df]\n",
        "            distance = - (1.0/(2.0 * sigma_square)) * distance\n",
        "            assert distance.dtype == torch.float32, \"distance is not float32\"\n",
        "            M_k = torch.zeros((h.shape[0], J, M), device=device) # conditional prob of x_{j} given r_{k}\n",
        "\n",
        "            u2r_ = u2r.permute(0, 2, 1, 3) # [BS, K, J, M]\n",
        "\n",
        "\n",
        "            '''Check from here'''\n",
        "            u2r_resource = u2r_[:, i, V[i]==1] # [BS, df, M]\n",
        "            prior_big = torch.zeros((h.shape[0], M**df, df), device=device) # [BS, M**df, df]\n",
        "\n",
        "            t1 = time()\n",
        "            for df_idx in range(df):\n",
        "                \n",
        "                u2r_resource_df = u2r_resource[:, df_idx, :] # [BS, M]\n",
        "                u2r_resource_df = u2r_resource_df.reshape(1, -1) # [1, BS * M]\n",
        "                symbols_resource_df = (self.symbols_resource[:, :, df_idx] - 1).repeat(h.shape[0], 1) # [BS, M**df]\n",
        "                incrementor = torch.arange(0, M * h.shape[0], M).view(-1, 1).to(device) # [BS, 1]\n",
        "                symbols_resource_df_mask = symbols_resource_df + incrementor # [BS, M**df]\n",
        "                symbols_resource_df_mask = symbols_resource_df_mask.view(1, -1) # [1, BS * M**df]\n",
        "                prior_big_df = u2r_resource_df[0, symbols_resource_df_mask] # [1, BS * M**df]\n",
        "                prior_big_df = prior_big_df.view(-1, M**df) # [BS, M**df]\n",
        "                prior_big[:, :, df_idx] = prior_big_df # [BS, M**df, df]\n",
        "\n",
        "            prior_big_sum = prior_big.sum(-1) # [BS, M**df]\n",
        "            \n",
        "            user_idx = (V[i]).nonzero().reshape(-1)\n",
        "            # print('Time df', time()-t1)\n",
        "            t1 = time()\n",
        "            for idx, j in enumerate(user_idx):\n",
        "                # pdb()\n",
        "                \n",
        "                '''Careful: codebook is [J, M, K, 2] which is numpy'''\n",
        "                df_unique = codebook[j, :, i, :] # [M, 2]\n",
        "                x_centres_temp = x_centres[:, :, idx] # [1, M**df, 2]\n",
        "                assert df_unique.dtype == x_centres_temp.dtype, \"comparison vars dont have same dtype for mask_temp\"\n",
        "\n",
        "\n",
        "                # pdb()\n",
        "                mask_temp = (df_unique.view(1, M, 1, 2) == x_centres_temp.view(1, 1, M**df, 2)) # [1, M, M**df, 2]\n",
        "                mask = (mask_temp.sum(-1)==2) # [1, M, M**df]: for matching, both real and img parts will match hence 2.\n",
        "                mask = mask.repeat(h.shape[0], 1, 1) # [BS, M, M**df]\n",
        "\n",
        "                prior_big_m = prior_big_sum - prior_big[:, :, idx] # [BS, M**df]\n",
        "                message = distance + prior_big_m # [BS, M**df]\n",
        "                lowest_message = message.min()\n",
        "\n",
        "                message = message.unsqueeze(1).repeat(1, M, 1) # [BS, M, M**df]\n",
        "\n",
        "                # pdb()\n",
        "                message = torch.where(mask, message, lowest_message) # [BS, M, M**df]\n",
        "                \n",
        "                #---log-sum-MPA\n",
        "                message_max = message.max(-1)[0] # [BS, M]: 0 is used to use take the max and not the indices\n",
        "                message = message - message_max.view(-1, M, 1)\n",
        "                message = torch.exp(message) # [BS, M, M**df]\n",
        "                message = torch.where(mask, message, torch.tensor([0.0], device=device))\n",
        "                message = message.sum(-1) # [BS, M]\n",
        "                \n",
        "                message = torch.log(message) # [BS, M]\n",
        "                message = message + message_max # [BS, M]\n",
        "                # pdb()\n",
        "                \n",
        "                #---Max log MPA\n",
        "                # message = message.max(-1)[0] # [BS]: 0 is used to use take the max and not the indices\n",
        "                \n",
        "                message_all[:, i, j] = message + beta\n",
        "            # print('Time user_idx', time()-t1)\n",
        "                \n",
        "        return message_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def user_to_resource(self, r2u, gamma):\n",
        "        '''\n",
        "        Input: r2u: [BS, K, J, M]\n",
        "        Returns [BS, J, K, M]\n",
        "        '''\n",
        "        assert (r2u == float(\"-inf\")).sum() == 0, \"r2u has -inf values!\" # if -inf, then need to implement the counting of -inf freq\n",
        "\n",
        "        message_all = torch.zeros((r2u.shape[0], J, K, M), device=device)\n",
        "        r2u_ = r2u.permute(0, 2, 1, 3) # [BS, J, K, M]\n",
        "        # to filter out messages to those resources which are not onnected to the user\n",
        "        mask = torch.where(r2u_!=0, torch.tensor([1.0], device=device), torch.tensor([0.0], device=device))\n",
        "\n",
        "        # pdb()\n",
        "        ''' Need to change'''\n",
        "        message_all = -r2u_ # [BS, J, K, M]\n",
        "        for i in range(J):\n",
        "            message_all[:, i, :, :] = message_all[:, i, :, :] + r2u_[:, i].sum(-2).view(-1, 1, M) + gamma # [BS, 1, M]\n",
        "        # pdb()\n",
        "        message_all = mask * message_all\n",
        "        \n",
        "        #---normalisation\n",
        "\n",
        "        Z = torch.exp(message_all)\n",
        "        Z = torch.where(message_all!=0, Z, torch.tensor([torch.exp(-gamma) / M], device=device)) # [BS, J, K, M]\n",
        "        Z = torch.log(Z.sum(-1))\n",
        "\n",
        "        message_all = message_all - Z.view(-1, J, K, 1) # [BS, J, K, M]\n",
        "\n",
        "        return message_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def calc_output(self, r2u):\n",
        "        '''\n",
        "        Input: r2u: [BS, K, J, M]\n",
        "        Output: [BS, J, M]\n",
        "        '''\n",
        "        r2u_ = r2u.permute(0, 2, 3, 1) # [BS, J, M, K]\n",
        "        return r2u_.sum(-1) + gamma\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, codebook, h, sigma_square, beta, gamma):\n",
        "        ''' \n",
        "        Inputs:\n",
        "        x: [BS, 2*K] , i.e., received signal\n",
        "        h: [BS, J, 2*K]\n",
        "\n",
        "        Variables used:\n",
        "        u2r [BS, J, K, M]: user to resource message\n",
        "        r2u [BS, K, J, M]: resource to user message\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "            centres_all: [batch_size, M**df, 2]\n",
        "            x_centres_all: [BS, M**df, 2*df]\n",
        "        '''\n",
        "        self.centres_all = torch.zeros((K, h.shape[0], M**df, 2), device=device)        # [K, BS, M**df, 2]\n",
        "        self.x_centres_all = torch.zeros((K, M**df, 2*df), device=device)               # [K, M**sd, 2*df]\n",
        "\n",
        "        t1 = time()\n",
        "        for i in range(K):\n",
        "            self.centres_all[i], self.x_centres_all[i] = self.get_codebook_centre_resource(codebook, h, i)\n",
        "        # print('centres:', time()-t1)\n",
        "        \n",
        "        u2r = torch.zeros((x.shape[0], J, K, M), device=device) # Initialise u2r with 0\n",
        "        u2r = u2r + gamma\n",
        "        \n",
        "        # pdb(True)\n",
        "        t1 = time()\n",
        "        r2u = self.resource_to_user(x, u2r, codebook, h, sigma_square, beta) # [BS, K, J, M]\n",
        "        # print('0: r2u: ', time()-t1)\n",
        "        u2r = self.user_to_resource(r2u, gamma) # [BS, J, K, M]\n",
        "        t1=time()\n",
        "        # print('0: u2r: ', time()-t1)\n",
        "\n",
        "        for i in range(n_iter - 1):\n",
        "            # pdb()\n",
        "            t1=time()\n",
        "            r2u = self.resource_to_user(x, u2r, codebook, h, sigma_square, beta) # [BS, K, J, M]\n",
        "            # print(i, ': r2u: ', time()-t1)\n",
        "            t1=time()\n",
        "            u2r = self.user_to_resource(r2u, gamma) # [BS, J, K, M]\n",
        "            # print(i, ': u2r: ', time()-t1)\n",
        "        # pdb()\n",
        "        t1=time()\n",
        "        y = self.calc_output(r2u)\n",
        "        # print('output: ', time()-t1)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J14v_J1_330_"
      },
      "source": [
        "def add_noise(codewords, J, snr):\n",
        "    '''\n",
        "    codewords: [batch_size, 2*K]\n",
        "    snr: scalar\n",
        "    '''\n",
        "    # pdb()\n",
        "    batch_size = codewords.shape[0]\n",
        "    Es = torch.mean(codewords**2) # per real value or img value\n",
        "    Es_db = 10*torch.log10(Es)\n",
        "    noise_db = Es_db - torch.as_tensor(snr).to(device)\n",
        "    noise_power = 10**(noise_db/10)\n",
        "    noise = torch.randn(batch_size, 2*K).to(device) * torch.sqrt(noise_power).to(device)\n",
        "    output = codewords + noise\n",
        "\n",
        "    # pdb()\n",
        "\n",
        "    return noise, noise_power"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkcaPNWb33xj"
      },
      "source": [
        "def compute_ber(symbols, decoded_symbols, M):\n",
        "    '''\n",
        "    Inputs: symbols, decoded_symbols: [BS*J]\n",
        "    '''\n",
        "    symbols_0 = symbols - 1\n",
        "    decoded_symbols_0 = decoded_symbols - 1\n",
        "    true = np.unpackbits(symbols_0.reshape(-1, 1).astype('uint8'), axis=1)[:, -int(np.log2(M)):] # [BS*J, log2(M)]\n",
        "    predicted = np.unpackbits(decoded_symbols_0.reshape(-1, 1).astype('uint8'), axis=1)[:, -int(np.log2(M)):] # [BS*J, log2(M)]\n",
        "\n",
        "    Nerr_check = (true!=predicted) # [BS*J, log2(M)]\n",
        "    ber_batch = (true!=predicted).mean() # [1]\n",
        "    \n",
        "    Nerr_per_user = np.sum(Nerr_check, axis=-1) # [BS*J]\n",
        "    Nerr_per_user = np.sum(Nerr_per_user.reshape(-1, J), axis=0) # [J]\n",
        "    \n",
        "\n",
        "    return ber_batch, Nerr_per_user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_5LZkaD33t8"
      },
      "source": [
        "batch_size = 1024\n",
        "\n",
        "df = (V[0]==1).sum() # num of users connected to a resource (needs to be same for all resources)\n",
        "dv = (V[:, 0]==1).sum() # num of resources connected to a user (needs to be same for all users)\n",
        "\n",
        "#---Initialize the condensed_codebook\n",
        "condensed_codebook = torch.randn((J, M, dv, 2), dtype = torch.float32, device = device, requires_grad = True)\n",
        "\n",
        "#---Create MPA\n",
        "mpa = MPA()\n",
        "\n",
        "#---Create Optimizer and Loss Function\n",
        "params = [condensed_codebook]\n",
        "\n",
        "# optimizer = torch.optim.RMSprop(params, lr=1e-4, alpha=0.99, eps=1e-08, weight_decay=1e-5) # wt decay was 1e-5\n",
        "optimizer = torch.optim.Adam(params, lr=1e-2, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor = 0.5, patience = 150, mode = 'min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBlwYUQh33rp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "17a455e4-2614-4688-f9c3-7048faf0e741"
      },
      "source": [
        "symbols = np.random.randint(1, M+1, (batch_size, J))\n",
        "enc = OneHotEncoder(categories=np.stack(J*[range(1,M+1)]))\n",
        "_ = enc.fit(symbols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:76: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if self.categories != 'auto':\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if self.categories == 'auto':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTV1EgM82sJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad249c8c-ded4-4d80-a521-970a0011b443"
      },
      "source": [
        "is_fading = False\n",
        "is_noise = True\n",
        "print(\"Fading: \", is_fading, \"  Noise: \", is_noise)\n",
        "\n",
        "iterations = 500\n",
        "n_iter = 3\n",
        "maxNumErrs = 100 # min number of bits error to be seen before ending the run\n",
        "maxNumBits = 1e7 # max number of bits per user to be processed\n",
        "\n",
        "beta = 0 #+ np.log(1/(2* math.pi * sigma_square)**0.5)\n",
        "\n",
        "Nerr_per_user = np.zeros([J])\n",
        "Nbits = 0 # number of bits processed till now\n",
        "\n",
        "for iteration in range(iterations):\n",
        "\n",
        "    EbN = 6\n",
        "    snr = EbN + 10*np.log10(np.log2(M)*J/K)\n",
        "\n",
        "    # pdb()\n",
        "    #---Get codebook\n",
        "    codebook = get_codebook_from_condensed_codebook()\n",
        "\n",
        "    #---Generate symbols and channel parameters\n",
        "    if is_fading is True:\n",
        "        h = (torch.randn(batch_size, J, 2*K)/torch.sqrt(torch.tensor(2.0))).to(device)\n",
        "    else:\n",
        "        h = torch.zeros(batch_size, J, 2*K).to(device)\n",
        "        real_idx = 2 * torch.arange(K).long()\n",
        "        h[:, :, real_idx] = 1.0\n",
        "\n",
        "    symbols = np.random.randint(1, M+1, (batch_size, J))\n",
        "    \n",
        "    codewords_faded, codewords_faded_without_h = encode(symbols, codebook, h = h)    \n",
        "    received_signal = codewords_faded\n",
        "    \n",
        "    '''noise is being calc using codewords_faded_without_h '''\n",
        "    noise, sigma_square = add_noise(codewords_faded_without_h, J, snr)\n",
        "    \n",
        "    if is_noise is True:\n",
        "        received_signal = received_signal + noise\n",
        "    \n",
        "\n",
        "    #---Decode\n",
        "    beta = torch.log((1/(2* math.pi * sigma_square)**0.5))\n",
        "    gamma = torch.log(torch.tensor((1.0/M), device=device, dtype=torch.float32)) # log(p_x{j})\n",
        "    decoded_symbols_one_hot = mpa.forward(received_signal, codebook, h, sigma_square, beta, gamma)\n",
        "    \n",
        "    # pdb()\n",
        "\n",
        "    # Backward pass\n",
        "    target = torch.LongTensor(symbols.reshape(-1) - 1).to(device)\n",
        "    loss = loss_func(decoded_symbols_one_hot.view(-1, M), target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # scheduler.step(loss)\n",
        "\n",
        "\n",
        "    # Compute metrics\n",
        "    decoded_symbols = torch.argmax(decoded_symbols_one_hot, dim=-1) + 1\n",
        "    accuracy_batch = np.mean(symbols == decoded_symbols.cpu().data.numpy())\n",
        "    ber_batch, Nerr_per_user_batch = compute_ber(symbols.reshape(batch_size*J), decoded_symbols.cpu().data.numpy().reshape(batch_size*J), M)\n",
        "    \n",
        "    Nerr_per_user += Nerr_per_user_batch\n",
        "    Nbits += batch_size * np.log2(M)\n",
        "\n",
        "    # if (iteration + 1) % 50 == 0:\n",
        "    print('iter: ', iteration+1, '/', iterations, ' Acc_batch = {:.4f}'.format(accuracy_batch),\n",
        "          ' SER_batch = {:.4f}'.format(1-accuracy_batch), ' BER_batch = {:.4f}'.format(ber_batch),\n",
        "          ' loss: {:.3f}'.format(loss), ' lr: {:.4f}'.format(optimizer.param_groups[0]['lr']))\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fading:  False   Noise:  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter:  1 / 500  Acc_batch = 0.8688  SER_batch = 0.1312  BER_batch = 0.0821  loss: 0.328  lr: 0.0100\n",
            "iter:  2 / 500  Acc_batch = 0.8703  SER_batch = 0.1297  BER_batch = 0.0845  loss: 0.324  lr: 0.0100\n",
            "iter:  3 / 500  Acc_batch = 0.8791  SER_batch = 0.1209  BER_batch = 0.0780  loss: 0.305  lr: 0.0100\n",
            "iter:  4 / 500  Acc_batch = 0.8771  SER_batch = 0.1229  BER_batch = 0.0786  loss: 0.307  lr: 0.0100\n",
            "iter:  5 / 500  Acc_batch = 0.8779  SER_batch = 0.1221  BER_batch = 0.0781  loss: 0.307  lr: 0.0100\n",
            "iter:  6 / 500  Acc_batch = 0.8870  SER_batch = 0.1130  BER_batch = 0.0729  loss: 0.284  lr: 0.0100\n",
            "iter:  7 / 500  Acc_batch = 0.8787  SER_batch = 0.1213  BER_batch = 0.0786  loss: 0.296  lr: 0.0100\n",
            "iter:  8 / 500  Acc_batch = 0.8794  SER_batch = 0.1206  BER_batch = 0.0793  loss: 0.284  lr: 0.0100\n",
            "iter:  9 / 500  Acc_batch = 0.8989  SER_batch = 0.1011  BER_batch = 0.0640  loss: 0.265  lr: 0.0100\n",
            "iter:  10 / 500  Acc_batch = 0.8976  SER_batch = 0.1024  BER_batch = 0.0679  loss: 0.260  lr: 0.0100\n",
            "iter:  11 / 500  Acc_batch = 0.8950  SER_batch = 0.1050  BER_batch = 0.0700  loss: 0.261  lr: 0.0100\n",
            "iter:  12 / 500  Acc_batch = 0.8843  SER_batch = 0.1157  BER_batch = 0.0748  loss: 0.282  lr: 0.0100\n",
            "iter:  13 / 500  Acc_batch = 0.8984  SER_batch = 0.1016  BER_batch = 0.0660  loss: 0.252  lr: 0.0100\n",
            "iter:  14 / 500  Acc_batch = 0.9079  SER_batch = 0.0921  BER_batch = 0.0595  loss: 0.254  lr: 0.0100\n",
            "iter:  15 / 500  Acc_batch = 0.8882  SER_batch = 0.1118  BER_batch = 0.0706  loss: 0.276  lr: 0.0100\n",
            "iter:  16 / 500  Acc_batch = 0.9023  SER_batch = 0.0977  BER_batch = 0.0632  loss: 0.243  lr: 0.0100\n",
            "iter:  17 / 500  Acc_batch = 0.8968  SER_batch = 0.1032  BER_batch = 0.0655  loss: 0.260  lr: 0.0100\n",
            "iter:  18 / 500  Acc_batch = 0.9071  SER_batch = 0.0929  BER_batch = 0.0620  loss: 0.236  lr: 0.0100\n",
            "iter:  19 / 500  Acc_batch = 0.9069  SER_batch = 0.0931  BER_batch = 0.0598  loss: 0.231  lr: 0.0100\n",
            "iter:  20 / 500  Acc_batch = 0.9089  SER_batch = 0.0911  BER_batch = 0.0592  loss: 0.242  lr: 0.0100\n",
            "iter:  21 / 500  Acc_batch = 0.9119  SER_batch = 0.0881  BER_batch = 0.0563  loss: 0.216  lr: 0.0100\n",
            "iter:  22 / 500  Acc_batch = 0.9132  SER_batch = 0.0868  BER_batch = 0.0562  loss: 0.219  lr: 0.0100\n",
            "iter:  23 / 500  Acc_batch = 0.9152  SER_batch = 0.0848  BER_batch = 0.0536  loss: 0.209  lr: 0.0100\n",
            "iter:  24 / 500  Acc_batch = 0.9142  SER_batch = 0.0858  BER_batch = 0.0565  loss: 0.214  lr: 0.0100\n",
            "iter:  25 / 500  Acc_batch = 0.9095  SER_batch = 0.0905  BER_batch = 0.0588  loss: 0.223  lr: 0.0100\n",
            "iter:  26 / 500  Acc_batch = 0.9051  SER_batch = 0.0949  BER_batch = 0.0615  loss: 0.237  lr: 0.0100\n",
            "iter:  27 / 500  Acc_batch = 0.9155  SER_batch = 0.0845  BER_batch = 0.0551  loss: 0.220  lr: 0.0100\n",
            "iter:  28 / 500  Acc_batch = 0.9178  SER_batch = 0.0822  BER_batch = 0.0528  loss: 0.205  lr: 0.0100\n",
            "iter:  29 / 500  Acc_batch = 0.9165  SER_batch = 0.0835  BER_batch = 0.0547  loss: 0.219  lr: 0.0100\n",
            "iter:  30 / 500  Acc_batch = 0.9191  SER_batch = 0.0809  BER_batch = 0.0544  loss: 0.203  lr: 0.0100\n",
            "iter:  31 / 500  Acc_batch = 0.9232  SER_batch = 0.0768  BER_batch = 0.0497  loss: 0.203  lr: 0.0100\n",
            "iter:  32 / 500  Acc_batch = 0.9229  SER_batch = 0.0771  BER_batch = 0.0498  loss: 0.200  lr: 0.0100\n",
            "iter:  33 / 500  Acc_batch = 0.9305  SER_batch = 0.0695  BER_batch = 0.0441  loss: 0.181  lr: 0.0100\n",
            "iter:  34 / 500  Acc_batch = 0.9220  SER_batch = 0.0780  BER_batch = 0.0500  loss: 0.196  lr: 0.0100\n",
            "iter:  35 / 500  Acc_batch = 0.9222  SER_batch = 0.0778  BER_batch = 0.0499  loss: 0.194  lr: 0.0100\n",
            "iter:  36 / 500  Acc_batch = 0.9178  SER_batch = 0.0822  BER_batch = 0.0535  loss: 0.209  lr: 0.0100\n",
            "iter:  37 / 500  Acc_batch = 0.9134  SER_batch = 0.0866  BER_batch = 0.0572  loss: 0.223  lr: 0.0100\n",
            "iter:  38 / 500  Acc_batch = 0.9292  SER_batch = 0.0708  BER_batch = 0.0453  loss: 0.184  lr: 0.0100\n",
            "iter:  39 / 500  Acc_batch = 0.9191  SER_batch = 0.0809  BER_batch = 0.0549  loss: 0.208  lr: 0.0100\n",
            "iter:  40 / 500  Acc_batch = 0.9300  SER_batch = 0.0700  BER_batch = 0.0435  loss: 0.176  lr: 0.0100\n",
            "iter:  41 / 500  Acc_batch = 0.9364  SER_batch = 0.0636  BER_batch = 0.0411  loss: 0.168  lr: 0.0100\n",
            "iter:  42 / 500  Acc_batch = 0.9232  SER_batch = 0.0768  BER_batch = 0.0486  loss: 0.186  lr: 0.0100\n",
            "iter:  43 / 500  Acc_batch = 0.9318  SER_batch = 0.0682  BER_batch = 0.0431  loss: 0.171  lr: 0.0100\n",
            "iter:  44 / 500  Acc_batch = 0.9339  SER_batch = 0.0661  BER_batch = 0.0442  loss: 0.177  lr: 0.0100\n",
            "iter:  45 / 500  Acc_batch = 0.9295  SER_batch = 0.0705  BER_batch = 0.0448  loss: 0.191  lr: 0.0100\n",
            "iter:  46 / 500  Acc_batch = 0.9338  SER_batch = 0.0662  BER_batch = 0.0426  loss: 0.173  lr: 0.0100\n",
            "iter:  47 / 500  Acc_batch = 0.9240  SER_batch = 0.0760  BER_batch = 0.0498  loss: 0.197  lr: 0.0100\n",
            "iter:  48 / 500  Acc_batch = 0.9393  SER_batch = 0.0607  BER_batch = 0.0382  loss: 0.160  lr: 0.0100\n",
            "iter:  49 / 500  Acc_batch = 0.9328  SER_batch = 0.0672  BER_batch = 0.0441  loss: 0.172  lr: 0.0100\n",
            "iter:  50 / 500  Acc_batch = 0.9344  SER_batch = 0.0656  BER_batch = 0.0422  loss: 0.171  lr: 0.0100\n",
            "iter:  51 / 500  Acc_batch = 0.9365  SER_batch = 0.0635  BER_batch = 0.0413  loss: 0.171  lr: 0.0100\n",
            "iter:  52 / 500  Acc_batch = 0.9313  SER_batch = 0.0687  BER_batch = 0.0433  loss: 0.182  lr: 0.0100\n",
            "iter:  53 / 500  Acc_batch = 0.9362  SER_batch = 0.0638  BER_batch = 0.0402  loss: 0.168  lr: 0.0100\n",
            "iter:  54 / 500  Acc_batch = 0.9323  SER_batch = 0.0677  BER_batch = 0.0440  loss: 0.178  lr: 0.0100\n",
            "iter:  55 / 500  Acc_batch = 0.9290  SER_batch = 0.0710  BER_batch = 0.0447  loss: 0.184  lr: 0.0100\n",
            "iter:  56 / 500  Acc_batch = 0.9328  SER_batch = 0.0672  BER_batch = 0.0431  loss: 0.177  lr: 0.0100\n",
            "iter:  57 / 500  Acc_batch = 0.9432  SER_batch = 0.0568  BER_batch = 0.0366  loss: 0.144  lr: 0.0100\n",
            "iter:  58 / 500  Acc_batch = 0.9349  SER_batch = 0.0651  BER_batch = 0.0423  loss: 0.174  lr: 0.0100\n",
            "iter:  59 / 500  Acc_batch = 0.9303  SER_batch = 0.0697  BER_batch = 0.0439  loss: 0.180  lr: 0.0100\n",
            "iter:  60 / 500  Acc_batch = 0.9318  SER_batch = 0.0682  BER_batch = 0.0431  loss: 0.179  lr: 0.0100\n",
            "iter:  61 / 500  Acc_batch = 0.9438  SER_batch = 0.0562  BER_batch = 0.0367  loss: 0.158  lr: 0.0100\n",
            "iter:  62 / 500  Acc_batch = 0.9312  SER_batch = 0.0688  BER_batch = 0.0431  loss: 0.167  lr: 0.0100\n",
            "iter:  63 / 500  Acc_batch = 0.9468  SER_batch = 0.0532  BER_batch = 0.0341  loss: 0.146  lr: 0.0100\n",
            "iter:  64 / 500  Acc_batch = 0.9368  SER_batch = 0.0632  BER_batch = 0.0405  loss: 0.166  lr: 0.0100\n",
            "iter:  65 / 500  Acc_batch = 0.9342  SER_batch = 0.0658  BER_batch = 0.0423  loss: 0.165  lr: 0.0100\n",
            "iter:  66 / 500  Acc_batch = 0.9383  SER_batch = 0.0617  BER_batch = 0.0391  loss: 0.162  lr: 0.0100\n",
            "iter:  67 / 500  Acc_batch = 0.9487  SER_batch = 0.0513  BER_batch = 0.0323  loss: 0.135  lr: 0.0100\n",
            "iter:  68 / 500  Acc_batch = 0.9461  SER_batch = 0.0539  BER_batch = 0.0334  loss: 0.148  lr: 0.0100\n",
            "iter:  69 / 500  Acc_batch = 0.9463  SER_batch = 0.0537  BER_batch = 0.0343  loss: 0.146  lr: 0.0100\n",
            "iter:  70 / 500  Acc_batch = 0.9409  SER_batch = 0.0591  BER_batch = 0.0391  loss: 0.154  lr: 0.0100\n",
            "iter:  71 / 500  Acc_batch = 0.9500  SER_batch = 0.0500  BER_batch = 0.0323  loss: 0.134  lr: 0.0100\n",
            "iter:  72 / 500  Acc_batch = 0.9373  SER_batch = 0.0627  BER_batch = 0.0404  loss: 0.165  lr: 0.0100\n",
            "iter:  73 / 500  Acc_batch = 0.9412  SER_batch = 0.0588  BER_batch = 0.0382  loss: 0.149  lr: 0.0100\n",
            "iter:  74 / 500  Acc_batch = 0.9406  SER_batch = 0.0594  BER_batch = 0.0374  loss: 0.164  lr: 0.0100\n",
            "iter:  75 / 500  Acc_batch = 0.9411  SER_batch = 0.0589  BER_batch = 0.0372  loss: 0.152  lr: 0.0100\n",
            "iter:  76 / 500  Acc_batch = 0.9440  SER_batch = 0.0560  BER_batch = 0.0370  loss: 0.143  lr: 0.0100\n",
            "iter:  77 / 500  Acc_batch = 0.9425  SER_batch = 0.0575  BER_batch = 0.0385  loss: 0.156  lr: 0.0100\n",
            "iter:  78 / 500  Acc_batch = 0.9398  SER_batch = 0.0602  BER_batch = 0.0404  loss: 0.155  lr: 0.0100\n",
            "iter:  79 / 500  Acc_batch = 0.9453  SER_batch = 0.0547  BER_batch = 0.0352  loss: 0.140  lr: 0.0100\n",
            "iter:  80 / 500  Acc_batch = 0.9530  SER_batch = 0.0470  BER_batch = 0.0305  loss: 0.135  lr: 0.0100\n",
            "iter:  81 / 500  Acc_batch = 0.9465  SER_batch = 0.0535  BER_batch = 0.0338  loss: 0.141  lr: 0.0100\n",
            "iter:  82 / 500  Acc_batch = 0.9419  SER_batch = 0.0581  BER_batch = 0.0372  loss: 0.155  lr: 0.0100\n",
            "iter:  83 / 500  Acc_batch = 0.9419  SER_batch = 0.0581  BER_batch = 0.0381  loss: 0.151  lr: 0.0100\n",
            "iter:  84 / 500  Acc_batch = 0.9448  SER_batch = 0.0552  BER_batch = 0.0356  loss: 0.144  lr: 0.0100\n",
            "iter:  85 / 500  Acc_batch = 0.9432  SER_batch = 0.0568  BER_batch = 0.0367  loss: 0.151  lr: 0.0100\n",
            "iter:  86 / 500  Acc_batch = 0.9471  SER_batch = 0.0529  BER_batch = 0.0347  loss: 0.138  lr: 0.0100\n",
            "iter:  87 / 500  Acc_batch = 0.9476  SER_batch = 0.0524  BER_batch = 0.0342  loss: 0.136  lr: 0.0100\n",
            "iter:  88 / 500  Acc_batch = 0.9482  SER_batch = 0.0518  BER_batch = 0.0339  loss: 0.149  lr: 0.0100\n",
            "iter:  89 / 500  Acc_batch = 0.9489  SER_batch = 0.0511  BER_batch = 0.0331  loss: 0.141  lr: 0.0100\n",
            "iter:  90 / 500  Acc_batch = 0.9492  SER_batch = 0.0508  BER_batch = 0.0324  loss: 0.130  lr: 0.0100\n",
            "iter:  91 / 500  Acc_batch = 0.9494  SER_batch = 0.0506  BER_batch = 0.0321  loss: 0.135  lr: 0.0100\n",
            "iter:  92 / 500  Acc_batch = 0.9510  SER_batch = 0.0490  BER_batch = 0.0310  loss: 0.132  lr: 0.0100\n",
            "iter:  93 / 500  Acc_batch = 0.9465  SER_batch = 0.0535  BER_batch = 0.0350  loss: 0.138  lr: 0.0100\n",
            "iter:  94 / 500  Acc_batch = 0.9451  SER_batch = 0.0549  BER_batch = 0.0356  loss: 0.140  lr: 0.0100\n",
            "iter:  95 / 500  Acc_batch = 0.9404  SER_batch = 0.0596  BER_batch = 0.0374  loss: 0.153  lr: 0.0100\n",
            "iter:  96 / 500  Acc_batch = 0.9432  SER_batch = 0.0568  BER_batch = 0.0357  loss: 0.135  lr: 0.0100\n",
            "iter:  97 / 500  Acc_batch = 0.9499  SER_batch = 0.0501  BER_batch = 0.0322  loss: 0.137  lr: 0.0100\n",
            "iter:  98 / 500  Acc_batch = 0.9499  SER_batch = 0.0501  BER_batch = 0.0324  loss: 0.142  lr: 0.0100\n",
            "iter:  99 / 500  Acc_batch = 0.9539  SER_batch = 0.0461  BER_batch = 0.0295  loss: 0.119  lr: 0.0100\n",
            "iter:  100 / 500  Acc_batch = 0.9469  SER_batch = 0.0531  BER_batch = 0.0348  loss: 0.137  lr: 0.0100\n",
            "iter:  101 / 500  Acc_batch = 0.9469  SER_batch = 0.0531  BER_batch = 0.0363  loss: 0.137  lr: 0.0100\n",
            "iter:  102 / 500  Acc_batch = 0.9494  SER_batch = 0.0506  BER_batch = 0.0321  loss: 0.128  lr: 0.0100\n",
            "iter:  103 / 500  Acc_batch = 0.9552  SER_batch = 0.0448  BER_batch = 0.0282  loss: 0.120  lr: 0.0100\n",
            "iter:  104 / 500  Acc_batch = 0.9507  SER_batch = 0.0493  BER_batch = 0.0312  loss: 0.133  lr: 0.0100\n",
            "iter:  105 / 500  Acc_batch = 0.9500  SER_batch = 0.0500  BER_batch = 0.0318  loss: 0.135  lr: 0.0100\n",
            "iter:  106 / 500  Acc_batch = 0.9479  SER_batch = 0.0521  BER_batch = 0.0339  loss: 0.136  lr: 0.0100\n",
            "iter:  107 / 500  Acc_batch = 0.9445  SER_batch = 0.0555  BER_batch = 0.0352  loss: 0.144  lr: 0.0100\n",
            "iter:  108 / 500  Acc_batch = 0.9544  SER_batch = 0.0456  BER_batch = 0.0299  loss: 0.120  lr: 0.0100\n",
            "iter:  109 / 500  Acc_batch = 0.9564  SER_batch = 0.0436  BER_batch = 0.0273  loss: 0.118  lr: 0.0100\n",
            "iter:  110 / 500  Acc_batch = 0.9543  SER_batch = 0.0457  BER_batch = 0.0308  loss: 0.134  lr: 0.0100\n",
            "iter:  111 / 500  Acc_batch = 0.9445  SER_batch = 0.0555  BER_batch = 0.0361  loss: 0.144  lr: 0.0100\n",
            "iter:  112 / 500  Acc_batch = 0.9458  SER_batch = 0.0542  BER_batch = 0.0358  loss: 0.140  lr: 0.0100\n",
            "iter:  113 / 500  Acc_batch = 0.9489  SER_batch = 0.0511  BER_batch = 0.0325  loss: 0.139  lr: 0.0100\n",
            "iter:  114 / 500  Acc_batch = 0.9569  SER_batch = 0.0431  BER_batch = 0.0274  loss: 0.116  lr: 0.0100\n",
            "iter:  115 / 500  Acc_batch = 0.9531  SER_batch = 0.0469  BER_batch = 0.0297  loss: 0.129  lr: 0.0100\n",
            "iter:  116 / 500  Acc_batch = 0.9499  SER_batch = 0.0501  BER_batch = 0.0330  loss: 0.122  lr: 0.0100\n",
            "iter:  117 / 500  Acc_batch = 0.9435  SER_batch = 0.0565  BER_batch = 0.0371  loss: 0.147  lr: 0.0100\n",
            "iter:  118 / 500  Acc_batch = 0.9549  SER_batch = 0.0451  BER_batch = 0.0292  loss: 0.126  lr: 0.0100\n",
            "iter:  119 / 500  Acc_batch = 0.9437  SER_batch = 0.0563  BER_batch = 0.0365  loss: 0.139  lr: 0.0100\n",
            "iter:  120 / 500  Acc_batch = 0.9463  SER_batch = 0.0537  BER_batch = 0.0353  loss: 0.135  lr: 0.0100\n",
            "iter:  121 / 500  Acc_batch = 0.9539  SER_batch = 0.0461  BER_batch = 0.0308  loss: 0.123  lr: 0.0100\n",
            "iter:  122 / 500  Acc_batch = 0.9437  SER_batch = 0.0563  BER_batch = 0.0368  loss: 0.149  lr: 0.0100\n",
            "iter:  123 / 500  Acc_batch = 0.9481  SER_batch = 0.0519  BER_batch = 0.0347  loss: 0.131  lr: 0.0100\n",
            "iter:  124 / 500  Acc_batch = 0.9601  SER_batch = 0.0399  BER_batch = 0.0263  loss: 0.114  lr: 0.0100\n",
            "iter:  125 / 500  Acc_batch = 0.9570  SER_batch = 0.0430  BER_batch = 0.0272  loss: 0.119  lr: 0.0100\n",
            "iter:  126 / 500  Acc_batch = 0.9478  SER_batch = 0.0522  BER_batch = 0.0343  loss: 0.128  lr: 0.0100\n",
            "iter:  127 / 500  Acc_batch = 0.9551  SER_batch = 0.0449  BER_batch = 0.0296  loss: 0.117  lr: 0.0100\n",
            "iter:  128 / 500  Acc_batch = 0.9510  SER_batch = 0.0490  BER_batch = 0.0330  loss: 0.126  lr: 0.0100\n",
            "iter:  129 / 500  Acc_batch = 0.9583  SER_batch = 0.0417  BER_batch = 0.0290  loss: 0.112  lr: 0.0100\n",
            "iter:  130 / 500  Acc_batch = 0.9572  SER_batch = 0.0428  BER_batch = 0.0289  loss: 0.119  lr: 0.0100\n",
            "iter:  131 / 500  Acc_batch = 0.9598  SER_batch = 0.0402  BER_batch = 0.0266  loss: 0.110  lr: 0.0100\n",
            "iter:  132 / 500  Acc_batch = 0.9601  SER_batch = 0.0399  BER_batch = 0.0269  loss: 0.108  lr: 0.0100\n",
            "iter:  133 / 500  Acc_batch = 0.9585  SER_batch = 0.0415  BER_batch = 0.0274  loss: 0.114  lr: 0.0100\n",
            "iter:  134 / 500  Acc_batch = 0.9518  SER_batch = 0.0482  BER_batch = 0.0315  loss: 0.126  lr: 0.0100\n",
            "iter:  135 / 500  Acc_batch = 0.9526  SER_batch = 0.0474  BER_batch = 0.0309  loss: 0.126  lr: 0.0100\n",
            "iter:  136 / 500  Acc_batch = 0.9551  SER_batch = 0.0449  BER_batch = 0.0312  loss: 0.124  lr: 0.0100\n",
            "iter:  137 / 500  Acc_batch = 0.9543  SER_batch = 0.0457  BER_batch = 0.0297  loss: 0.122  lr: 0.0100\n",
            "iter:  138 / 500  Acc_batch = 0.9603  SER_batch = 0.0397  BER_batch = 0.0273  loss: 0.110  lr: 0.0100\n",
            "iter:  139 / 500  Acc_batch = 0.9543  SER_batch = 0.0457  BER_batch = 0.0299  loss: 0.122  lr: 0.0100\n",
            "iter:  140 / 500  Acc_batch = 0.9557  SER_batch = 0.0443  BER_batch = 0.0297  loss: 0.117  lr: 0.0100\n",
            "iter:  141 / 500  Acc_batch = 0.9564  SER_batch = 0.0436  BER_batch = 0.0286  loss: 0.120  lr: 0.0100\n",
            "iter:  142 / 500  Acc_batch = 0.9551  SER_batch = 0.0449  BER_batch = 0.0290  loss: 0.119  lr: 0.0100\n",
            "iter:  143 / 500  Acc_batch = 0.9492  SER_batch = 0.0508  BER_batch = 0.0342  loss: 0.132  lr: 0.0100\n",
            "iter:  144 / 500  Acc_batch = 0.9577  SER_batch = 0.0423  BER_batch = 0.0289  loss: 0.113  lr: 0.0100\n",
            "iter:  145 / 500  Acc_batch = 0.9518  SER_batch = 0.0482  BER_batch = 0.0315  loss: 0.131  lr: 0.0100\n",
            "iter:  146 / 500  Acc_batch = 0.9575  SER_batch = 0.0425  BER_batch = 0.0286  loss: 0.109  lr: 0.0100\n",
            "iter:  147 / 500  Acc_batch = 0.9588  SER_batch = 0.0412  BER_batch = 0.0283  loss: 0.106  lr: 0.0100\n",
            "iter:  148 / 500  Acc_batch = 0.9570  SER_batch = 0.0430  BER_batch = 0.0289  loss: 0.121  lr: 0.0100\n",
            "iter:  149 / 500  Acc_batch = 0.9517  SER_batch = 0.0483  BER_batch = 0.0318  loss: 0.121  lr: 0.0100\n",
            "iter:  150 / 500  Acc_batch = 0.9582  SER_batch = 0.0418  BER_batch = 0.0282  loss: 0.106  lr: 0.0100\n",
            "iter:  151 / 500  Acc_batch = 0.9661  SER_batch = 0.0339  BER_batch = 0.0238  loss: 0.100  lr: 0.0100\n",
            "iter:  152 / 500  Acc_batch = 0.9539  SER_batch = 0.0461  BER_batch = 0.0301  loss: 0.123  lr: 0.0100\n",
            "iter:  153 / 500  Acc_batch = 0.9544  SER_batch = 0.0456  BER_batch = 0.0305  loss: 0.120  lr: 0.0100\n",
            "iter:  154 / 500  Acc_batch = 0.9577  SER_batch = 0.0423  BER_batch = 0.0283  loss: 0.113  lr: 0.0100\n",
            "iter:  155 / 500  Acc_batch = 0.9481  SER_batch = 0.0519  BER_batch = 0.0355  loss: 0.120  lr: 0.0100\n",
            "iter:  156 / 500  Acc_batch = 0.9587  SER_batch = 0.0413  BER_batch = 0.0273  loss: 0.118  lr: 0.0100\n",
            "iter:  157 / 500  Acc_batch = 0.9578  SER_batch = 0.0422  BER_batch = 0.0298  loss: 0.118  lr: 0.0100\n",
            "iter:  158 / 500  Acc_batch = 0.9557  SER_batch = 0.0443  BER_batch = 0.0298  loss: 0.121  lr: 0.0100\n",
            "iter:  159 / 500  Acc_batch = 0.9520  SER_batch = 0.0480  BER_batch = 0.0318  loss: 0.127  lr: 0.0100\n",
            "iter:  160 / 500  Acc_batch = 0.9593  SER_batch = 0.0407  BER_batch = 0.0270  loss: 0.105  lr: 0.0100\n",
            "iter:  161 / 500  Acc_batch = 0.9541  SER_batch = 0.0459  BER_batch = 0.0305  loss: 0.117  lr: 0.0100\n",
            "iter:  162 / 500  Acc_batch = 0.9499  SER_batch = 0.0501  BER_batch = 0.0334  loss: 0.124  lr: 0.0100\n",
            "iter:  163 / 500  Acc_batch = 0.9567  SER_batch = 0.0433  BER_batch = 0.0284  loss: 0.118  lr: 0.0100\n",
            "iter:  164 / 500  Acc_batch = 0.9609  SER_batch = 0.0391  BER_batch = 0.0258  loss: 0.104  lr: 0.0100\n",
            "iter:  165 / 500  Acc_batch = 0.9551  SER_batch = 0.0449  BER_batch = 0.0314  loss: 0.119  lr: 0.0100\n",
            "iter:  166 / 500  Acc_batch = 0.9552  SER_batch = 0.0448  BER_batch = 0.0292  loss: 0.117  lr: 0.0100\n",
            "iter:  167 / 500  Acc_batch = 0.9596  SER_batch = 0.0404  BER_batch = 0.0273  loss: 0.104  lr: 0.0100\n",
            "iter:  168 / 500  Acc_batch = 0.9631  SER_batch = 0.0369  BER_batch = 0.0240  loss: 0.100  lr: 0.0100\n",
            "iter:  169 / 500  Acc_batch = 0.9543  SER_batch = 0.0457  BER_batch = 0.0297  loss: 0.121  lr: 0.0100\n",
            "iter:  170 / 500  Acc_batch = 0.9570  SER_batch = 0.0430  BER_batch = 0.0276  loss: 0.116  lr: 0.0100\n",
            "iter:  171 / 500  Acc_batch = 0.9557  SER_batch = 0.0443  BER_batch = 0.0295  loss: 0.115  lr: 0.0100\n",
            "iter:  172 / 500  Acc_batch = 0.9583  SER_batch = 0.0417  BER_batch = 0.0280  loss: 0.121  lr: 0.0100\n",
            "iter:  173 / 500  Acc_batch = 0.9556  SER_batch = 0.0444  BER_batch = 0.0286  loss: 0.127  lr: 0.0100\n",
            "iter:  174 / 500  Acc_batch = 0.9538  SER_batch = 0.0462  BER_batch = 0.0303  loss: 0.113  lr: 0.0100\n",
            "iter:  175 / 500  Acc_batch = 0.9580  SER_batch = 0.0420  BER_batch = 0.0289  loss: 0.102  lr: 0.0100\n",
            "iter:  176 / 500  Acc_batch = 0.9567  SER_batch = 0.0433  BER_batch = 0.0286  loss: 0.117  lr: 0.0100\n",
            "iter:  177 / 500  Acc_batch = 0.9588  SER_batch = 0.0412  BER_batch = 0.0282  loss: 0.108  lr: 0.0100\n",
            "iter:  178 / 500  Acc_batch = 0.9593  SER_batch = 0.0407  BER_batch = 0.0274  loss: 0.102  lr: 0.0100\n",
            "iter:  179 / 500  Acc_batch = 0.9544  SER_batch = 0.0456  BER_batch = 0.0311  loss: 0.117  lr: 0.0100\n",
            "iter:  180 / 500  Acc_batch = 0.9559  SER_batch = 0.0441  BER_batch = 0.0292  loss: 0.111  lr: 0.0100\n",
            "iter:  181 / 500  Acc_batch = 0.9580  SER_batch = 0.0420  BER_batch = 0.0273  loss: 0.117  lr: 0.0100\n",
            "iter:  182 / 500  Acc_batch = 0.9626  SER_batch = 0.0374  BER_batch = 0.0260  loss: 0.104  lr: 0.0100\n",
            "iter:  183 / 500  Acc_batch = 0.9627  SER_batch = 0.0373  BER_batch = 0.0242  loss: 0.105  lr: 0.0100\n",
            "iter:  184 / 500  Acc_batch = 0.9525  SER_batch = 0.0475  BER_batch = 0.0316  loss: 0.133  lr: 0.0100\n",
            "iter:  185 / 500  Acc_batch = 0.9634  SER_batch = 0.0366  BER_batch = 0.0241  loss: 0.101  lr: 0.0100\n",
            "iter:  186 / 500  Acc_batch = 0.9577  SER_batch = 0.0423  BER_batch = 0.0276  loss: 0.119  lr: 0.0100\n",
            "iter:  187 / 500  Acc_batch = 0.9587  SER_batch = 0.0413  BER_batch = 0.0273  loss: 0.102  lr: 0.0100\n",
            "iter:  188 / 500  Acc_batch = 0.9598  SER_batch = 0.0402  BER_batch = 0.0277  loss: 0.111  lr: 0.0100\n",
            "iter:  189 / 500  Acc_batch = 0.9658  SER_batch = 0.0342  BER_batch = 0.0229  loss: 0.097  lr: 0.0100\n",
            "iter:  190 / 500  Acc_batch = 0.9590  SER_batch = 0.0410  BER_batch = 0.0273  loss: 0.112  lr: 0.0100\n",
            "iter:  191 / 500  Acc_batch = 0.9600  SER_batch = 0.0400  BER_batch = 0.0252  loss: 0.106  lr: 0.0100\n",
            "iter:  192 / 500  Acc_batch = 0.9621  SER_batch = 0.0379  BER_batch = 0.0262  loss: 0.101  lr: 0.0100\n",
            "iter:  193 / 500  Acc_batch = 0.9520  SER_batch = 0.0480  BER_batch = 0.0321  loss: 0.120  lr: 0.0100\n",
            "iter:  194 / 500  Acc_batch = 0.9505  SER_batch = 0.0495  BER_batch = 0.0324  loss: 0.126  lr: 0.0100\n",
            "iter:  195 / 500  Acc_batch = 0.9572  SER_batch = 0.0428  BER_batch = 0.0291  loss: 0.114  lr: 0.0100\n",
            "iter:  196 / 500  Acc_batch = 0.9619  SER_batch = 0.0381  BER_batch = 0.0256  loss: 0.106  lr: 0.0100\n",
            "iter:  197 / 500  Acc_batch = 0.9611  SER_batch = 0.0389  BER_batch = 0.0263  loss: 0.099  lr: 0.0100\n",
            "iter:  198 / 500  Acc_batch = 0.9554  SER_batch = 0.0446  BER_batch = 0.0298  loss: 0.113  lr: 0.0100\n",
            "iter:  199 / 500  Acc_batch = 0.9601  SER_batch = 0.0399  BER_batch = 0.0267  loss: 0.105  lr: 0.0100\n",
            "iter:  200 / 500  Acc_batch = 0.9640  SER_batch = 0.0360  BER_batch = 0.0240  loss: 0.103  lr: 0.0100\n",
            "iter:  201 / 500  Acc_batch = 0.9634  SER_batch = 0.0366  BER_batch = 0.0243  loss: 0.100  lr: 0.0100\n",
            "iter:  202 / 500  Acc_batch = 0.9596  SER_batch = 0.0404  BER_batch = 0.0262  loss: 0.098  lr: 0.0100\n",
            "iter:  203 / 500  Acc_batch = 0.9562  SER_batch = 0.0438  BER_batch = 0.0290  loss: 0.108  lr: 0.0100\n",
            "iter:  204 / 500  Acc_batch = 0.9595  SER_batch = 0.0405  BER_batch = 0.0272  loss: 0.111  lr: 0.0100\n",
            "iter:  205 / 500  Acc_batch = 0.9595  SER_batch = 0.0405  BER_batch = 0.0271  loss: 0.105  lr: 0.0100\n",
            "iter:  206 / 500  Acc_batch = 0.9583  SER_batch = 0.0417  BER_batch = 0.0279  loss: 0.098  lr: 0.0100\n",
            "iter:  207 / 500  Acc_batch = 0.9618  SER_batch = 0.0382  BER_batch = 0.0251  loss: 0.108  lr: 0.0100\n",
            "iter:  208 / 500  Acc_batch = 0.9596  SER_batch = 0.0404  BER_batch = 0.0252  loss: 0.107  lr: 0.0100\n",
            "iter:  209 / 500  Acc_batch = 0.9598  SER_batch = 0.0402  BER_batch = 0.0264  loss: 0.107  lr: 0.0100\n",
            "iter:  210 / 500  Acc_batch = 0.9621  SER_batch = 0.0379  BER_batch = 0.0247  loss: 0.108  lr: 0.0100\n",
            "iter:  211 / 500  Acc_batch = 0.9619  SER_batch = 0.0381  BER_batch = 0.0243  loss: 0.099  lr: 0.0100\n",
            "iter:  212 / 500  Acc_batch = 0.9657  SER_batch = 0.0343  BER_batch = 0.0217  loss: 0.099  lr: 0.0100\n",
            "iter:  213 / 500  Acc_batch = 0.9632  SER_batch = 0.0368  BER_batch = 0.0244  loss: 0.101  lr: 0.0100\n",
            "iter:  214 / 500  Acc_batch = 0.9621  SER_batch = 0.0379  BER_batch = 0.0252  loss: 0.104  lr: 0.0100\n",
            "iter:  215 / 500  Acc_batch = 0.9624  SER_batch = 0.0376  BER_batch = 0.0246  loss: 0.112  lr: 0.0100\n",
            "iter:  216 / 500  Acc_batch = 0.9603  SER_batch = 0.0397  BER_batch = 0.0254  loss: 0.104  lr: 0.0100\n",
            "iter:  217 / 500  Acc_batch = 0.9676  SER_batch = 0.0324  BER_batch = 0.0214  loss: 0.095  lr: 0.0100\n",
            "iter:  218 / 500  Acc_batch = 0.9618  SER_batch = 0.0382  BER_batch = 0.0251  loss: 0.108  lr: 0.0100\n",
            "iter:  219 / 500  Acc_batch = 0.9631  SER_batch = 0.0369  BER_batch = 0.0239  loss: 0.095  lr: 0.0100\n",
            "iter:  220 / 500  Acc_batch = 0.9618  SER_batch = 0.0382  BER_batch = 0.0252  loss: 0.100  lr: 0.0100\n",
            "iter:  221 / 500  Acc_batch = 0.9629  SER_batch = 0.0371  BER_batch = 0.0236  loss: 0.101  lr: 0.0100\n",
            "iter:  222 / 500  Acc_batch = 0.9575  SER_batch = 0.0425  BER_batch = 0.0278  loss: 0.106  lr: 0.0100\n",
            "iter:  223 / 500  Acc_batch = 0.9642  SER_batch = 0.0358  BER_batch = 0.0237  loss: 0.095  lr: 0.0100\n",
            "iter:  224 / 500  Acc_batch = 0.9611  SER_batch = 0.0389  BER_batch = 0.0257  loss: 0.108  lr: 0.0100\n",
            "iter:  225 / 500  Acc_batch = 0.9635  SER_batch = 0.0365  BER_batch = 0.0250  loss: 0.099  lr: 0.0100\n",
            "iter:  226 / 500  Acc_batch = 0.9626  SER_batch = 0.0374  BER_batch = 0.0250  loss: 0.092  lr: 0.0100\n",
            "iter:  227 / 500  Acc_batch = 0.9655  SER_batch = 0.0345  BER_batch = 0.0232  loss: 0.094  lr: 0.0100\n",
            "iter:  228 / 500  Acc_batch = 0.9640  SER_batch = 0.0360  BER_batch = 0.0243  loss: 0.100  lr: 0.0100\n",
            "iter:  229 / 500  Acc_batch = 0.9557  SER_batch = 0.0443  BER_batch = 0.0279  loss: 0.112  lr: 0.0100\n",
            "iter:  230 / 500  Acc_batch = 0.9634  SER_batch = 0.0366  BER_batch = 0.0249  loss: 0.095  lr: 0.0100\n",
            "iter:  231 / 500  Acc_batch = 0.9622  SER_batch = 0.0378  BER_batch = 0.0247  loss: 0.115  lr: 0.0100\n",
            "iter:  232 / 500  Acc_batch = 0.9621  SER_batch = 0.0379  BER_batch = 0.0258  loss: 0.097  lr: 0.0100\n",
            "iter:  233 / 500  Acc_batch = 0.9683  SER_batch = 0.0317  BER_batch = 0.0204  loss: 0.097  lr: 0.0100\n",
            "iter:  234 / 500  Acc_batch = 0.9647  SER_batch = 0.0353  BER_batch = 0.0234  loss: 0.090  lr: 0.0100\n",
            "iter:  235 / 500  Acc_batch = 0.9671  SER_batch = 0.0329  BER_batch = 0.0213  loss: 0.091  lr: 0.0100\n",
            "iter:  236 / 500  Acc_batch = 0.9561  SER_batch = 0.0439  BER_batch = 0.0290  loss: 0.109  lr: 0.0100\n",
            "iter:  237 / 500  Acc_batch = 0.9631  SER_batch = 0.0369  BER_batch = 0.0240  loss: 0.104  lr: 0.0100\n",
            "iter:  238 / 500  Acc_batch = 0.9631  SER_batch = 0.0369  BER_batch = 0.0233  loss: 0.107  lr: 0.0100\n",
            "iter:  239 / 500  Acc_batch = 0.9582  SER_batch = 0.0418  BER_batch = 0.0272  loss: 0.108  lr: 0.0100\n",
            "iter:  240 / 500  Acc_batch = 0.9686  SER_batch = 0.0314  BER_batch = 0.0200  loss: 0.096  lr: 0.0100\n",
            "iter:  241 / 500  Acc_batch = 0.9600  SER_batch = 0.0400  BER_batch = 0.0264  loss: 0.111  lr: 0.0100\n",
            "iter:  242 / 500  Acc_batch = 0.9575  SER_batch = 0.0425  BER_batch = 0.0282  loss: 0.110  lr: 0.0100\n",
            "iter:  243 / 500  Acc_batch = 0.9635  SER_batch = 0.0365  BER_batch = 0.0233  loss: 0.095  lr: 0.0100\n",
            "iter:  244 / 500  Acc_batch = 0.9624  SER_batch = 0.0376  BER_batch = 0.0261  loss: 0.100  lr: 0.0100\n",
            "iter:  245 / 500  Acc_batch = 0.9645  SER_batch = 0.0355  BER_batch = 0.0227  loss: 0.104  lr: 0.0100\n",
            "iter:  246 / 500  Acc_batch = 0.9696  SER_batch = 0.0304  BER_batch = 0.0200  loss: 0.093  lr: 0.0100\n",
            "iter:  247 / 500  Acc_batch = 0.9637  SER_batch = 0.0363  BER_batch = 0.0225  loss: 0.097  lr: 0.0100\n",
            "iter:  248 / 500  Acc_batch = 0.9635  SER_batch = 0.0365  BER_batch = 0.0247  loss: 0.094  lr: 0.0100\n",
            "iter:  249 / 500  Acc_batch = 0.9637  SER_batch = 0.0363  BER_batch = 0.0233  loss: 0.108  lr: 0.0100\n",
            "iter:  250 / 500  Acc_batch = 0.9603  SER_batch = 0.0397  BER_batch = 0.0267  loss: 0.098  lr: 0.0100\n",
            "iter:  251 / 500  Acc_batch = 0.9635  SER_batch = 0.0365  BER_batch = 0.0234  loss: 0.097  lr: 0.0100\n",
            "iter:  252 / 500  Acc_batch = 0.9689  SER_batch = 0.0311  BER_batch = 0.0207  loss: 0.082  lr: 0.0100\n",
            "iter:  253 / 500  Acc_batch = 0.9570  SER_batch = 0.0430  BER_batch = 0.0286  loss: 0.107  lr: 0.0100\n",
            "iter:  254 / 500  Acc_batch = 0.9609  SER_batch = 0.0391  BER_batch = 0.0264  loss: 0.102  lr: 0.0100\n",
            "iter:  255 / 500  Acc_batch = 0.9552  SER_batch = 0.0448  BER_batch = 0.0292  loss: 0.111  lr: 0.0100\n",
            "iter:  256 / 500  Acc_batch = 0.9510  SER_batch = 0.0490  BER_batch = 0.0317  loss: 0.120  lr: 0.0100\n",
            "iter:  257 / 500  Acc_batch = 0.9578  SER_batch = 0.0422  BER_batch = 0.0263  loss: 0.114  lr: 0.0100\n",
            "iter:  258 / 500  Acc_batch = 0.9644  SER_batch = 0.0356  BER_batch = 0.0237  loss: 0.103  lr: 0.0100\n",
            "iter:  259 / 500  Acc_batch = 0.9665  SER_batch = 0.0335  BER_batch = 0.0221  loss: 0.089  lr: 0.0100\n",
            "iter:  260 / 500  Acc_batch = 0.9709  SER_batch = 0.0291  BER_batch = 0.0187  loss: 0.089  lr: 0.0100\n",
            "iter:  261 / 500  Acc_batch = 0.9635  SER_batch = 0.0365  BER_batch = 0.0241  loss: 0.098  lr: 0.0100\n",
            "iter:  262 / 500  Acc_batch = 0.9657  SER_batch = 0.0343  BER_batch = 0.0214  loss: 0.097  lr: 0.0100\n",
            "iter:  263 / 500  Acc_batch = 0.9714  SER_batch = 0.0286  BER_batch = 0.0187  loss: 0.084  lr: 0.0100\n",
            "iter:  264 / 500  Acc_batch = 0.9622  SER_batch = 0.0378  BER_batch = 0.0251  loss: 0.098  lr: 0.0100\n",
            "iter:  265 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0231  loss: 0.093  lr: 0.0100\n",
            "iter:  266 / 500  Acc_batch = 0.9611  SER_batch = 0.0389  BER_batch = 0.0265  loss: 0.101  lr: 0.0100\n",
            "iter:  267 / 500  Acc_batch = 0.9608  SER_batch = 0.0392  BER_batch = 0.0263  loss: 0.110  lr: 0.0100\n",
            "iter:  268 / 500  Acc_batch = 0.9577  SER_batch = 0.0423  BER_batch = 0.0284  loss: 0.105  lr: 0.0100\n",
            "iter:  269 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0218  loss: 0.099  lr: 0.0100\n",
            "iter:  270 / 500  Acc_batch = 0.9681  SER_batch = 0.0319  BER_batch = 0.0211  loss: 0.089  lr: 0.0100\n",
            "iter:  271 / 500  Acc_batch = 0.9626  SER_batch = 0.0374  BER_batch = 0.0241  loss: 0.105  lr: 0.0100\n",
            "iter:  272 / 500  Acc_batch = 0.9648  SER_batch = 0.0352  BER_batch = 0.0233  loss: 0.100  lr: 0.0100\n",
            "iter:  273 / 500  Acc_batch = 0.9681  SER_batch = 0.0319  BER_batch = 0.0212  loss: 0.094  lr: 0.0100\n",
            "iter:  274 / 500  Acc_batch = 0.9596  SER_batch = 0.0404  BER_batch = 0.0273  loss: 0.103  lr: 0.0100\n",
            "iter:  275 / 500  Acc_batch = 0.9640  SER_batch = 0.0360  BER_batch = 0.0234  loss: 0.100  lr: 0.0100\n",
            "iter:  276 / 500  Acc_batch = 0.9598  SER_batch = 0.0402  BER_batch = 0.0271  loss: 0.103  lr: 0.0100\n",
            "iter:  277 / 500  Acc_batch = 0.9640  SER_batch = 0.0360  BER_batch = 0.0248  loss: 0.101  lr: 0.0100\n",
            "iter:  278 / 500  Acc_batch = 0.9658  SER_batch = 0.0342  BER_batch = 0.0224  loss: 0.097  lr: 0.0100\n",
            "iter:  279 / 500  Acc_batch = 0.9622  SER_batch = 0.0378  BER_batch = 0.0253  loss: 0.104  lr: 0.0100\n",
            "iter:  280 / 500  Acc_batch = 0.9653  SER_batch = 0.0347  BER_batch = 0.0235  loss: 0.094  lr: 0.0100\n",
            "iter:  281 / 500  Acc_batch = 0.9598  SER_batch = 0.0402  BER_batch = 0.0260  loss: 0.109  lr: 0.0100\n",
            "iter:  282 / 500  Acc_batch = 0.9686  SER_batch = 0.0314  BER_batch = 0.0212  loss: 0.090  lr: 0.0100\n",
            "iter:  283 / 500  Acc_batch = 0.9723  SER_batch = 0.0277  BER_batch = 0.0195  loss: 0.082  lr: 0.0100\n",
            "iter:  284 / 500  Acc_batch = 0.9622  SER_batch = 0.0378  BER_batch = 0.0253  loss: 0.101  lr: 0.0100\n",
            "iter:  285 / 500  Acc_batch = 0.9572  SER_batch = 0.0428  BER_batch = 0.0283  loss: 0.112  lr: 0.0100\n",
            "iter:  286 / 500  Acc_batch = 0.9601  SER_batch = 0.0399  BER_batch = 0.0273  loss: 0.106  lr: 0.0100\n",
            "iter:  287 / 500  Acc_batch = 0.9727  SER_batch = 0.0273  BER_batch = 0.0181  loss: 0.084  lr: 0.0100\n",
            "iter:  288 / 500  Acc_batch = 0.9678  SER_batch = 0.0322  BER_batch = 0.0221  loss: 0.086  lr: 0.0100\n",
            "iter:  289 / 500  Acc_batch = 0.9616  SER_batch = 0.0384  BER_batch = 0.0256  loss: 0.106  lr: 0.0100\n",
            "iter:  290 / 500  Acc_batch = 0.9642  SER_batch = 0.0358  BER_batch = 0.0241  loss: 0.091  lr: 0.0100\n",
            "iter:  291 / 500  Acc_batch = 0.9616  SER_batch = 0.0384  BER_batch = 0.0261  loss: 0.106  lr: 0.0100\n",
            "iter:  292 / 500  Acc_batch = 0.9684  SER_batch = 0.0316  BER_batch = 0.0210  loss: 0.083  lr: 0.0100\n",
            "iter:  293 / 500  Acc_batch = 0.9603  SER_batch = 0.0397  BER_batch = 0.0264  loss: 0.109  lr: 0.0100\n",
            "iter:  294 / 500  Acc_batch = 0.9606  SER_batch = 0.0394  BER_batch = 0.0261  loss: 0.105  lr: 0.0100\n",
            "iter:  295 / 500  Acc_batch = 0.9645  SER_batch = 0.0355  BER_batch = 0.0229  loss: 0.088  lr: 0.0100\n",
            "iter:  296 / 500  Acc_batch = 0.9653  SER_batch = 0.0347  BER_batch = 0.0243  loss: 0.104  lr: 0.0100\n",
            "iter:  297 / 500  Acc_batch = 0.9644  SER_batch = 0.0356  BER_batch = 0.0237  loss: 0.109  lr: 0.0100\n",
            "iter:  298 / 500  Acc_batch = 0.9684  SER_batch = 0.0316  BER_batch = 0.0208  loss: 0.087  lr: 0.0100\n",
            "iter:  299 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0212  loss: 0.095  lr: 0.0100\n",
            "iter:  300 / 500  Acc_batch = 0.9661  SER_batch = 0.0339  BER_batch = 0.0227  loss: 0.095  lr: 0.0100\n",
            "iter:  301 / 500  Acc_batch = 0.9619  SER_batch = 0.0381  BER_batch = 0.0256  loss: 0.103  lr: 0.0100\n",
            "iter:  302 / 500  Acc_batch = 0.9635  SER_batch = 0.0365  BER_batch = 0.0248  loss: 0.096  lr: 0.0100\n",
            "iter:  303 / 500  Acc_batch = 0.9684  SER_batch = 0.0316  BER_batch = 0.0201  loss: 0.085  lr: 0.0100\n",
            "iter:  304 / 500  Acc_batch = 0.9688  SER_batch = 0.0312  BER_batch = 0.0207  loss: 0.088  lr: 0.0100\n",
            "iter:  305 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0222  loss: 0.093  lr: 0.0100\n",
            "iter:  306 / 500  Acc_batch = 0.9686  SER_batch = 0.0314  BER_batch = 0.0206  loss: 0.088  lr: 0.0100\n",
            "iter:  307 / 500  Acc_batch = 0.9639  SER_batch = 0.0361  BER_batch = 0.0250  loss: 0.090  lr: 0.0100\n",
            "iter:  308 / 500  Acc_batch = 0.9645  SER_batch = 0.0355  BER_batch = 0.0235  loss: 0.090  lr: 0.0100\n",
            "iter:  309 / 500  Acc_batch = 0.9692  SER_batch = 0.0308  BER_batch = 0.0206  loss: 0.081  lr: 0.0100\n",
            "iter:  310 / 500  Acc_batch = 0.9644  SER_batch = 0.0356  BER_batch = 0.0247  loss: 0.100  lr: 0.0100\n",
            "iter:  311 / 500  Acc_batch = 0.9666  SER_batch = 0.0334  BER_batch = 0.0222  loss: 0.086  lr: 0.0100\n",
            "iter:  312 / 500  Acc_batch = 0.9648  SER_batch = 0.0352  BER_batch = 0.0235  loss: 0.097  lr: 0.0100\n",
            "iter:  313 / 500  Acc_batch = 0.9637  SER_batch = 0.0363  BER_batch = 0.0247  loss: 0.092  lr: 0.0100\n",
            "iter:  314 / 500  Acc_batch = 0.9671  SER_batch = 0.0329  BER_batch = 0.0216  loss: 0.091  lr: 0.0100\n",
            "iter:  315 / 500  Acc_batch = 0.9609  SER_batch = 0.0391  BER_batch = 0.0278  loss: 0.105  lr: 0.0100\n",
            "iter:  316 / 500  Acc_batch = 0.9637  SER_batch = 0.0363  BER_batch = 0.0249  loss: 0.092  lr: 0.0100\n",
            "iter:  317 / 500  Acc_batch = 0.9658  SER_batch = 0.0342  BER_batch = 0.0226  loss: 0.095  lr: 0.0100\n",
            "iter:  318 / 500  Acc_batch = 0.9606  SER_batch = 0.0394  BER_batch = 0.0275  loss: 0.105  lr: 0.0100\n",
            "iter:  319 / 500  Acc_batch = 0.9591  SER_batch = 0.0409  BER_batch = 0.0269  loss: 0.109  lr: 0.0100\n",
            "iter:  320 / 500  Acc_batch = 0.9653  SER_batch = 0.0347  BER_batch = 0.0226  loss: 0.095  lr: 0.0100\n",
            "iter:  321 / 500  Acc_batch = 0.9674  SER_batch = 0.0326  BER_batch = 0.0211  loss: 0.089  lr: 0.0100\n",
            "iter:  322 / 500  Acc_batch = 0.9632  SER_batch = 0.0368  BER_batch = 0.0243  loss: 0.102  lr: 0.0100\n",
            "iter:  323 / 500  Acc_batch = 0.9618  SER_batch = 0.0382  BER_batch = 0.0258  loss: 0.101  lr: 0.0100\n",
            "iter:  324 / 500  Acc_batch = 0.9679  SER_batch = 0.0321  BER_batch = 0.0215  loss: 0.081  lr: 0.0100\n",
            "iter:  325 / 500  Acc_batch = 0.9722  SER_batch = 0.0278  BER_batch = 0.0186  loss: 0.082  lr: 0.0100\n",
            "iter:  326 / 500  Acc_batch = 0.9704  SER_batch = 0.0296  BER_batch = 0.0200  loss: 0.083  lr: 0.0100\n",
            "iter:  327 / 500  Acc_batch = 0.9655  SER_batch = 0.0345  BER_batch = 0.0226  loss: 0.097  lr: 0.0100\n",
            "iter:  328 / 500  Acc_batch = 0.9670  SER_batch = 0.0330  BER_batch = 0.0221  loss: 0.090  lr: 0.0100\n",
            "iter:  329 / 500  Acc_batch = 0.9666  SER_batch = 0.0334  BER_batch = 0.0215  loss: 0.096  lr: 0.0100\n",
            "iter:  330 / 500  Acc_batch = 0.9619  SER_batch = 0.0381  BER_batch = 0.0256  loss: 0.099  lr: 0.0100\n",
            "iter:  331 / 500  Acc_batch = 0.9639  SER_batch = 0.0361  BER_batch = 0.0238  loss: 0.101  lr: 0.0100\n",
            "iter:  332 / 500  Acc_batch = 0.9640  SER_batch = 0.0360  BER_batch = 0.0226  loss: 0.093  lr: 0.0100\n",
            "iter:  333 / 500  Acc_batch = 0.9673  SER_batch = 0.0327  BER_batch = 0.0217  loss: 0.088  lr: 0.0100\n",
            "iter:  334 / 500  Acc_batch = 0.9652  SER_batch = 0.0348  BER_batch = 0.0242  loss: 0.095  lr: 0.0100\n",
            "iter:  335 / 500  Acc_batch = 0.9665  SER_batch = 0.0335  BER_batch = 0.0223  loss: 0.096  lr: 0.0100\n",
            "iter:  336 / 500  Acc_batch = 0.9622  SER_batch = 0.0378  BER_batch = 0.0252  loss: 0.103  lr: 0.0100\n",
            "iter:  337 / 500  Acc_batch = 0.9647  SER_batch = 0.0353  BER_batch = 0.0238  loss: 0.097  lr: 0.0100\n",
            "iter:  338 / 500  Acc_batch = 0.9627  SER_batch = 0.0373  BER_batch = 0.0236  loss: 0.103  lr: 0.0100\n",
            "iter:  339 / 500  Acc_batch = 0.9658  SER_batch = 0.0342  BER_batch = 0.0223  loss: 0.095  lr: 0.0100\n",
            "iter:  340 / 500  Acc_batch = 0.9661  SER_batch = 0.0339  BER_batch = 0.0227  loss: 0.093  lr: 0.0100\n",
            "iter:  341 / 500  Acc_batch = 0.9613  SER_batch = 0.0387  BER_batch = 0.0253  loss: 0.110  lr: 0.0100\n",
            "iter:  342 / 500  Acc_batch = 0.9598  SER_batch = 0.0402  BER_batch = 0.0275  loss: 0.099  lr: 0.0100\n",
            "iter:  343 / 500  Acc_batch = 0.9678  SER_batch = 0.0322  BER_batch = 0.0224  loss: 0.090  lr: 0.0100\n",
            "iter:  344 / 500  Acc_batch = 0.9702  SER_batch = 0.0298  BER_batch = 0.0202  loss: 0.077  lr: 0.0100\n",
            "iter:  345 / 500  Acc_batch = 0.9598  SER_batch = 0.0402  BER_batch = 0.0262  loss: 0.096  lr: 0.0100\n",
            "iter:  346 / 500  Acc_batch = 0.9657  SER_batch = 0.0343  BER_batch = 0.0229  loss: 0.091  lr: 0.0100\n",
            "iter:  347 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0229  loss: 0.085  lr: 0.0100\n",
            "iter:  348 / 500  Acc_batch = 0.9676  SER_batch = 0.0324  BER_batch = 0.0216  loss: 0.093  lr: 0.0100\n",
            "iter:  349 / 500  Acc_batch = 0.9639  SER_batch = 0.0361  BER_batch = 0.0235  loss: 0.098  lr: 0.0100\n",
            "iter:  350 / 500  Acc_batch = 0.9645  SER_batch = 0.0355  BER_batch = 0.0225  loss: 0.101  lr: 0.0100\n",
            "iter:  351 / 500  Acc_batch = 0.9701  SER_batch = 0.0299  BER_batch = 0.0190  loss: 0.084  lr: 0.0100\n",
            "iter:  352 / 500  Acc_batch = 0.9671  SER_batch = 0.0329  BER_batch = 0.0223  loss: 0.095  lr: 0.0100\n",
            "iter:  353 / 500  Acc_batch = 0.9653  SER_batch = 0.0347  BER_batch = 0.0229  loss: 0.091  lr: 0.0100\n",
            "iter:  354 / 500  Acc_batch = 0.9639  SER_batch = 0.0361  BER_batch = 0.0247  loss: 0.088  lr: 0.0100\n",
            "iter:  355 / 500  Acc_batch = 0.9668  SER_batch = 0.0332  BER_batch = 0.0222  loss: 0.095  lr: 0.0100\n",
            "iter:  356 / 500  Acc_batch = 0.9658  SER_batch = 0.0342  BER_batch = 0.0225  loss: 0.097  lr: 0.0100\n",
            "iter:  357 / 500  Acc_batch = 0.9634  SER_batch = 0.0366  BER_batch = 0.0239  loss: 0.096  lr: 0.0100\n",
            "iter:  358 / 500  Acc_batch = 0.9678  SER_batch = 0.0322  BER_batch = 0.0212  loss: 0.088  lr: 0.0100\n",
            "iter:  359 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0229  loss: 0.092  lr: 0.0100\n",
            "iter:  360 / 500  Acc_batch = 0.9652  SER_batch = 0.0348  BER_batch = 0.0222  loss: 0.093  lr: 0.0100\n",
            "iter:  361 / 500  Acc_batch = 0.9668  SER_batch = 0.0332  BER_batch = 0.0221  loss: 0.087  lr: 0.0100\n",
            "iter:  362 / 500  Acc_batch = 0.9678  SER_batch = 0.0322  BER_batch = 0.0218  loss: 0.084  lr: 0.0100\n",
            "iter:  363 / 500  Acc_batch = 0.9658  SER_batch = 0.0342  BER_batch = 0.0217  loss: 0.090  lr: 0.0100\n",
            "iter:  364 / 500  Acc_batch = 0.9676  SER_batch = 0.0324  BER_batch = 0.0221  loss: 0.097  lr: 0.0100\n",
            "iter:  365 / 500  Acc_batch = 0.9661  SER_batch = 0.0339  BER_batch = 0.0225  loss: 0.088  lr: 0.0100\n",
            "iter:  366 / 500  Acc_batch = 0.9658  SER_batch = 0.0342  BER_batch = 0.0229  loss: 0.097  lr: 0.0100\n",
            "iter:  367 / 500  Acc_batch = 0.9622  SER_batch = 0.0378  BER_batch = 0.0256  loss: 0.100  lr: 0.0100\n",
            "iter:  368 / 500  Acc_batch = 0.9688  SER_batch = 0.0312  BER_batch = 0.0212  loss: 0.083  lr: 0.0100\n",
            "iter:  369 / 500  Acc_batch = 0.9705  SER_batch = 0.0295  BER_batch = 0.0187  loss: 0.086  lr: 0.0100\n",
            "iter:  370 / 500  Acc_batch = 0.9694  SER_batch = 0.0306  BER_batch = 0.0206  loss: 0.087  lr: 0.0100\n",
            "iter:  371 / 500  Acc_batch = 0.9746  SER_batch = 0.0254  BER_batch = 0.0169  loss: 0.072  lr: 0.0100\n",
            "iter:  372 / 500  Acc_batch = 0.9603  SER_batch = 0.0397  BER_batch = 0.0247  loss: 0.108  lr: 0.0100\n",
            "iter:  373 / 500  Acc_batch = 0.9686  SER_batch = 0.0314  BER_batch = 0.0206  loss: 0.091  lr: 0.0100\n",
            "iter:  374 / 500  Acc_batch = 0.9694  SER_batch = 0.0306  BER_batch = 0.0203  loss: 0.082  lr: 0.0100\n",
            "iter:  375 / 500  Acc_batch = 0.9632  SER_batch = 0.0368  BER_batch = 0.0245  loss: 0.092  lr: 0.0100\n",
            "iter:  376 / 500  Acc_batch = 0.9645  SER_batch = 0.0355  BER_batch = 0.0234  loss: 0.089  lr: 0.0100\n",
            "iter:  377 / 500  Acc_batch = 0.9663  SER_batch = 0.0337  BER_batch = 0.0227  loss: 0.089  lr: 0.0100\n",
            "iter:  378 / 500  Acc_batch = 0.9674  SER_batch = 0.0326  BER_batch = 0.0224  loss: 0.091  lr: 0.0100\n",
            "iter:  379 / 500  Acc_batch = 0.9738  SER_batch = 0.0262  BER_batch = 0.0173  loss: 0.076  lr: 0.0100\n",
            "iter:  380 / 500  Acc_batch = 0.9663  SER_batch = 0.0337  BER_batch = 0.0218  loss: 0.090  lr: 0.0100\n",
            "iter:  381 / 500  Acc_batch = 0.9647  SER_batch = 0.0353  BER_batch = 0.0239  loss: 0.089  lr: 0.0100\n",
            "iter:  382 / 500  Acc_batch = 0.9648  SER_batch = 0.0352  BER_batch = 0.0236  loss: 0.095  lr: 0.0100\n",
            "iter:  383 / 500  Acc_batch = 0.9645  SER_batch = 0.0355  BER_batch = 0.0237  loss: 0.095  lr: 0.0100\n",
            "iter:  384 / 500  Acc_batch = 0.9701  SER_batch = 0.0299  BER_batch = 0.0200  loss: 0.091  lr: 0.0100\n",
            "iter:  385 / 500  Acc_batch = 0.9697  SER_batch = 0.0303  BER_batch = 0.0190  loss: 0.086  lr: 0.0100\n",
            "iter:  386 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0224  loss: 0.083  lr: 0.0100\n",
            "iter:  387 / 500  Acc_batch = 0.9665  SER_batch = 0.0335  BER_batch = 0.0231  loss: 0.085  lr: 0.0100\n",
            "iter:  388 / 500  Acc_batch = 0.9694  SER_batch = 0.0306  BER_batch = 0.0208  loss: 0.085  lr: 0.0100\n",
            "iter:  389 / 500  Acc_batch = 0.9697  SER_batch = 0.0303  BER_batch = 0.0203  loss: 0.085  lr: 0.0100\n",
            "iter:  390 / 500  Acc_batch = 0.9665  SER_batch = 0.0335  BER_batch = 0.0228  loss: 0.089  lr: 0.0100\n",
            "iter:  391 / 500  Acc_batch = 0.9714  SER_batch = 0.0286  BER_batch = 0.0194  loss: 0.084  lr: 0.0100\n",
            "iter:  392 / 500  Acc_batch = 0.9705  SER_batch = 0.0295  BER_batch = 0.0199  loss: 0.088  lr: 0.0100\n",
            "iter:  393 / 500  Acc_batch = 0.9702  SER_batch = 0.0298  BER_batch = 0.0203  loss: 0.081  lr: 0.0100\n",
            "iter:  394 / 500  Acc_batch = 0.9618  SER_batch = 0.0382  BER_batch = 0.0254  loss: 0.101  lr: 0.0100\n",
            "iter:  395 / 500  Acc_batch = 0.9645  SER_batch = 0.0355  BER_batch = 0.0233  loss: 0.094  lr: 0.0100\n",
            "iter:  396 / 500  Acc_batch = 0.9728  SER_batch = 0.0272  BER_batch = 0.0180  loss: 0.077  lr: 0.0100\n",
            "iter:  397 / 500  Acc_batch = 0.9674  SER_batch = 0.0326  BER_batch = 0.0214  loss: 0.097  lr: 0.0100\n",
            "iter:  398 / 500  Acc_batch = 0.9679  SER_batch = 0.0321  BER_batch = 0.0224  loss: 0.087  lr: 0.0100\n",
            "iter:  399 / 500  Acc_batch = 0.9670  SER_batch = 0.0330  BER_batch = 0.0215  loss: 0.094  lr: 0.0100\n",
            "iter:  400 / 500  Acc_batch = 0.9702  SER_batch = 0.0298  BER_batch = 0.0204  loss: 0.085  lr: 0.0100\n",
            "iter:  401 / 500  Acc_batch = 0.9637  SER_batch = 0.0363  BER_batch = 0.0233  loss: 0.097  lr: 0.0100\n",
            "iter:  402 / 500  Acc_batch = 0.9629  SER_batch = 0.0371  BER_batch = 0.0251  loss: 0.102  lr: 0.0100\n",
            "iter:  403 / 500  Acc_batch = 0.9691  SER_batch = 0.0309  BER_batch = 0.0207  loss: 0.091  lr: 0.0100\n",
            "iter:  404 / 500  Acc_batch = 0.9701  SER_batch = 0.0299  BER_batch = 0.0198  loss: 0.086  lr: 0.0100\n",
            "iter:  405 / 500  Acc_batch = 0.9699  SER_batch = 0.0301  BER_batch = 0.0203  loss: 0.084  lr: 0.0100\n",
            "iter:  406 / 500  Acc_batch = 0.9666  SER_batch = 0.0334  BER_batch = 0.0221  loss: 0.098  lr: 0.0100\n",
            "iter:  407 / 500  Acc_batch = 0.9679  SER_batch = 0.0321  BER_batch = 0.0212  loss: 0.086  lr: 0.0100\n",
            "iter:  408 / 500  Acc_batch = 0.9674  SER_batch = 0.0326  BER_batch = 0.0219  loss: 0.095  lr: 0.0100\n",
            "iter:  409 / 500  Acc_batch = 0.9686  SER_batch = 0.0314  BER_batch = 0.0212  loss: 0.079  lr: 0.0100\n",
            "iter:  410 / 500  Acc_batch = 0.9661  SER_batch = 0.0339  BER_batch = 0.0226  loss: 0.101  lr: 0.0100\n",
            "iter:  411 / 500  Acc_batch = 0.9681  SER_batch = 0.0319  BER_batch = 0.0214  loss: 0.087  lr: 0.0100\n",
            "iter:  412 / 500  Acc_batch = 0.9650  SER_batch = 0.0350  BER_batch = 0.0234  loss: 0.092  lr: 0.0100\n",
            "iter:  413 / 500  Acc_batch = 0.9666  SER_batch = 0.0334  BER_batch = 0.0221  loss: 0.088  lr: 0.0100\n",
            "iter:  414 / 500  Acc_batch = 0.9655  SER_batch = 0.0345  BER_batch = 0.0232  loss: 0.090  lr: 0.0100\n",
            "iter:  415 / 500  Acc_batch = 0.9668  SER_batch = 0.0332  BER_batch = 0.0218  loss: 0.088  lr: 0.0100\n",
            "iter:  416 / 500  Acc_batch = 0.9710  SER_batch = 0.0290  BER_batch = 0.0198  loss: 0.083  lr: 0.0100\n",
            "iter:  417 / 500  Acc_batch = 0.9688  SER_batch = 0.0312  BER_batch = 0.0210  loss: 0.085  lr: 0.0100\n",
            "iter:  418 / 500  Acc_batch = 0.9655  SER_batch = 0.0345  BER_batch = 0.0229  loss: 0.089  lr: 0.0100\n",
            "iter:  419 / 500  Acc_batch = 0.9634  SER_batch = 0.0366  BER_batch = 0.0246  loss: 0.094  lr: 0.0100\n",
            "iter:  420 / 500  Acc_batch = 0.9671  SER_batch = 0.0329  BER_batch = 0.0223  loss: 0.088  lr: 0.0100\n",
            "iter:  421 / 500  Acc_batch = 0.9697  SER_batch = 0.0303  BER_batch = 0.0203  loss: 0.082  lr: 0.0100\n",
            "iter:  422 / 500  Acc_batch = 0.9699  SER_batch = 0.0301  BER_batch = 0.0196  loss: 0.089  lr: 0.0100\n",
            "iter:  423 / 500  Acc_batch = 0.9668  SER_batch = 0.0332  BER_batch = 0.0221  loss: 0.089  lr: 0.0100\n",
            "iter:  424 / 500  Acc_batch = 0.9679  SER_batch = 0.0321  BER_batch = 0.0218  loss: 0.085  lr: 0.0100\n",
            "iter:  425 / 500  Acc_batch = 0.9725  SER_batch = 0.0275  BER_batch = 0.0183  loss: 0.075  lr: 0.0100\n",
            "iter:  426 / 500  Acc_batch = 0.9671  SER_batch = 0.0329  BER_batch = 0.0218  loss: 0.088  lr: 0.0100\n",
            "iter:  427 / 500  Acc_batch = 0.9717  SER_batch = 0.0283  BER_batch = 0.0192  loss: 0.076  lr: 0.0100\n",
            "iter:  428 / 500  Acc_batch = 0.9657  SER_batch = 0.0343  BER_batch = 0.0231  loss: 0.087  lr: 0.0100\n",
            "iter:  429 / 500  Acc_batch = 0.9614  SER_batch = 0.0386  BER_batch = 0.0260  loss: 0.101  lr: 0.0100\n",
            "iter:  430 / 500  Acc_batch = 0.9679  SER_batch = 0.0321  BER_batch = 0.0216  loss: 0.093  lr: 0.0100\n",
            "iter:  431 / 500  Acc_batch = 0.9671  SER_batch = 0.0329  BER_batch = 0.0212  loss: 0.084  lr: 0.0100\n",
            "iter:  432 / 500  Acc_batch = 0.9621  SER_batch = 0.0379  BER_batch = 0.0251  loss: 0.101  lr: 0.0100\n",
            "iter:  433 / 500  Acc_batch = 0.9688  SER_batch = 0.0312  BER_batch = 0.0213  loss: 0.088  lr: 0.0100\n",
            "iter:  434 / 500  Acc_batch = 0.9637  SER_batch = 0.0363  BER_batch = 0.0248  loss: 0.096  lr: 0.0100\n",
            "iter:  435 / 500  Acc_batch = 0.9689  SER_batch = 0.0311  BER_batch = 0.0217  loss: 0.085  lr: 0.0100\n",
            "iter:  436 / 500  Acc_batch = 0.9674  SER_batch = 0.0326  BER_batch = 0.0214  loss: 0.086  lr: 0.0100\n",
            "iter:  437 / 500  Acc_batch = 0.9691  SER_batch = 0.0309  BER_batch = 0.0211  loss: 0.086  lr: 0.0100\n",
            "iter:  438 / 500  Acc_batch = 0.9704  SER_batch = 0.0296  BER_batch = 0.0201  loss: 0.082  lr: 0.0100\n",
            "iter:  439 / 500  Acc_batch = 0.9691  SER_batch = 0.0309  BER_batch = 0.0208  loss: 0.079  lr: 0.0100\n",
            "iter:  440 / 500  Acc_batch = 0.9650  SER_batch = 0.0350  BER_batch = 0.0226  loss: 0.098  lr: 0.0100\n",
            "iter:  441 / 500  Acc_batch = 0.9681  SER_batch = 0.0319  BER_batch = 0.0220  loss: 0.084  lr: 0.0100\n",
            "iter:  442 / 500  Acc_batch = 0.9709  SER_batch = 0.0291  BER_batch = 0.0190  loss: 0.081  lr: 0.0100\n",
            "iter:  443 / 500  Acc_batch = 0.9689  SER_batch = 0.0311  BER_batch = 0.0203  loss: 0.084  lr: 0.0100\n",
            "iter:  444 / 500  Acc_batch = 0.9647  SER_batch = 0.0353  BER_batch = 0.0247  loss: 0.089  lr: 0.0100\n",
            "iter:  445 / 500  Acc_batch = 0.9635  SER_batch = 0.0365  BER_batch = 0.0234  loss: 0.094  lr: 0.0100\n",
            "iter:  446 / 500  Acc_batch = 0.9681  SER_batch = 0.0319  BER_batch = 0.0217  loss: 0.082  lr: 0.0100\n",
            "iter:  447 / 500  Acc_batch = 0.9661  SER_batch = 0.0339  BER_batch = 0.0224  loss: 0.087  lr: 0.0100\n",
            "iter:  448 / 500  Acc_batch = 0.9718  SER_batch = 0.0282  BER_batch = 0.0183  loss: 0.082  lr: 0.0100\n",
            "iter:  449 / 500  Acc_batch = 0.9722  SER_batch = 0.0278  BER_batch = 0.0188  loss: 0.076  lr: 0.0100\n",
            "iter:  450 / 500  Acc_batch = 0.9642  SER_batch = 0.0358  BER_batch = 0.0237  loss: 0.097  lr: 0.0100\n",
            "iter:  451 / 500  Acc_batch = 0.9720  SER_batch = 0.0280  BER_batch = 0.0186  loss: 0.080  lr: 0.0100\n",
            "iter:  452 / 500  Acc_batch = 0.9741  SER_batch = 0.0259  BER_batch = 0.0172  loss: 0.077  lr: 0.0100\n",
            "iter:  453 / 500  Acc_batch = 0.9699  SER_batch = 0.0301  BER_batch = 0.0207  loss: 0.085  lr: 0.0100\n",
            "iter:  454 / 500  Acc_batch = 0.9663  SER_batch = 0.0337  BER_batch = 0.0231  loss: 0.093  lr: 0.0100\n",
            "iter:  455 / 500  Acc_batch = 0.9683  SER_batch = 0.0317  BER_batch = 0.0212  loss: 0.086  lr: 0.0100\n",
            "iter:  456 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0229  loss: 0.088  lr: 0.0100\n",
            "iter:  457 / 500  Acc_batch = 0.9705  SER_batch = 0.0295  BER_batch = 0.0195  loss: 0.082  lr: 0.0100\n",
            "iter:  458 / 500  Acc_batch = 0.9718  SER_batch = 0.0282  BER_batch = 0.0188  loss: 0.083  lr: 0.0100\n",
            "iter:  459 / 500  Acc_batch = 0.9718  SER_batch = 0.0282  BER_batch = 0.0180  loss: 0.082  lr: 0.0100\n",
            "iter:  460 / 500  Acc_batch = 0.9663  SER_batch = 0.0337  BER_batch = 0.0230  loss: 0.098  lr: 0.0100\n",
            "iter:  461 / 500  Acc_batch = 0.9681  SER_batch = 0.0319  BER_batch = 0.0204  loss: 0.084  lr: 0.0100\n",
            "iter:  462 / 500  Acc_batch = 0.9707  SER_batch = 0.0293  BER_batch = 0.0201  loss: 0.087  lr: 0.0100\n",
            "iter:  463 / 500  Acc_batch = 0.9655  SER_batch = 0.0345  BER_batch = 0.0234  loss: 0.093  lr: 0.0100\n",
            "iter:  464 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0228  loss: 0.090  lr: 0.0100\n",
            "iter:  465 / 500  Acc_batch = 0.9688  SER_batch = 0.0312  BER_batch = 0.0208  loss: 0.087  lr: 0.0100\n",
            "iter:  466 / 500  Acc_batch = 0.9657  SER_batch = 0.0343  BER_batch = 0.0230  loss: 0.093  lr: 0.0100\n",
            "iter:  467 / 500  Acc_batch = 0.9762  SER_batch = 0.0238  BER_batch = 0.0159  loss: 0.072  lr: 0.0100\n",
            "iter:  468 / 500  Acc_batch = 0.9639  SER_batch = 0.0361  BER_batch = 0.0241  loss: 0.100  lr: 0.0100\n",
            "iter:  469 / 500  Acc_batch = 0.9704  SER_batch = 0.0296  BER_batch = 0.0201  loss: 0.078  lr: 0.0100\n",
            "iter:  470 / 500  Acc_batch = 0.9673  SER_batch = 0.0327  BER_batch = 0.0220  loss: 0.083  lr: 0.0100\n",
            "iter:  471 / 500  Acc_batch = 0.9671  SER_batch = 0.0329  BER_batch = 0.0211  loss: 0.089  lr: 0.0100\n",
            "iter:  472 / 500  Acc_batch = 0.9661  SER_batch = 0.0339  BER_batch = 0.0219  loss: 0.091  lr: 0.0100\n",
            "iter:  473 / 500  Acc_batch = 0.9696  SER_batch = 0.0304  BER_batch = 0.0203  loss: 0.090  lr: 0.0100\n",
            "iter:  474 / 500  Acc_batch = 0.9705  SER_batch = 0.0295  BER_batch = 0.0196  loss: 0.083  lr: 0.0100\n",
            "iter:  475 / 500  Acc_batch = 0.9671  SER_batch = 0.0329  BER_batch = 0.0220  loss: 0.088  lr: 0.0100\n",
            "iter:  476 / 500  Acc_batch = 0.9559  SER_batch = 0.0441  BER_batch = 0.0294  loss: 0.110  lr: 0.0100\n",
            "iter:  477 / 500  Acc_batch = 0.9670  SER_batch = 0.0330  BER_batch = 0.0213  loss: 0.085  lr: 0.0100\n",
            "iter:  478 / 500  Acc_batch = 0.9666  SER_batch = 0.0334  BER_batch = 0.0213  loss: 0.095  lr: 0.0100\n",
            "iter:  479 / 500  Acc_batch = 0.9666  SER_batch = 0.0334  BER_batch = 0.0211  loss: 0.092  lr: 0.0100\n",
            "iter:  480 / 500  Acc_batch = 0.9705  SER_batch = 0.0295  BER_batch = 0.0195  loss: 0.085  lr: 0.0100\n",
            "iter:  481 / 500  Acc_batch = 0.9688  SER_batch = 0.0312  BER_batch = 0.0200  loss: 0.086  lr: 0.0100\n",
            "iter:  482 / 500  Acc_batch = 0.9722  SER_batch = 0.0278  BER_batch = 0.0181  loss: 0.080  lr: 0.0100\n",
            "iter:  483 / 500  Acc_batch = 0.9648  SER_batch = 0.0352  BER_batch = 0.0243  loss: 0.089  lr: 0.0100\n",
            "iter:  484 / 500  Acc_batch = 0.9660  SER_batch = 0.0340  BER_batch = 0.0226  loss: 0.093  lr: 0.0100\n",
            "iter:  485 / 500  Acc_batch = 0.9614  SER_batch = 0.0386  BER_batch = 0.0262  loss: 0.098  lr: 0.0100\n",
            "iter:  486 / 500  Acc_batch = 0.9653  SER_batch = 0.0347  BER_batch = 0.0229  loss: 0.093  lr: 0.0100\n",
            "iter:  487 / 500  Acc_batch = 0.9652  SER_batch = 0.0348  BER_batch = 0.0229  loss: 0.094  lr: 0.0100\n",
            "iter:  488 / 500  Acc_batch = 0.9674  SER_batch = 0.0326  BER_batch = 0.0210  loss: 0.083  lr: 0.0100\n",
            "iter:  489 / 500  Acc_batch = 0.9697  SER_batch = 0.0303  BER_batch = 0.0207  loss: 0.074  lr: 0.0100\n",
            "iter:  490 / 500  Acc_batch = 0.9686  SER_batch = 0.0314  BER_batch = 0.0212  loss: 0.088  lr: 0.0100\n",
            "iter:  491 / 500  Acc_batch = 0.9684  SER_batch = 0.0316  BER_batch = 0.0221  loss: 0.092  lr: 0.0100\n",
            "iter:  492 / 500  Acc_batch = 0.9673  SER_batch = 0.0327  BER_batch = 0.0218  loss: 0.094  lr: 0.0100\n",
            "iter:  493 / 500  Acc_batch = 0.9697  SER_batch = 0.0303  BER_batch = 0.0198  loss: 0.082  lr: 0.0100\n",
            "iter:  494 / 500  Acc_batch = 0.9668  SER_batch = 0.0332  BER_batch = 0.0215  loss: 0.093  lr: 0.0100\n",
            "iter:  495 / 500  Acc_batch = 0.9670  SER_batch = 0.0330  BER_batch = 0.0225  loss: 0.094  lr: 0.0100\n",
            "iter:  496 / 500  Acc_batch = 0.9697  SER_batch = 0.0303  BER_batch = 0.0204  loss: 0.088  lr: 0.0100\n",
            "iter:  497 / 500  Acc_batch = 0.9613  SER_batch = 0.0387  BER_batch = 0.0255  loss: 0.107  lr: 0.0100\n",
            "iter:  498 / 500  Acc_batch = 0.9642  SER_batch = 0.0358  BER_batch = 0.0239  loss: 0.092  lr: 0.0100\n",
            "iter:  499 / 500  Acc_batch = 0.9681  SER_batch = 0.0319  BER_batch = 0.0214  loss: 0.091  lr: 0.0100\n",
            "iter:  500 / 500  Acc_batch = 0.9661  SER_batch = 0.0339  BER_batch = 0.0228  loss: 0.087  lr: 0.0100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wpxsX1p7Ecr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b96c4c5f-6f6e-4a76-f2eb-842fb8e08613"
      },
      "source": [
        "#---Testing\n",
        "# is_fading = True\n",
        "# is_noise  = True\n",
        "EbN = np.arange(0, 15+1)\n",
        "\n",
        "accuracy_all = np.zeros_like(EbN ,dtype=np.float64)\n",
        "ber_all = np.zeros_like(EbN ,dtype=np.float64)\n",
        "test_samples = 10000\n",
        "steps = 10 # total_samples = step * test_samples\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, ebn in enumerate(EbN):\n",
        "\n",
        "        Nerr_per_user_ebn = np.zeros([J])\n",
        "        Nbits = 0 # number of bits processed till now\n",
        "        accuracy_ebn = []\n",
        "        wrong_symbols_ebn = []\n",
        "        ber_ebn = []\n",
        "        wrong_bits_ebn = []\n",
        "\n",
        "        # steps = test_samples // 10000\n",
        "        step = 1\n",
        "\n",
        "        # while(step <= steps):\n",
        "        while( (Nerr_per_user_ebn.min() < maxNumErrs) and (Nbits < maxNumBits) ):\n",
        "        \n",
        "            step = step + 1\n",
        "\n",
        "            snr = ebn + 10*np.log10(np.log2(M)*J/K)\n",
        "\n",
        "            codebook = get_codebook_from_condensed_codebook()\n",
        "\n",
        "            #---Generate symbols and channel parameters\n",
        "            symbols = np.random.randint(1, M+1, (test_samples, J))\n",
        "            symbols_one_hot = torch.Tensor(enc.transform(symbols).toarray().reshape((test_samples, J, M))).to(device) # [test_samples, J, M]\n",
        "\n",
        "            #---Fading\n",
        "            if is_fading is True:\n",
        "                h = (torch.randn(test_samples, J, 2*K)/torch.sqrt(torch.tensor(2.0))).to(device)\n",
        "            else:\n",
        "                h = torch.zeros(test_samples, J, 2*K).to(device)\n",
        "                real_idx = 2 * torch.arange(K).long()\n",
        "                h[:, :, real_idx] = 1.0\n",
        "                \n",
        "            #---Encode\n",
        "            codewords_faded, codewords_faded_without_h = encode(symbols, codebook, h = h)    \n",
        "            received_signal = codewords_faded\n",
        "            \n",
        "            #---Noise\n",
        "            '''noise is being calc using codewords_faded_without_h '''\n",
        "            noise, sigma_square = add_noise(codewords_faded_without_h, J, snr)\n",
        "            \n",
        "            if is_noise is True:\n",
        "                received_signal = received_signal + noise\n",
        "            \n",
        "\n",
        "            #---Decode\n",
        "            beta = torch.log((1/(2* math.pi * sigma_square)**0.5))\n",
        "            gamma = torch.log(torch.tensor((1.0/M), device=device, dtype=torch.float32)) # log(p_x{j})\n",
        "            decoded_symbols_one_hot = mpa.forward(received_signal, codebook, h, sigma_square, beta, gamma)\n",
        "            \n",
        "            \n",
        "            #---Compute metrics\n",
        "            decoded_symbols = torch.argmax(decoded_symbols_one_hot, dim=-1) + 1\n",
        "            accuracy_batch = np.mean(symbols == decoded_symbols.cpu().data.numpy())\n",
        "            wrong_symbols_batch = (symbols != decoded_symbols.cpu().data.numpy()).sum()\n",
        "            ber_batch, Nerr_per_user = compute_ber(symbols.reshape(test_samples*J), decoded_symbols.cpu().data.numpy().reshape(test_samples*J), M)\n",
        "            wrong_bits_batch = Nerr_per_user.sum()\n",
        "\n",
        "            accuracy_ebn.append(accuracy_batch)\n",
        "            wrong_symbols_ebn.append(wrong_symbols_batch)\n",
        "            ber_ebn.append(ber_batch)\n",
        "            wrong_bits_ebn.append(wrong_bits_batch)\n",
        "\n",
        "            Nerr_per_user_ebn += Nerr_per_user\n",
        "            Nbits += test_samples * np.log2(M)\n",
        "\n",
        "        accuracy = np.array(accuracy_ebn).mean()\n",
        "        wrong_symbols = np.array(wrong_symbols_ebn).sum()\n",
        "        ber = np.array(ber_ebn).mean()\n",
        "        wrong_bits = np.array(wrong_bits_ebn).sum()\n",
        "        accuracy_all[i] = accuracy\n",
        "        ber_all[i] = ber\n",
        "\n",
        "        print(\"EbN:\", ebn, ' Acc: {:.8f}'.format(accuracy),\n",
        "            ' SER: {:.8f}'.format(1-accuracy),'BER: {:.8f}'.format(ber),\n",
        "            ' Wrong Symbols: ', wrong_symbols, '/', test_samples*(step-1)*J,\n",
        "            ' Wrong Bits: ', wrong_bits, '/', test_samples*(step-1)*J*np.log2(M))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EbN: 0  Acc: 0.73516667  SER: 0.26483333 BER: 0.17461667  Wrong Symbols:  15890 / 60000  Wrong Bits:  20954 / 120000.0\n",
            "EbN: 1  Acc: 0.77595000  SER: 0.22405000 BER: 0.14719167  Wrong Symbols:  13443 / 60000  Wrong Bits:  17663 / 120000.0\n",
            "EbN: 2  Acc: 0.82293333  SER: 0.17706667 BER: 0.11757500  Wrong Symbols:  10624 / 60000  Wrong Bits:  14109 / 120000.0\n",
            "EbN: 3  Acc: 0.86551667  SER: 0.13448333 BER: 0.08934167  Wrong Symbols:  8069 / 60000  Wrong Bits:  10721 / 120000.0\n",
            "EbN: 4  Acc: 0.90613333  SER: 0.09386667 BER: 0.06224167  Wrong Symbols:  5632 / 60000  Wrong Bits:  7469 / 120000.0\n",
            "EbN: 5  Acc: 0.94210000  SER: 0.05790000 BER: 0.03847500  Wrong Symbols:  3474 / 60000  Wrong Bits:  4617 / 120000.0\n",
            "EbN: 6  Acc: 0.96808333  SER: 0.03191667 BER: 0.02140000  Wrong Symbols:  1915 / 60000  Wrong Bits:  2568 / 120000.0\n",
            "EbN: 7  Acc: 0.98503333  SER: 0.01496667 BER: 0.00989583  Wrong Symbols:  1796 / 120000  Wrong Bits:  2375 / 240000.0\n",
            "EbN: 8  Acc: 0.99355833  SER: 0.00644167 BER: 0.00441042  Wrong Symbols:  1546 / 240000  Wrong Bits:  2117 / 480000.0\n",
            "EbN: 9  Acc: 0.99806310  SER: 0.00193690 BER: 0.00133036  Wrong Symbols:  1627 / 840000  Wrong Bits:  2235 / 1680000.0\n",
            "EbN: 10  Acc: 0.99952608  SER: 0.00047392 BER: 0.00032083  Wrong Symbols:  1763 / 3720000  Wrong Bits:  2387 / 7440000.0\n",
            "EbN: 11  Acc: 0.99991079  SER: 0.00008921 BER: 0.00006013  Wrong Symbols:  2034 / 22800000  Wrong Bits:  2742 / 45600000.0\n",
            "EbN: 12  Acc: 0.99998793  SER: 0.00001207 BER: 0.00000815  Wrong Symbols:  362 / 30000000  Wrong Bits:  489 / 60000000.0\n",
            "EbN: 13  Acc: 0.99999897  SER: 0.00000103 BER: 0.00000072  Wrong Symbols:  31 / 30000000  Wrong Bits:  43 / 60000000.0\n",
            "EbN: 14  Acc: 0.99999990  SER: 0.00000010 BER: 0.00000007  Wrong Symbols:  3 / 30000000  Wrong Bits:  4 / 60000000.0\n",
            "EbN: 15  Acc: 1.00000000  SER: 0.00000000 BER: 0.00000000  Wrong Symbols:  0 / 30000000  Wrong Bits:  0 / 60000000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO1i8WRmw7ey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "1879b8fc-10f0-4b03-cf50-3b7af0cf3ccd"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 11 05:27:14 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0    47W /  70W |   1221MiB / 15079MiB |     55%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcK9bak8wTqB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "e4722931-c2b5-448c-c1c9-a3c84d127ac9"
      },
      "source": [
        "#---Plot Metrics across different channels\n",
        "ymin = 1e-3\n",
        "ymax = 1e-0\n",
        "xmin = EbN[0]\n",
        "xmax = EbN[-1]\n",
        "\n",
        "\n",
        "f1 = plt.figure()\n",
        "f2 = plt.figure()\n",
        "\n",
        "ax1 = f1.add_subplot(111)\n",
        "ax1.set_title('SER vs EbN')\n",
        "ax1.set_yscale('log')\n",
        "ax1.set_ylim([ymin,ymax])\n",
        "# ax1.set_xlim([xmin,xmax])\n",
        "ax1.set_xticks(EbN)\n",
        "ax1.grid(which=\"both\")\n",
        "ax1.plot(EbN, 1-accuracy_all, '-or')\n",
        "\n",
        "ax2 = f2.add_subplot(111)\n",
        "ax2.set_title('BER vs EbN')\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_ylim([ymin,ymax])\n",
        "# ax1.set_xlim([xmin,xmax])\n",
        "ax2.set_xticks(EbN)\n",
        "ax2.grid(which=\"both\")\n",
        "ax2.plot(EbN, ber_all, '-or')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5b3H8c/PBNC4ASqgbBEXXlKtVrpXK6BVpHWr2EJTlxZFbb1tXS5qcUG9XFtRalst1iLqLQji1talorjVl6170aJWFAVcSd21uCD53T+ewYaQhCRnnjkn53zfr9e8mDkz853fkOSXyXOe8xxzd0REJP/WS7sAERGpDDV8EZGCUMMXESkINXwRkYJQwxcRKQg1fBGRglDDF0mZmdWamZtZddq1SL6p4UunY2ZLzOx9M3vPzN40s5vNrH+j+68ws49K96++PFa6b3VzXb19iZmdUoGaW6ypDc+dVKr5W422VZe21caqWfJHDV86q/3cfSNgS2A58Osm95/n7hs1uuzc5P7upeePBk43s69VoOZ11dSaN4CzzKwqVnGSf2r40qm5+wfAtcCQDj7/YeAJYJfm7jezaWZ2fpNtfzSzE0rXTzazl8zsXTN72sz27EgdJd83s5fN7BUzO6nJfbcCHwHfTZAvBaeGL52amdUA3wbu7+DzvwjsCDzbwkNmA982Mys9vgewNzDHzAYDxwGfc/eNgX2AJR2po2Q4sF0p/2Qz26vRfQ6cDpxpZl0SvIYUmBq+dFZ/MLO3gLeBrwFTmtx/kpm91ehyZZP7XzOz94G/Ab8B/tDC69xLaLa7l26PBv7m7i8Dq4BuwBAz6+LuS9x9cSs1r6ums9z93+7+D+ByYGzjO939T8C/gCNbeQ2RFqnhS2d1oLt3B9YnfMq+x8z6NLr/fHfv3uhyeJPnbw5sBJwIDAOa/dTsYXXBOfyn+X4HmFW671ngJ8AkoN7M5pjZVq3UvK6aXmh0fSnQXNZpwMTS+xZpFzV86dTcfZW7X0/4tL1bB547FfgA+EErD50NjDazgcAXgOsaZVzl7rsBAwl/Cfy8nW+hsf6Nrg8AXm6m5tsJw0+t1SvSLDV86dQsOADoATzVwZifARPMrNlPze7+d+A1YDowz93fKr32YDMbYWbdCL803gcaOlgDhNlCNWb2KeB7wNUtPG4iMCHB60hBqeFLZ3Wjmb0HvANMBg539yca3T+hyZz311rJuhl4EziqlcdcBexV+ne1boRfFq8BrwK9gFNbyVhXTfcQPr3fQRj+ua25EHe/D3iwldcRaZbpBCgiIsWgT/giIgVRsbU7zGxDwvS3j4C73X1WpV5bREQSfsI3sxlmVm9mC5tsH1k66vDZRuuUfBO41t2PAvZP8roiItJ+SYd0rgBGNt5QWuvjYmBfwuHuY81sCNCP/8wzXpXwdUVEpJ0SDem4+1+aWa3v88Cz7v4cgJnNAQ4AXiQ0/QW08ovGzMYD4wE22GCDof3792/poa1qaGhgvfXKt4ui3HkxMouWFyMz63kxMrOeFyMz73mLFi16zd23WOsOd090AWqBhY1ujwamN7p9KHARsCHhcPFpQF1bsocOHeodddddd3X4uZXIi5FZtLwYmVnPi5GZ9bwYmXnPAx72ZnpqxXbauvu/CQeTiIhICmJMy3yJNQ8R71faJiIiKUp84FVpDP8md9+xdLsaWATsSWj0DwHf8TWPglxX5n7Afn379j1q5syZHarrvffeY6ONNurQcyuRFyOzaHkxMrOeFyMz63kxMvOeN3z48Efc/bNr3dHcOE9bL4RFpV4BVhJ2yo4rbR9FaPqLgYkdzdcYvvIqnZn1vBiZWc+LkZn3PGKM4bv72Ba23wLckiRbRETKK5Nr6WhIR3lpZWY9L0Zm1vNiZOY9L8qQTuyLhnSUV+nMrOfFyMx6XozMvOfRwpCOFk8TESkINXwRkYJQwxcRKQjttE0pL0Zm0fJiZGY9L0Zm1vNiZOY9TzttE9KOqOzlxcjMel6MzKznxcjMex7aaSsiUmxq+CIiBaEx/JTyYmQWLS9GZtbzYmRmPS9GZt7zNIafkMYls5cXIzPreTEys54XIzPveWgMX0Sk2NTwRUQKQg1fRKQg1PBFRApCs3RSyouRWbS8GJlZz4uRmfW8GJl5z9MsnYQ08yB7eTEys54XIzPreTEy856HZumIiBSbGr6ISEGo4YuIFIQavohIQajhi4gUhKZlppQXI7NoeTEys54XIzPreTEy856naZkJaapZ9vJiZGY9L0Zm1vNiZOY9D03LFBEpNjV8EZGCUMMXESkINXwRkYJQwxcRKQg1fBGRglDDFxEpCB14lVJejMyi5cXIzHpejMys58XIzHueDrxKSAeTZC8vRmbW82JkZj0vRmbe89CBVyIixaaGLyJSEGr4IiIFoYYvIlIQavgiIgWhhi8iUhBq+CIiBaGGLyJSEGr4IiIFoYYvIlIQavgiIgWhxdNSyouRWbS8GJlZz4uRmfW8GJl5zyvO4mkzZ7oPHOgNZu4DB4bbZaAFobKXFyMz63kxMrOeFyMz73m0sHhadYJfQtkzaxaMHw8rVmAAS5eG2wB1dWlWJiKSunyN4U+cCCtWrLltxYqwXUSk4PLV8Jcta3770qVw443wzjuVrUdEJEPy1fAHDGj5vv33h5494ctfhjPOgL/8BT76qHK1iYikLF8Nf/JkqKlZc1tNDVx+Odx5J5x8MqxaFR63xx7QoweMGgVTp8Ljj0NDQzp1i4hUQL4afl0dXHopDByIm8HAgeH2EUfA8OGh0T/wALz+OtxwA3zve/Dcc3DiibDzztCnD4wdC5ddFoaBIOwIrq1ljxEjoLY23BYR6YTyNUsHQtOvq+Oeu+9m2LBhzT+me3c48MBwAXjhBbjjDpg/P/w7Z07Y3qsXvPEGfPyxZv2ISKeXr0/4HdW/f/grYOZMePllWLgQLrwQ3n0XPv54zceuWAGnnppKmSIiSajhN2UGn/oU/PjH8MEHzT/mhRdg9Gi47rqWHyMikjFq+K1padbPxhvDvfeGpt+rV/jrYN68tf8aEBHJEDX81rQ062faNHjpJbjtNjj44LADeORI2Gor+OEP4b77NONHRDJHDb81Lc36qauD6mr42tfClM/ly+H662HYMJgxA3bbDbbeOkwDXbAAMrhAnYgUjxr+utTVwZIl3HPnnbBkSfOzc9ZfHw46CObOhfp6+L//C/sBLrgAPvOZcP2cc+DZZ8PjNdVTRFKghl9uG28Mhx4Kt9wCr74ahn823zwc3bvddjBoUJj/v3Qp5v6fqZ5q+iISmRp+TJtvDsccE5ZxWLYMpkyBF1+ElSvXfJwWeBORClDDr5T+/eGkk1qeybN0aZj3LyISiRp+pbW2wFvfvmH+/6JFlatHRAqjYg3fzAaZ2WVmdm2lXjOTWprqOWlSWNFz2jQYPBj23TfsB9D0ThEpkzY1fDObYWb1ZrawyfaRZva0mT1rZqe0luHuz7n7uCTF5kJLUz3PPDMs7bBsGZx1Fjz2GHz966H5X3ghvP122pWLSCfX1k/4VwAjG28wsyrgYmBfYAgw1syGmNlOZnZTk0uvslbd2bU21bNPnzCjZ8kSmD07HMl7/PFhuOcHP4Ann0yrahHp5MzbeFCQmdUCN7n7jqXbXwImufs+pdunArj7uevIudbdR7dy/3hgPEDv3r2Hzlm9cmU7Ze0s8kkyN1q0iL433EDvO+5gvZUreXPXXXnxoIN4/Utfgqoqes2fz6Dp0+lWX8+HvXrx3JFHUr/XXhWrL628GJlZz4uRmfW8GJl5zxs+fPgj7v7Zte5o7szmzV2AWmBho9ujgemNbh8KXNTK8zcDLgEWA6e25TWHDh3a4bO2Z+0s8mXJrK93/9//de/Xzx3ca2vdx4xx32CDcHv1pabGfebMytdX4bwYmVnPi5GZ9bwYmXnPAx72ZnpqxXbauvvr7n6Mu2/j6/grQFqwxRZhaebnn4drrw0zfubMgfffX/NxmtcvIs2o+JBOG19rP2C/vn37HjVz5swOZWTtT6xYmXsMHx5OztKEm4V9BAkU5f+wM+XFyMx6XozMvOfFGNKpBp4Dtga6Ao8Bn2prXlsuGtJpg4ED1xzOWX3p3t39/ffTry9iXozMrOfFyMx6XozMvOeRZEjHzGYDfwMGm9mLZjbO3T8GjgPmAU8Bc939iQ7/SpKOaW5ef1UVvPUW7LADXH21VusUEaCN0zLdfay7b+nuXdy9n7tfVtp+i7tv72FcfnLcUqVZzc3rv/LKcH7eTTeFMWPgK1+Bv/0t7UpFJGVtHsOvJI3hlylv1Sr6zJvH1pddRrc33qB++HCeGz+eD/r0yUZ9GczMel6MzKznxcjMe17iMfw0LhrDL1Peu++6n3FGmL7ZrZv7hAnub72VnfoylJn1vBiZWc+LkZn3PNKelikp2mijsFzDokVhiOe882DbbeE3v9F5eEUKRA2/SPr1gyuugEcegR13DOff/fSn4eabtWNXpAA0hp9SXozMduW5s9l997HNb39LzYsv8sbQoSw+5hj+ve222agvpcys58XIzHpejMy852kMP6Hcjkt++KH7L3/p3rOnu5n797/vftFF7gMHeoNZmOdfhmUaOlxfhTOznhcjM+t5MTLznofG8KVZXbvCj34UTrB+/PFhyOe443TOXZEcUsOXoEcPuOCCsDxzU1qbRyQX1PBlTa+80vz2ZcsqW4eIlJ122qaUFyOzHHlfHDOG9ZcvX2v7qq5duX/uXFZuummHs4vyfxgzL0Zm1vNiZOY9TzttEyrMjqiZM8N6+o0XYuvSxb2qKqzDf++96dYXOTPreTEys54XIzPveWinrbRJc2vzXH45PPggdOsGw4bBz36mk6uLdEJq+LK25s65u+uu8OijMHp0OAnLqFFQX592pSLSDmr40nabbBJOrH7JJXD33bDLLnDPPWlXJSJtpIYv7WMGRx8NDzwAG28MI0bAOefAqlVpVyYi66BZOinlxcisdF7VihVs/4tf0Hv+fN7cdVeenDiRlT17Vqy+GJlZz4uRmfW8GJl5z9MsnYQ086AFDQ3u06e7r7++e+/e7nfckSyvnXLxf5hyZtbzYmTmPQ/N0pEozGDcOHjooXC07l57wZlnaohHJIPU8KU8dtwRHn4YDjsMzj47NP6WjtoVkVSo4Uv5bLhhWHxt9bz9XXaB229PuyoRKVHDl/I74ogwxLPFFrDPPnDggTBwIHuMGAG1tVp5UyQl1WkXIDk1ZEj4lD9qFPzxjwAY/Ge5ZQgHdIlIxWhaZkp5MTKzmNfSYmwf9O7N/XPmJMqGbL7nmHkxMrOeFyMz73malpmQppp1kNmaC7Gtvpglz/aMvueIeTEys54XIzPveWhapqRiwIDmt7dygJaIxKGGL3FNngw1NWtuW289eP11OO+88HlfRCpCDV/iam655RkzYMwYOPlkOOkkLbUsUiGapSPx1dVBXR333H03w4YNC9sOPTRM25w6NSyzPGMGdOmSapkieaeGL+lYbz345S+hd2847bQwxHPNNeHgLRGJQkM6kh4zmDgxDPnMmwd77hkav4hEoYYv6TvqKLj2WliwAHbfHV54Ie2KRHJJB16llBcjs7PnbbpgATuddhof19Tw+HnnsaK2NnM1pp0XIzPreTEy856nA68S0sEkFcpbsMC9Tx/3Hj3c//rX8mS2Q9bzYmRmPS9GZt7z0IFX0insvDPcdx9stlkY07/llrQrEskNNXzJnkGDQtPfYQfYf3/4/e/TrkgkF9TwJZt69YK77oI99ggnVbnggrQrEun01PAluzbZJAzpHHJIOCJ3wgQtxSCSgA68kmzr1g1mzw6f+KdMCUfl/u53OipXpAP0CV+yr6oKfv1rOOssuPJKOOigcBrF2lqdRUukHfQJXzoHMzjjjLAUwzHHwJ//DA0NOouWSDvoE750LkcfDZtvvvYKmytWhGUaRKRFavjS+bS03s6yZZWtQ6STUcOXzqels2i1tF1EADV86YyaO4tWdXXYLiIt0uJpKeXFyCxSXq/58xk0fTrd6utZtf76VL//Pv88+WReHTkyMzXGyIuRmfW8GJl5z9PiaQlpQajs5X2S+dFH7nvt5d6li/u99ybPKyN932QzM+95aPE0ya0uXWDuXNh66zBH//nn065IJJPU8CUfevSAm26CVatgv/3gnXfSrkgkc9TwJT+22y6cOevpp2Hs2ND8ReQTaviSLyNGwEUXhUXXJkxIuxqRTNHSCpI/Rx8NTzwBU6fCkCEwblzaFYlkgj7hSz5NnQp77w3HHgv33JN2NSKZoIYv+VRdDVdfDdtsAwcfDIsXp12RSOrU8CW/uneHG28MJ03Zbz94++20KxJJlRq+5Nu228J118Ezz8C3vw0ff5x2RSKpUcOX/Bs2DKZNg3nzwqkSRQpKs3SkGI48Ep58En7xC9hhhzCTR6Rg9AlfimPKFBg1Co47Du68M+1qRCpODV+Ko6oqnBB9++1h9Ogwri9SIGr4UiybbBJm7lRVwTe+AW++mXZFIhWjhi/FM2gQXH99WFXzW9+ClSvTrkikItTwpZh23x0uvRTmz4fjj0+7GpGKqNgsHTM7EPg6sAlwmbvfVqnXFmnWEUeEmTtTpoSZOz/8YdoViUTVpk/4ZjbDzOrNbGGT7SPN7Gkze9bMTmktw93/4O5HAccA3+54ySJldO654Sjc//ov6N2bPUaMgNpamDUr7cpEyq6tQzpXAGucLNTMqoCLgX2BIcBYMxtiZjuZ2U1NLr0aPfW00vNE0ldVBQccEK7X12PusHQpjB+vpi+50+aTmJtZLXCTu+9Yuv0lYJK771O6fSqAu5/bwvMN+Blwu7vPb+V1xgPjAXr37j10zpw5bX0va8jaSYUrkVm0vHJlfnHMGNZfvnyt7R/07s39Hfz+Wy2r77kz5cXIzHte4pOYA7XAwka3RwPTG90+FLiolef/CHgEuAQ4pi2vqZOYK68imWbuYYm1NS9m2agvcmbW82Jk5j2PFk5iXrGdtu7+K+BXlXo9kTYbMCAM4zS3XSRHkkzLfAno3+h2v9I2kc5l8mSoqVl7+ze/WflaRCJKMoZfDSwC9iQ0+oeA77j7E4mLMtsP2K9v375HzZw5s0MZWRtTq0Rm0fLKmdlr/nwGTZ9Ot/p6PtxiCxqqquj69ts8Om0aKxJ80s/ye+4seTEy856XaAwfmA28AqwEXgTGlbaPIjT9xcDEtmS156IxfOVVOvOTvGXL3Hv1ch882P2tt5LnlVHWvy56z+nnkWQM393HtrD9FuCW9v/+Ecm4/v3hmmtgzz3hsMPghhtgPR2YLp1bm4d0KklDOspLK7NpXt/rr2e7X/+a5484gqWHH556fTEys54XIzPveYmnZaZx0ZCO8iqduVZeQ4P7YYeFaZo33pg8rwyy/nXRe04/jxaGdPQ3qkhrzOCSS2DoUKirg6efTrsikQ5TwxdZlw02CMspd+0KBx4I77yTdkUiHaKGL9IWAwbA3LnhLFmHHw4NDWlXJNJu2mmbUl6MzKLlxchcV16/a69l24sv5rlx41j23e9WvL4YmVnPi5GZ9zzttE1IO6Kylxcjc515DQ3udXVhnZ2bb06e1wFZ/7roPaefh3baipSBWThT1s47w3e+oxOhS6eihi/SXjU14UCs6uqwE/fdd9OuSKRNNIafUl6MzKLlxchsT173Rx5h5wkTeG233Xhi0qTw6T9yfTEys54XIzPveRrDT0jjktnLi5HZ7rzzzw8HZZ17bnny2iD191zhvBiZec9DY/giEZxwAowZAz/9Kdx6a9rViLRKDV8kCTOYPh122gnGjoXFi9OuSKRFavgiSW24YdiJaxZ24r73XtoViTRLDV+kHAYNgquvhiefhHHjwllxRTJGs3RSyouRWbS8GJlJ8/rPns02l17K4qOP5oUxYwrxnmPnxcjMe55m6SSkmQfZy4uRmTivocH9kEPCzJ1evbzBzH3gQPeZM8tRnrtn8D1HzouRmfc8NEtHpALMYOTI8G99PeYOS5fC+PEwa1ba1UnBqeGLlNvZZ689hr9iBUycmE49IiVq+CLltmxZ+7aLVIgavki5DRjQvu0iFaKGL1JukyeHBdYa69IlbBdJkaZlppQXI7NoeTEyy5XXa/58Bk2fTrf6ehq6dsVWreL+q6/mo549M1NjZ8mLkZn3PE3LTEhTzbKXFyMzSt4zz7hXV7sfe2z5Msso63kxMvOeh6ZliqRk223DtMzf/U4nTJFUqeGLVMLpp0O3bnDaaWlXIgWmhi9SCX36wIknwty58NBDaVcjBaWGL1IpJ54Im28Op5yixdUkFWr4IpWyySZhaOfOO+H229OuRgpIDV+kko4+GrbeGk4+GRoa0q5GCkYNX6SSunWDc86BBQtgzpy0q5GC0YFXKeXFyCxaXozMiuQ1NPDZ8eOpWrGCB6+8Eu/SJXs1ZigvRmbe83TgVUI6mCR7eTEyK5Z3661hzfxf/ap8mR2U9bwYmXnPQwdeiWTI3nvD8OFheOfdd9OuRgpCDV8kDWbw85/Dv/4FF1yQdjVSEGr4Imn53OfgkEPg/PNh+fK0q5ECUMMXSdP//A988EEY2hGJTA1fJE3bbw9HHQW//S0sXpx2NZJzavgiaTvjDOjaVQurSXRq+CJp23JLOP74cCDWI4+kXY3kmBq+SBb893/DZpvBqaemXYnkmBq+SBZsumkY0rn9di2sJtGo4YtkxbHHwsCBYflkLawmEajhi2TF6oXVHn0Urrkm7Wokh7R4Wkp5MTKLlhcjM/W8VavCwmoffMCDV1zR7MJqqddY4bwYmXnP0+JpCWlBqOzlxcjMRN4tt4SF1S66qHyZrch6XozMvOehxdNEOomRI2GPPeDss+G999KuRnJEDV8ka1YvrFZfD1Onpl2N5IgavkgWfeELcPDBMGVKaPwiZaCGL5JVkyfD+++HBdZEykANXySrBg+GcePgkkvguefSrkZyQA1fJMvOPBOqq+H009OuRHJADV8ky7baCn7yE7jqKvj739OuRjo5NXyRrJswAXr21MJqkpgavkjWde8OEyfCvHnQuzd7jBgBtbUwa1balUknU512ASLSBj17hvn59fUYwNKlMH58uK+uLs3KpBPRJ3yRzmDSJGi67tWKFeGTv0gbqeGLdAbLlrVvu0gz1PBFOoMBA9q3XaQZavgincHkyVBTs+a2mpqwXaSN1PBFOoO6Orj0Uhg4kE9G8s86SztspV3U8EU6i7o6WLKEv15/fTg71jPPpF2RdDJq+CKdzMoePeDww+HKK7WSprRLxRq+me1gZpeY2bVmdmylXlckl044AT78EC6+OO1KpBNpU8M3sxlmVm9mC5tsH2lmT5vZs2Z2SmsZ7v6Uux8DfAv4SsdLFhEGD4b99w8Nf8WKtKuRTqKtn/CvAEY23mBmVcDFwL7AEGCsmQ0xs53M7KYml16l5+wP3AzcUrZ3IFJUJ50Er78OV1yRdiXSSZg3PXqvpQea1QI3ufuOpdtfAia5+z6l26cCuPu5bci62d2/3sJ944HSMeMMBp5uU4Fr2xx4rYPPrURejMyi5cXIzHpejMys58XIzHveQHffounGJGvp9AVeaHT7ReALLT3YzIYB3wS60confHe/FLg0QV2rX+9hd/9s0pxYeTEyi5YXIzPreTEys54XI7NoeatVbPE0d78buLtSryciImtKMkvnJaB/o9v9SttERCSDkjT8h4DtzGxrM+sKjAH+VJ6yyiLxsFDkvBiZRcuLkZn1vBiZWc+LkVm0PKCNO23NbDYwjLAjYTlwprtfZmajgAuBKmCGu2thDxGRjGrzLB0REenctLSCiEhB5LLht+cI4DZkNXuUcYK8/mZ2l5k9aWZPmNmPy5C5vpk9aGaPlTLPKlOtVWb2dzO7qQxZS8zsH2a2wMweLkNe99IyHf80s6dKx4UkyRtcqm315R0z+0nCzONLX4+FZjbbzNZPmPfjUtYTHa2tue9nM+tpZreb2TOlf3skzDukVGODmbVramELeVNKX+fHzewGM+tehsxzSnkLzOw2M9sqSV6j+040MzezzRPWN8nMXmr0/TiqrXmtcvdcXQj7ExYDg4CuwGPAkAR5XwV2BRaWqb4tgV1L1zcGFiWpr5RjwEal612AB4AvlqHWE4CrCAfcJc1aAmxexq/zlcCRpetdge5l/h56lXDwSkcz+gLPAxuUbs8FjkiQtyOwEKghTKeeD2zbgZy1vp+B84BTStdPAX6eMG8HwkGTdwOfLUN9ewPVpes/b099rWRu0uj6j4BLkuSVtvcH5gFL2/O93kJ9k4CTyvH93PiSx0/4nweedffn3P0jYA5wQEfD3P0vwBvlKs7dX3H3R0vX3wWeIjSHJJnu7u+VbnYpXRLtnDGzfsDXgelJcmIws00JPySXAbj7R+7+VhlfYk9gsbsvTZhTDWxgZtWERv1ygqwdgAfcfYW7fwzcQziQsV1a+H4+gPALlNK/BybJ87BuVoeOkG8h77bSewa4nzAFPGnmO41ubkg7fl5a6Qm/ACa0J2sdeWWXx4bf3BHAiRpqLKXlKj5D+ESeNKvKzBYA9cDt7p4080LCN29D0tpKHLjNzB4pLZ+RxNbAv4DLS0NO081sw+QlfmIMMDtJgLu/BJwPLANeAd5299sSRC4EdjezzcysBhjFmsfBJNHb3V8pXX8V6F2m3Bi+D/y5HEFmNtnMXgDqgDMSZh0AvOTuj5WjtpLjSsNOM9ozzNaaPDb8TsHMNgKuA37S5NNGh7j7KnffhfDp5/NmtmOC2r4B1Lv7I0nramQ3d9+VsNjeD83sqwmyqgl/Ak9z988A/yYMRSRWOqZkf+CahDk9CJ+ctwa2AjY0s+92NM/dnyIMZ9wG3AosAFYlqbGF13ES/nUYi5lNBD4GZpUjz90nunv/Ut5xCeqqAX5Kwl8aTUwDtgF2IXxguKAcoXls+Jk/AtjMuhCa/Sx3v76c2aWhjbtosrppO30F2N/MlhCGxEaY2cyEdb1U+rceuIEw9NZRLwIvNvor5lrCL4By2Bd41N2XJ8zZC3je3f/l7iuB64EvJwl098vcfai7fxV4k7D/pxyWm9mWAKV/M8ToqaoAAAHKSURBVHdWFTM7AvgGUFf6pVROs4CDEzx/G8Iv9sdKPzP9gEfNrE9HA919eelDXAPwO5L9vHwijw0/00cAm5kRxp6fcvepZcrcYvXMBTPbAPga8M+O5rn7qe7ez91rCf9/d7p7hz+dmtmGZrbx6uuEnXAdnvXk7q8CL5jZ4NKmPYEnO5rXxFgSDueULAO+aGY1pa/5noT9NR1m/1lmfABh/P6qxFUGfwIOL10/HPhjmXLLwsxGEoYX93f3siz+b2bbNbp5AMl+Xv7h7r3cvbb0M/MiYWLGqwnq27LRzYNI8POyhnLvBc7ChTC+uYgwW2diwqzZhD+pVhK+kOMS5u1G+JP5ccKf5QuAUQkzPw38vZS5EDijjP+Xw0g4S4cwY+qx0uWJpF+TUuYuwMOl9/wHoEcZMjcEXgc2LdP/3VmERrIQ+D3QLWHevYRfbI8Be3YwY63vZ2Az4A7gGcLsn54J8w4qXf+QcGT+vIR5zxL2y63+eWnzjJpWMq8rfV0eB24E+ibJa3L/Eto3S6e5+n4P/KNU35+ALcvxPakjbUVECiKPQzoiItIMNXwRkYJQwxcRKQg1fBGRglDDFxEpCDV8EZGCUMMXESmI/wc/lErPggApTwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7_MDz6szq8m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1bd494ff-ab6b-4ea5-b456-93640821cf1d"
      },
      "source": [
        "#---See the learned Codebook\n",
        "batch_size_plot = M\n",
        "symbols = np.arange(M)+1\n",
        "symbols = np.stack(J*[symbols], axis=0).transpose()\n",
        "\n",
        "#---Get codebook\n",
        "codebook = get_codebook_from_condensed_codebook()\n",
        "\n",
        "h_all = []\n",
        "real_idx = 2 * torch.arange(K).long()\n",
        "\n",
        "for i in range(J):\n",
        "  h = np.zeros([J, 2*K])\n",
        "#   h[i, :] = np.ones([1, 2*K])\n",
        "  h[i, real_idx] = 1.0\n",
        "  h = np.stack(batch_size_plot*[h], axis = 0)\n",
        "  h = torch.Tensor(h).to(device)\n",
        "  h_all.append(h)\n",
        "\n",
        "\n",
        "#---Forward Pass\n",
        "codewords_faded_all = []\n",
        "for i in range(J):\n",
        "    codewords_faded, codewords_faded_without_h = encode(symbols, codebook, h = h_all[i])\n",
        "    codewords_faded_all.append(codewords_faded)\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#---code for color assignment for arbitrary num of users\n",
        "# user_colors = {}\n",
        "# users = np.arange(J)\n",
        "# unique_users = list(set(users)) \n",
        "# step_size = (256**3) // len(unique_users)\n",
        "# for i, user in enumerate(unique_users):\n",
        "#     temp = step_size*i\n",
        "#     color = np.zeros([3], dtype=np.float32) # RGB\n",
        "#     for pigment in range(3):\n",
        "#       r = temp % 256\n",
        "#       color[pigment] = r/256\n",
        "#       temp = temp//256\n",
        "#     user_colors[user] = color.tolist()\n",
        "# colors = [user_colors[user] for user in users]\n",
        "\n",
        "colors = ['cyan', 'green', 'blue', 'magenta', 'orange', 'red']\n",
        "\n",
        "for k in range(K):\n",
        "  fig, ax = plt.subplots()\n",
        "  for user in range(J):\n",
        "    data = codewords_faded_all[user].cpu().detach().numpy()\n",
        "    if np.abs(data[:, 2*k]).sum()!=0:\n",
        "      ax.scatter(data[:, 2*k], data[:, 2*k+1], c=colors[user], label='User '+str(user+1))\n",
        "\n",
        "  ax.legend()\n",
        "  ax.axhline(y=0, color='k')\n",
        "  ax.axvline(x=0, color='k')\n",
        "  ax.set_title('Resource: '+str(k+1))\n",
        "  ax.imshow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJR2Yy0PGdNo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}